{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33menkeejunior1\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/enkeejunior1/fbprize/runs/2k3h1n1q\" target=\"_blank\">fresh-donkey-28</a></strong> to <a href=\"https://wandb.ai/enkeejunior1/fbprize\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from train_yh import *\n",
    "from utils_yh import *\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from einops import rearrange, reduce, repeat\n",
    "import tez\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "import transformers\n",
    "\n",
    "import copy\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from joblib import Parallel, delayed \n",
    "from tez import enums\n",
    "from tez.callbacks import Callback\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/enkeejunior1/kaggle_feedbackprize/fb-sbert-yh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting\n",
    "class set_args_submission:\n",
    "    def __init__(self):\n",
    "        self.model: str = 'longformer-base-4096'\n",
    "        self.sbert: bool = False\n",
    "        self.output: str = '../model'\n",
    "        self.input: str = '../input'\n",
    "        self.max_len: int = 4096\n",
    "        self.valid_batch_size: int = 4\n",
    "        self.kfold = 5\n",
    "            \n",
    "args = set_args_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translation(df, src2tgt_tokenizer, src2tgt, tgt2src_tokenizer, tgt2src, args, num_jobs):\n",
    "    total_ids = df[\"id\"].unique()\n",
    "    total_ids_splits = np.array_split(total_ids, num_jobs)\n",
    "\n",
    "    results = Parallel(n_jobs=num_jobs, backend=\"multiprocessing\")(\n",
    "        delayed(_back_translation_helper)(idx, src2tgt_tokenizer, src2tgt, tgt2src_tokenizer, tgt2src, args) for idx in total_ids_splits\n",
    "    )\n",
    "    \n",
    "    return\n",
    "\n",
    "def _back_translation_helper(total_ids, src2tgt_tokenizer, src2tgt, tgt2src_tokenizer, tgt2src, args):\n",
    "    for idx in tqdm(total_ids):\n",
    "        filename = os.path.join(args.input, \"train\", idx + \".txt\")\n",
    "        with open(filename, \"r\") as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # translation\n",
    "        input_ids_split = [src2tgt_tokenizer.encode(sent, return_tensors=\"pt\") for sent in nltk.sent_tokenize(text)]\n",
    "        outputs_split = []\n",
    "        for input_ids in input_ids_split:\n",
    "            outputs = src2tgt.generate(input_ids)\n",
    "            outputs_split.append(outputs)\n",
    "        decode_split = []\n",
    "        for outputs in outputs_split:\n",
    "            decode_split.append(src2tgt_tokenizer.decode(outputs.squeeze(), skip_special_tokens = True))\n",
    "        \n",
    "        # back-translation\n",
    "        input_ids_split = [tgt2src_tokenizer.encode(sent, return_tensors=\"pt\") for sent in decode_split]\n",
    "        outputs_split = []\n",
    "        for input_ids in input_ids_split:\n",
    "            outputs = tgt2src.generate(input_ids)\n",
    "            outputs_split.append(outputs)\n",
    "        decode_split = []\n",
    "        for outputs in outputs_split:\n",
    "            decode_split.append(tgt2src_tokenizer.decode(outputs.squeeze(), skip_special_tokens = True))\n",
    "        \n",
    "        filename = os.path.join(args.input, \"aug_back_translation\", idx + \".txt\")\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(' '.join(decode_split))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_JOBS = 12\n",
    "\n",
    "# read kfold data\n",
    "if os.path.isfile(args.input + '/' + f'train_{args.kfold}folds.csv'):\n",
    "    df = pd.read_csv(args.input + '/' + f'train_{args.kfold}folds.csv')\n",
    "else:\n",
    "    df = pd.read_csv(args.input_path + '/' + f'train.csv')\n",
    "    df = get_stratified_fold(df, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "src2tgt_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
    "src2tgt = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
    "\n",
    "tgt2src_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")\n",
    "tgt2src = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")\n",
    "\n",
    "src2tgt.eval()\n",
    "tgt2src.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "n_jobs = psutil.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 535/15594 [3:36:20<221:16:04, 52.90s/it]"
     ]
    }
   ],
   "source": [
    "back_translation(df, src2tgt_tokenizer, src2tgt, tgt2src_tokenizer, tgt2src, args, num_jobs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snu36",
   "language": "python",
   "name": "snu36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
