{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_yh import *\n",
    "from utils_yh import *\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from einops import rearrange, reduce, repeat\n",
    "import tez\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, AutoConfig, AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "\n",
    "import copy\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AdamW, AutoConfig, AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "from joblib import Parallel, delayed \n",
    "from tez import enums\n",
    "from tez.callbacks import Callback\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting\n",
    "class set_args_submission:\n",
    "    def __init__(self):\n",
    "        self.model: str = 'longformer-base-4096'\n",
    "        self.sbert: bool = True\n",
    "        self.output: str = '../model'\n",
    "        self.input: str = '../input'\n",
    "        self.max_len: int = 4096\n",
    "        self.valid_batch_size: int = 4\n",
    "            \n",
    "args = set_args_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:10<00:00,  5.38s/it]\n"
     ]
    }
   ],
   "source": [
    "# 모델 불러오기\n",
    "model = FeedbackModel(\n",
    "            model_name=args.model,\n",
    "            num_train_steps=0,\n",
    "            learning_rate=0,\n",
    "            num_labels=len(target_id_map) - 1,\n",
    "            steps_per_epoch=0,\n",
    "            args=args,\n",
    "        )\n",
    "\n",
    "weight_path = os.path.join(args.output, 'longformer-base-4096-sbert.bin')\n",
    "model.load_state_dict(torch.load(weight_path))\n",
    "model.eval()\n",
    "\n",
    "# 데이터 불러오기\n",
    "df = pd.read_csv(os.path.join(\"../input/\", \"sample_submission.csv\"))\n",
    "df_ids = df[\"id\"].unique()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
    "test_samples = prepare_test_data(df, tokenizer, args)\n",
    "collate = ValidCollate(tokenizer, args)\n",
    "\n",
    "test_dataset = FeedbackDatasetValid(test_samples, args.max_len, tokenizer, args = args)\n",
    "\n",
    "# preds_iter = model.predict(test_dataset, batch_size=args.valid_batch_size, n_jobs=-1, collate_fn=collate)\n",
    "\n",
    "import psutil\n",
    "n_jobs = psutil.cpu_count()\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=args.valid_batch_size, \n",
    "    num_workers=n_jobs, \n",
    "    collate_fn=collate, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "def predict(data_loader, model):\n",
    "    tk0 = tqdm(data_loader, total = len(data_loader))\n",
    "    for _, data in enumerate(tk0):\n",
    "        with torch.no_grad():\n",
    "            data['ids'] = data['ids'].to(model.device)\n",
    "            data['mask'] = data['mask'].to(model.device)\n",
    "            data['input_type_list'] = [s.to(model.device) for s in data['input_type_list']]\n",
    "\n",
    "            output, _, _ = model(**data)\n",
    "            output = output.cpu().detach().numpy()\n",
    "            yield output\n",
    "    tk0.close()\n",
    "\n",
    "\n",
    "preds_iter = predict(data_loader, model)\n",
    "\n",
    "raw_preds = []\n",
    "current_idx = 0\n",
    "\n",
    "for preds in preds_iter:\n",
    "    preds = preds.astype(np.float16)\n",
    "    raw_preds.append(preds)\n",
    "    current_idx += 1\n",
    "    \n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "final_preds = []\n",
    "final_scores = []\n",
    "\n",
    "for rp in raw_preds:\n",
    "    pred_class = np.argmax(rp, axis=2)\n",
    "    pred_scrs = np.max(rp, axis=2)\n",
    "    for pred, pred_scr in zip(pred_class, pred_scrs):\n",
    "        pred = pred.tolist()\n",
    "        pred_scr = pred_scr.tolist()\n",
    "        final_preds.append(pred)\n",
    "        final_scores.append(pred_scr)\n",
    "\n",
    "for j in range(len(test_samples)):\n",
    "    tt = [id_target_map[p] for p in final_preds[j][1:]]\n",
    "    tt_score = final_scores[j][1:]\n",
    "    test_samples[j][\"preds\"] = tt\n",
    "    test_samples[j][\"pred_scores\"] = tt_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jn(pst, start, end):\n",
    "    return \" \".join([str(x) for x in pst[start:end]])\n",
    "\n",
    "def link_evidence(oof):\n",
    "    thresh = 1\n",
    "    idu = oof['id'].unique()\n",
    "    idc = idu[1]\n",
    "    eoof = oof[oof['class'] == \"Evidence\"]\n",
    "    neoof = oof[oof['class'] != \"Evidence\"]\n",
    "    for thresh2 in range(26,27, 1):\n",
    "        retval = []\n",
    "        for idv in idu:\n",
    "            for c in  ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n",
    "                   'Counterclaim', 'Rebuttal']:\n",
    "                q = eoof[(eoof['id'] == idv) & (eoof['class'] == c)]\n",
    "                if len(q) == 0:\n",
    "                    continue\n",
    "                pst = []\n",
    "                for i,r in q.iterrows():\n",
    "                    pst = pst +[-1] + [int(x) for x in r['predictionstring'].split()]\n",
    "                start = 1\n",
    "                end = 1\n",
    "                for i in range(2,len(pst)):\n",
    "                    cur = pst[i]\n",
    "                    end = i\n",
    "                    #if pst[start] == 205:\n",
    "                    #   print(cur, pst[start], cur - pst[start])\n",
    "                    if (cur == -1 and c != 'Evidence') or ((cur == -1) and ((pst[i+1] > pst[end-1] + thresh) or (pst[i+1] - pst[start] > thresh2))):\n",
    "                        retval.append((idv, c, jn(pst, start, end)))\n",
    "                        start = i + 1\n",
    "                v = (idv, c, jn(pst, start, end+1))\n",
    "                #print(v)\n",
    "                retval.append(v)\n",
    "        roof = pd.DataFrame(retval, columns = ['id', 'class', 'predictionstring']) \n",
    "        roof = roof.merge(neoof, how='outer')\n",
    "        return roof\n",
    "\n",
    "proba_thresh = {\n",
    "    \"Lead\": 0.7,\n",
    "    \"Position\": 0.55,\n",
    "    \"Evidence\": 0.65,\n",
    "    \"Claim\": 0.55,\n",
    "    \"Concluding Statement\": 0.7,\n",
    "    \"Counterclaim\": 0.5,\n",
    "    \"Rebuttal\": 0.55,\n",
    "}\n",
    "\n",
    "min_thresh = {\n",
    "    \"Lead\": 9,\n",
    "    \"Position\": 5,\n",
    "    \"Evidence\": 14,\n",
    "    \"Claim\": 3,\n",
    "    \"Concluding Statement\": 11,\n",
    "    \"Counterclaim\": 6,\n",
    "    \"Rebuttal\": 4,\n",
    "}\n",
    "\n",
    "submission = []\n",
    "for sample_idx, sample in enumerate(test_samples):\n",
    "    preds = sample[\"preds\"]\n",
    "    offset_mapping = sample[\"offset_mapping\"]\n",
    "    sample_id = sample[\"id\"]\n",
    "    sample_text = sample[\"text\"]\n",
    "    sample_input_ids = sample[\"input_ids\"]\n",
    "    sample_pred_scores = sample[\"pred_scores\"]\n",
    "    sample_preds = []\n",
    "\n",
    "    if len(preds) < len(offset_mapping):\n",
    "        preds = preds + [\"O\"] * (len(offset_mapping) - len(preds))\n",
    "        sample_pred_scores = sample_pred_scores + [0] * (len(offset_mapping) - len(sample_pred_scores))\n",
    "    \n",
    "    idx = 0\n",
    "    phrase_preds = []\n",
    "    while idx < len(offset_mapping):\n",
    "        start, _ = offset_mapping[idx]\n",
    "        if preds[idx] != \"O\":\n",
    "            label = preds[idx][2:]\n",
    "        else:\n",
    "            label = \"O\"\n",
    "        phrase_scores = []\n",
    "        phrase_scores.append(sample_pred_scores[idx])\n",
    "        idx += 1\n",
    "        while idx < len(offset_mapping):\n",
    "            if label == \"O\":\n",
    "                matching_label = \"O\"\n",
    "            else:\n",
    "                matching_label = f\"I-{label}\"\n",
    "            if preds[idx] == matching_label:\n",
    "                _, end = offset_mapping[idx]\n",
    "                phrase_scores.append(sample_pred_scores[idx])\n",
    "                idx += 1\n",
    "            else:\n",
    "                break\n",
    "        if \"end\" in locals():\n",
    "            phrase = sample_text[start:end]\n",
    "            phrase_preds.append((phrase, start, end, label, phrase_scores))\n",
    "\n",
    "    temp_df = []\n",
    "    for phrase_idx, (phrase, start, end, label, phrase_scores) in enumerate(phrase_preds):\n",
    "        word_start = len(sample_text[:start].split())\n",
    "        word_end = word_start + len(sample_text[start:end].split())\n",
    "        word_end = min(word_end, len(sample_text.split()))\n",
    "        ps = \" \".join([str(x) for x in range(word_start, word_end)])\n",
    "        if label != \"O\":\n",
    "            if sum(phrase_scores) / len(phrase_scores) >= proba_thresh[label]:\n",
    "                if len(ps.split()) >= min_thresh[label]:\n",
    "                    temp_df.append((sample_id, label, ps))\n",
    "    \n",
    "    temp_df = pd.DataFrame(temp_df, columns=[\"id\", \"class\", \"predictionstring\"])\n",
    "    submission.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat(submission).reset_index(drop=True)\n",
    "submission = link_evidence(submission)\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snu36",
   "language": "python",
   "name": "snu36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
