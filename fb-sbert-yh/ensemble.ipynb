{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_yh import *\n",
    "from utils_yh import *\n",
    "from preprocess import *\n",
    "from model_yh import *\n",
    "from data import *\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from einops import rearrange, reduce, repeat\n",
    "import tez\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, AutoConfig, AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "\n",
    "import copy\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AdamW, AutoConfig, AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "from joblib import Parallel, delayed \n",
    "from tez import enums\n",
    "from tez.callbacks import Callback\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/enkeejunior1/kaggle_feedbackprize/fb-sbert-yh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting\n",
    "class set_args_submission:\n",
    "    def __init__(self):\n",
    "        self.model: str = 'longformer-large-4096'\n",
    "        self.sbert: bool = False\n",
    "        self.output: str = '../submission_model'\n",
    "        self.input: str = '../input'\n",
    "        self.max_len: int = 4096 # need to debug\n",
    "        self.valid_batch_size: int = 4\n",
    "        self.kfold = 5\n",
    "        self.fold = 0\n",
    "            \n",
    "args = set_args_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read kfold data\n",
    "if os.path.isfile(args.input + '/' + f'train_{args.kfold}folds.csv'):\n",
    "    df = pd.read_csv(args.input + '/' + f'train_{args.kfold}folds.csv')\n",
    "else:\n",
    "    df = pd.read_csv(args.input_path + '/' + f'train.csv')\n",
    "    df = get_stratified_fold(df, args)\n",
    "\n",
    "train_df = df[df[f\"{args.kfold}fold\"] != args.fold].reset_index(drop=True)\n",
    "valid_df = df[df[f\"{args.kfold}fold\"] == args.fold].reset_index(drop=True)\n",
    "\n",
    "args_path = os.listdir(args.output)\n",
    "args_path = list(filter(lambda x: '.json' in x, args_path))\n",
    "\n",
    "args_dict_list = []\n",
    "for path in args_path:\n",
    "    with open(os.path.join(args.output, path), 'r') as fp:\n",
    "        args_model = json.load(fp)\n",
    "    args_dict_list.append(args_model)\n",
    "    \n",
    "class ArgsModelCheck:\n",
    "    @classmethod\n",
    "    def __init__(cls, args_dict):\n",
    "        for k, v in args_dict.items():\n",
    "            setattr(cls, k, v)\n",
    "\n",
    "args_list = []\n",
    "for i, args_dict in enumerate(args_dict_list):\n",
    "    fold = args_dict['fold']\n",
    "    cv_score = args_dict['cv_score']\n",
    "    globals()[f'args_{i}'] = ArgsModelCheck(args_dict)\n",
    "    args_list.append(globals()[f'args_{i}'])\n",
    "    \n",
    "# 모델 불러오기\n",
    "model_list = []\n",
    "for i, args_model in enumerate(args_list):\n",
    "    globals()[f'model_{i}'] = FeedbackModel(\n",
    "            model_name=args.model,\n",
    "            num_train_steps=0,\n",
    "            learning_rate=0,\n",
    "            num_labels=len(target_id_map) - 1,\n",
    "            steps_per_epoch=0,\n",
    "            args=args\n",
    "        )\n",
    "    weight_path = os.path.join(args.output, f'{args_model.model}-{args_model.time}.bin')\n",
    "    globals()[f'model_{i}'].load_state_dict(torch.load(weight_path))\n",
    "    globals()[f'model_{i}'].eval()\n",
    "    model_list.append(globals()[f'model_{i}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = vars(args)\n",
    "for args_model_dict in args_dict_list:\n",
    "    for k, v in args_model_dict.items():\n",
    "        if k not in args_dict.keys():\n",
    "            setattr(args, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6229 > 4096). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7614 > 4096). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8580 > 4096). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8862 > 4096). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14778 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
    "train_samples = prepare_training_data(df, tokenizer, args, num_jobs = 12)\n",
    "collate = TrainCollate(tokenizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FeedbackDataset(train_samples, args.max_len, tokenizer, args = args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=1, \n",
    "    num_workers=1, \n",
    "    collate_fn=collate, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "for batch in data_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 가 높게 나오는 sample 살펴보기\n",
    "# sentence 구분이랑 target 이랑 얼마나 일치하는지 확인하기\n",
    "# 실제 prediction 살펴보고, 잘 못하는 sample 확인하기 (아직 token level 에서만 확인)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-Lead': 0,\n",
       " 'I-Lead': 1,\n",
       " 'B-Position': 2,\n",
       " 'I-Position': 3,\n",
       " 'B-Evidence': 4,\n",
       " 'I-Evidence': 5,\n",
       " 'B-Claim': 6,\n",
       " 'I-Claim': 7,\n",
       " 'B-Concluding Statement': 8,\n",
       " 'I-Concluding Statement': 9,\n",
       " 'B-Counterclaim': 10,\n",
       " 'I-Counterclaim': 11,\n",
       " 'B-Rebuttal': 12,\n",
       " 'I-Rebuttal': 13,\n",
       " 'O': 14,\n",
       " 'PAD': -100}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_loader, model):\n",
    "    global device, args\n",
    "    tk0 = tqdm(data_loader, total = len(data_loader))\n",
    "    for _, data in enumerate(tk0):\n",
    "        with torch.no_grad():\n",
    "            data['ids'] = data['ids'].to(device)\n",
    "            data['mask'] = data['mask'].to(device)\n",
    "            if args.sbert:\n",
    "                data['input_type_list'] = [s.to(device) for s in data['input_type_list']]\n",
    "\n",
    "            output, _, _ = model(**data)\n",
    "            output = output[data['mask'] == 1]\n",
    "            output = output.cpu().detach().numpy()\n",
    "            yield output, data['target']\n",
    "    tk0.close()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "preds_iter_list = []\n",
    "for model in model_list:\n",
    "    model.to(device)\n",
    "    preds_iter = predict(data_loader, model)\n",
    "    preds_iter_list.append(preds_iter)\n",
    "\n",
    "raw_preds = []\n",
    "current_idx = 0\n",
    "\n",
    "preds_iter = preds_iter_list[0]\n",
    "for preds in preds_iter:\n",
    "    preds = preds.astype(np.float16)\n",
    "    raw_preds.append(preds)\n",
    "    current_idx += 1\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "final_preds = []\n",
    "final_scores = []\n",
    "\n",
    "for rp in raw_preds:\n",
    "    pred_class = np.argmax(rp, axis=2)\n",
    "    pred_scrs = np.max(rp, axis=2)\n",
    "    for pred, pred_scr in zip(pred_class, pred_scrs):\n",
    "        pred = pred.tolist()\n",
    "        pred_scr = pred_scr.tolist()\n",
    "        final_preds.append(pred)\n",
    "        final_scores.append(pred_scr)\n",
    "\n",
    "for j in range(len(test_samples)):\n",
    "    tt = [id_target_map[p] for p in final_preds[j][1:]]\n",
    "    tt_score = final_scores[j][1:]\n",
    "    test_samples[j][\"preds\"] = tt\n",
    "    test_samples[j][\"pred_scores\"] = tt_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create examplar input\n",
    "num_ex = 100 # number of text\n",
    "num_seq = 100\n",
    "num_ensemble = 10\n",
    "num_class = len(target_id_map)\n",
    "\n",
    "pred_iter_ex = [torch.rand(num_class, num_ensemble).softmax(dim = -1).view(-1).tolist() for _ in range(num_seq * num_ex)] # [num_class * num_ensemble, num_seq * num_ex]\n",
    "true_token_class = torch.randint(0, num_class + 1, (num_seq, num_ex)).view(-1).tolist() # [num_seq * num_ex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44min 26s, sys: 2.93 s, total: 44min 29s\n",
      "Wall time: 9min 34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "             gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.300000012,\n",
       "             max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=100, n_jobs=40,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "my_model = XGBRegressor(tree_method = \"gpu_hist\")\n",
    "my_model.fit(pred_iter_ex, true_token_class, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_iter = [torch.rand(num_class, num_ensemble).softmax(dim = -1) for range(num_seq)]\n",
    "true_token_class = torch.randint(0, num_class + 1, (num_seq, num_ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_path = os.listdir('../model')\n",
    "args_path = list(filter(lambda x: '.json' in x, args_path))\n",
    "\n",
    "args_dict_list = []\n",
    "for path in args_path:\n",
    "    with open(os.path.join('../model', path), 'r') as fp:\n",
    "        args = json.load(fp)\n",
    "    args_dict_list.append(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgsModelCheck:\n",
    "    @classmethod\n",
    "    def __init__(cls, args_dict):\n",
    "        for k, v in args_dict.items():\n",
    "            setattr(cls, k, v)\n",
    "\n",
    "args_list = []\n",
    "for i, args_dict in enumerate(args_dict_list):\n",
    "    fold = args_dict['fold']\n",
    "    cv_score = args_dict['cv_score']\n",
    "    globals()[f'args_{i}'] = ArgsModelCheck(args_dict)\n",
    "    args_list.append(globals()[f'args_{i}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out of fold ensemble -> cv check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6229 > 4096). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7614 > 4096). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8580 > 4096). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8862 > 4096). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14778 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# 모델 불러오기\n",
    "model = FeedbackModel(\n",
    "            model_name=args.model,\n",
    "            num_train_steps=0,\n",
    "            learning_rate=0,\n",
    "            num_labels=len(target_id_map) - 1,\n",
    "            steps_per_epoch=0,\n",
    "            args=args,\n",
    "        )\n",
    "\n",
    "weight_path = os.path.join(args.output, f'{args.model}-{args.time}.bin')\n",
    "model.load_state_dict(torch.load(weight_path))\n",
    "model.eval()\n",
    "\n",
    "# 데이터 불러오기\n",
    "df = pd.read_csv(args.input + '/' + f'train_{args.kfold}folds.csv')\n",
    "df_ids = df[\"id\"].unique()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
    "training_samples = prepare_training_data(df, tokenizer, args, num_jobs=12)\n",
    "collate = TrainCollate(tokenizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = FeedbackDataset(training_samples, args.max_len, tokenizer, args = args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3899 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (0) must match the size of tensor b (4) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-27a720c2106c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mcurrent_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mraw_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-27a720c2106c>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(data_loader, model)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_type_list'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_type_list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle_feedbackprize/fb-sbert-yh/train_yh.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ids, mask, input_type_list, token_type_ids, targets)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msbert_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msbert_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_type_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msbert_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (0) must match the size of tensor b (4) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "n_jobs = psutil.cpu_count()\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    training_dataset, \n",
    "    batch_size=args.batch_size, \n",
    "    num_workers=n_jobs,  \n",
    "    collate_fn=collate, \n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "def predict(data_loader, model):\n",
    "    global device\n",
    "    tk0 = tqdm(data_loader, total = len(data_loader))\n",
    "    for _, data in enumerate(tk0):\n",
    "        with torch.no_grad():\n",
    "            data['ids'] = data['ids'].to(device)\n",
    "            data['mask'] = data['mask'].to(device)\n",
    "            data['input_type_list'] = [s.to(device) for s in data['input_type_list']]\n",
    "\n",
    "            output, _, _ = model(**data)\n",
    "            output = output.cpu().detach().numpy()\n",
    "            yield output\n",
    "    tk0.close()\n",
    "\n",
    "model.to(device)\n",
    "preds_iter = predict(data_loader, model)\n",
    "\n",
    "raw_preds = []\n",
    "current_idx = 0\n",
    "\n",
    "for preds in preds_iter:\n",
    "    preds = preds.astype(np.float16)\n",
    "    raw_preds.append(preds)\n",
    "    current_idx += 1\n",
    "    \n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = []\n",
    "final_scores = []\n",
    "\n",
    "for rp in raw_preds:\n",
    "    pred_class = np.argmax(rp, axis=2)\n",
    "    pred_scrs = np.max(rp, axis=2)\n",
    "    for pred, pred_scr in zip(pred_class, pred_scrs):\n",
    "        pred = pred.tolist()\n",
    "        pred_scr = pred_scr.tolist()\n",
    "        final_preds.append(pred)\n",
    "        final_scores.append(pred_scr)\n",
    "\n",
    "for j in range(len(test_samples)):\n",
    "    tt = [id_target_map[p] for p in final_preds[j][1:]]\n",
    "    tt_score = final_scores[j][1:]\n",
    "    test_samples[j][\"preds\"] = tt\n",
    "    test_samples[j][\"pred_scores\"] = tt_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jn(pst, start, end):\n",
    "    return \" \".join([str(x) for x in pst[start:end]])\n",
    "\n",
    "def link_evidence(oof):\n",
    "    thresh = 1\n",
    "    idu = oof['id'].unique()\n",
    "    idc = idu[1]\n",
    "    eoof = oof[oof['class'] == \"Evidence\"]\n",
    "    neoof = oof[oof['class'] != \"Evidence\"]\n",
    "    for thresh2 in range(26,27, 1):\n",
    "        retval = []\n",
    "        for idv in idu:\n",
    "            for c in  ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n",
    "                   'Counterclaim', 'Rebuttal']:\n",
    "                q = eoof[(eoof['id'] == idv) & (eoof['class'] == c)]\n",
    "                if len(q) == 0:\n",
    "                    continue\n",
    "                pst = []\n",
    "                for i,r in q.iterrows():\n",
    "                    pst = pst +[-1] + [int(x) for x in r['predictionstring'].split()]\n",
    "                start = 1\n",
    "                end = 1\n",
    "                for i in range(2,len(pst)):\n",
    "                    cur = pst[i]\n",
    "                    end = i\n",
    "                    #if pst[start] == 205:\n",
    "                    #   print(cur, pst[start], cur - pst[start])\n",
    "                    if (cur == -1 and c != 'Evidence') or ((cur == -1) and ((pst[i+1] > pst[end-1] + thresh) or (pst[i+1] - pst[start] > thresh2))):\n",
    "                        retval.append((idv, c, jn(pst, start, end)))\n",
    "                        start = i + 1\n",
    "                v = (idv, c, jn(pst, start, end+1))\n",
    "                #print(v)\n",
    "                retval.append(v)\n",
    "        roof = pd.DataFrame(retval, columns = ['id', 'class', 'predictionstring']) \n",
    "        roof = roof.merge(neoof, how='outer')\n",
    "        return roof\n",
    "\n",
    "proba_thresh = {\n",
    "    \"Lead\": 0.7,\n",
    "    \"Position\": 0.55,\n",
    "    \"Evidence\": 0.65,\n",
    "    \"Claim\": 0.55,\n",
    "    \"Concluding Statement\": 0.7,\n",
    "    \"Counterclaim\": 0.5,\n",
    "    \"Rebuttal\": 0.55,\n",
    "}\n",
    "\n",
    "min_thresh = {\n",
    "    \"Lead\": 9,\n",
    "    \"Position\": 5,\n",
    "    \"Evidence\": 14,\n",
    "    \"Claim\": 3,\n",
    "    \"Concluding Statement\": 11,\n",
    "    \"Counterclaim\": 6,\n",
    "    \"Rebuttal\": 4,\n",
    "}\n",
    "\n",
    "submission = []\n",
    "for sample_idx, sample in enumerate(test_samples):\n",
    "    preds = sample[\"preds\"]\n",
    "    offset_mapping = sample[\"offset_mapping\"]\n",
    "    sample_id = sample[\"id\"]\n",
    "    sample_text = sample[\"text\"]\n",
    "    sample_input_ids = sample[\"input_ids\"]\n",
    "    sample_pred_scores = sample[\"pred_scores\"]\n",
    "    sample_preds = []\n",
    "\n",
    "    if len(preds) < len(offset_mapping):\n",
    "        preds = preds + [\"O\"] * (len(offset_mapping) - len(preds))\n",
    "        sample_pred_scores = sample_pred_scores + [0] * (len(offset_mapping) - len(sample_pred_scores))\n",
    "    \n",
    "    idx = 0\n",
    "    phrase_preds = []\n",
    "    while idx < len(offset_mapping):\n",
    "        start, _ = offset_mapping[idx]\n",
    "        if preds[idx] != \"O\":\n",
    "            label = preds[idx][2:]\n",
    "        else:\n",
    "            label = \"O\"\n",
    "        phrase_scores = []\n",
    "        phrase_scores.append(sample_pred_scores[idx])\n",
    "        idx += 1\n",
    "        while idx < len(offset_mapping):\n",
    "            if label == \"O\":\n",
    "                matching_label = \"O\"\n",
    "            else:\n",
    "                matching_label = f\"I-{label}\"\n",
    "            if preds[idx] == matching_label:\n",
    "                _, end = offset_mapping[idx]\n",
    "                phrase_scores.append(sample_pred_scores[idx])\n",
    "                idx += 1\n",
    "            else:\n",
    "                break\n",
    "        if \"end\" in locals():\n",
    "            phrase = sample_text[start:end]\n",
    "            phrase_preds.append((phrase, start, end, label, phrase_scores))\n",
    "\n",
    "    temp_df = []\n",
    "    for phrase_idx, (phrase, start, end, label, phrase_scores) in enumerate(phrase_preds):\n",
    "        word_start = len(sample_text[:start].split())\n",
    "        word_end = word_start + len(sample_text[start:end].split())\n",
    "        word_end = min(word_end, len(sample_text.split()))\n",
    "        ps = \" \".join([str(x) for x in range(word_start, word_end)])\n",
    "        if label != \"O\":\n",
    "            if sum(phrase_scores) / len(phrase_scores) >= proba_thresh[label]:\n",
    "                if len(ps.split()) >= min_thresh[label]:\n",
    "                    temp_df.append((sample_id, label, ps))\n",
    "    \n",
    "    temp_df = pd.DataFrame(temp_df, columns=[\"id\", \"class\", \"predictionstring\"])\n",
    "    submission.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat(submission).reset_index(drop=True)\n",
    "submission = link_evidence(submission)\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snu36",
   "language": "python",
   "name": "snu36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
