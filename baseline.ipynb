{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "22.01.02 understand and implement baseline code from kaggle\n",
    "    main ref) https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615?scriptVersionId=83230719\n",
    "    ref) https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define config\n",
    "config = {'model_name': MODEL_NAME,   \n",
    "         'max_length': 1024,\n",
    "         'train_batch_size':4,\n",
    "         'valid_batch_size':4,\n",
    "         'epochs':5,\n",
    "         'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n",
    "         'max_grad_norm':10,\n",
    "         'device': 'cuda' if cuda.is_available() else 'cpu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666cc845bdc54244acb9aff81cef5626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51674fa78df049babf1e80f7e1b66fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e6301eb1b248a7aa55f1f9062defcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/846k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055c9165c0c64092805d94e89973a1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/775 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9508107e9dd543fa9c6ed48270f8d929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdForTokenClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# if you are first running this code, please download LM.\n",
    "MODEL_NAME = 'google/bigbird-roberta-base' # choose which model to download\n",
    "DOWNLOADED_MODEL_PATH = 'model'            # choose where to download the model\n",
    "\n",
    "if DOWNLOADED_MODEL_PATH == 'model':\n",
    "    os.mkdir('model')\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n",
    "    tokenizer.save_pretrained('model')\n",
    "\n",
    "    config_model = AutoConfig.from_pretrained(MODEL_NAME) \n",
    "    config_model.num_labels = 15\n",
    "    config_model.save_pretrained('model')\n",
    "\n",
    "    backbone = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, \n",
    "                                                               config=config_model)\n",
    "    backbone.save_pretrained('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15594/15594 [00:17<00:00, 875.31it/s] \n",
      "15594it [02:19, 112.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert the competition as a NER problem.\n",
    "# ref) https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615?scriptVersionId=83230719\n",
    "PATH = os.path.join(os.getcwd(), 'input')\n",
    "TRAIN_NER_PATH = os.path.join(PATH, 'train_NER.csv')\n",
    "\n",
    "# import original text files\n",
    "train_ids, train_texts = [], []\n",
    "for f in tqdm(list(os.listdir(os.path.join(PATH, 'train')))):\n",
    "    train_ids.append(f.replace('.txt', ''))\n",
    "    train_texts.append(open(os.path.join(PATH, 'train', f), 'r').read())\n",
    "\n",
    "train_text_df = pd.DataFrame({'id': train_ids, 'text': train_texts})\n",
    "\n",
    "# convert text to NER labels (or import processed tokens)\n",
    "try:\n",
    "    print('import NER labels from the processed file...')\n",
    "    from ast import literal_eval\n",
    "    train_text_df = pd.read_csv(TRAIN_NER_PATH)\n",
    "    # pandas saves lists as string, we must convert back\n",
    "    train_text_df.entities = train_text_df.entities.apply(lambda x: literal_eval(x) )\n",
    "    \n",
    "except:\n",
    "    print('converting text to NER labels...')\n",
    "    train_df = pd.read_csv(os.path.join(PATH, 'train.csv'))\n",
    "    all_entities = []\n",
    "    for i in tqdm(train_text_df.iterrows()):\n",
    "        total = i[1]['text'].split().__len__()\n",
    "        entities = [\"O\"]*total\n",
    "        for j in train_df[train_df['id'] == i[1]['id']].iterrows():\n",
    "            discourse = j[1]['discourse_type']\n",
    "            list_ix = [int(x) for x in j[1]['predictionstring'].split(' ')]\n",
    "            entities[list_ix[0]] = f\"B-{discourse}\"\n",
    "            for k in list_ix[1:]: entities[k] = f\"I-{discourse}\"\n",
    "        all_entities.append(entities)\n",
    "    train_text_df['entities'] = all_entities\n",
    "    train_text_df.to_csv(TRAIN_NER_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DICTIONARIES THAT WE CAN USE DURING TRAIN AND INFER\n",
    "output_labels = [\n",
    "    'O', 'B-Lead', 'I-Lead', 'B-Position', \n",
    "    'I-Position', 'B-Claim', 'I-Claim', \n",
    "    'B-Counterclaim', 'I-Counterclaim', 'B-Rebuttal', \n",
    "    'I-Rebuttal', 'B-Evidence', 'I-Evidence', \n",
    "    'B-Concluding Statement', 'I-Concluding Statement'\n",
    "]\n",
    "\n",
    "labels_to_ids = {v:k for k,v in enumerate(output_labels)}\n",
    "ids_to_labels = {k:v for k,v in enumerate(output_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_ALL_SUBTOKENS = True\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, get_wids):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.get_wids = get_wids #!# for validation\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # GET TEXT AND WORD LABELS \n",
    "        text = self.data.text[index]        \n",
    "        word_labels = self.data.entities[index] if not self.get_wids else None\n",
    "\n",
    "        # TOKENIZE TEXT\n",
    "        encoding = self.tokenizer(text.split(),\n",
    "                             is_split_into_words=True, #!#\n",
    "                             #return_offsets_mapping=True, \n",
    "                             padding='max_length', \n",
    "                             truncation=True, \n",
    "                             max_length=self.max_len)\n",
    "        \n",
    "        word_ids = encoding.word_ids()  \n",
    "        \n",
    "        # CREATE TARGETS\n",
    "        # null = -100\n",
    "        if not self.get_wids:\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:                            \n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:              \n",
    "                    label_ids.append( labels_to_ids[word_labels[word_idx]] )\n",
    "                else:\n",
    "                    if LABEL_ALL_SUBTOKENS:\n",
    "                        label_ids.append( labels_to_ids[word_labels[word_idx]] )\n",
    "                    else:\n",
    "                        label_ids.append(-100)\n",
    "                previous_word_idx = word_idx\n",
    "            encoding['labels'] = label_ids\n",
    "\n",
    "        # CONVERT TO TORCH TENSORS\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        if self.get_wids:\n",
    "            word_ids2 = [w if w is not None else -1 for w in word_ids]\n",
    "            item['wids'] = torch.as_tensor(word_ids2)\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15594 train texts. We will split 90% 10% for validation.\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE VALIDATION INDEXES\n",
    "IDS = train_df.id.unique()\n",
    "print('There are',len(IDS),'train texts. We will split 90% 10% for validation.')\n",
    "\n",
    "# TRAIN VALID SPLIT 90% 10%\n",
    "train_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\n",
    "valid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (15594, 3)\n",
      "TRAIN Dataset: (14034, 2)\n",
      "VALID Dataset: (1560, 3)\n"
     ]
    }
   ],
   "source": [
    "# CREATE TRAIN SUBSET AND VALID SUBSET\n",
    "data = train_text_df[['id','text', 'entities']]\n",
    "train_dataset = data.loc[data['id'].isin(IDS[train_idx]),['text', 'entities']].reset_index(drop=True)\n",
    "valid_dataset = data.loc[data['id'].isin(IDS[valid_idx])].reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"VALID Dataset: {}\".format(valid_dataset.shape))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DOWNLOADED_MODEL_PATH) \n",
    "train_set = dataset(train_dataset, tokenizer, config['max_length'], False)\n",
    "valid_set = dataset(valid_dataset, tokenizer, config['max_length'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN DATASET AND VALID DATASET\n",
    "train_params = {'batch_size': config['train_batch_size'],\n",
    "                'shuffle': True,\n",
    "                'num_workers': 2,\n",
    "                'pin_memory':True\n",
    "                }\n",
    "\n",
    "valid_params = {'batch_size': config['valid_batch_size'],\n",
    "                'shuffle': False,\n",
    "                'num_workers': 2,\n",
    "                'pin_memory':True\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(train_set, **train_params)\n",
    "validing_loader = DataLoader(valid_set, **valid_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    # tr_preds, tr_labels = [], []\n",
    "    \n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        \n",
    "        ids = batch['input_ids'].to(config['device'], dtype = torch.long)\n",
    "        mask = batch['attention_mask'].to(config['device'], dtype = torch.long)\n",
    "        labels = batch['labels'].to(config['device'], dtype = torch.long)\n",
    "\n",
    "        loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels,\n",
    "                               return_dict=False)\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "        \n",
    "        if idx % 200==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss after {idx:04d} training steps: {loss_step}\")\n",
    "           \n",
    "        # compute training accuracy\n",
    "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        \n",
    "        # only compute accuracy at active labels\n",
    "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
    "        \n",
    "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        #tr_labels.extend(labels)\n",
    "        #tr_preds.extend(predictions)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=config['max_grad_norm']\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu\n",
    "\n",
    "# VERSION FOR SAVING MODEL WEIGHTS\n",
    "VER=26\n",
    "\n",
    "# IF VARIABLE IS NONE, THEN NOTEBOOK COMPUTES TOKENS\n",
    "# OTHERWISE NOTEBOOK LOADS TOKENS FROM PATH\n",
    "LOAD_TOKENS_FROM = '../input/py-bigbird-v26'\n",
    "\n",
    "# IF VARIABLE IS NONE, THEN NOTEBOOK TRAINS A NEW MODEL\n",
    "# OTHERWISE IT LOADS YOUR PREVIOUSLY TRAINED MODEL\n",
    "LOAD_MODEL_FROM = '../input/py-bigbird-v26'\n",
    "\n",
    "# IF FOLLOWING IS NONE, THEN NOTEBOOK \n",
    "# USES INTERNET AND DOWNLOADS HUGGINGFACE \n",
    "# CONFIG, TOKENIZER, AND MODEL\n",
    "DOWNLOADED_MODEL_PATH = '../input/py-bigbird-v26' \n",
    "\n",
    "if DOWNLOADED_MODEL_PATH is None:\n",
    "    DOWNLOADED_MODEL_PATH = 'model'    \n",
    "MODEL_NAME = 'google/bigbird-roberta-base'\n",
    "\n",
    "config = {'model_name': MODEL_NAME,   \n",
    "          'max_length': 1024,\n",
    "          'train_batch_size':4,\n",
    "          'valid_batch_size':4,\n",
    "          'epochs':5,\n",
    "          'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n",
    "          'max_grad_norm':10,\n",
    "          'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n",
    "\n",
    "# THIS WILL COMPUTE VAL SCORE DURING COMMIT BUT NOT DURING SUBMIT\n",
    "COMPUTE_VAL_SCORE = True\n",
    "if len( os.listdir('../input/feedback-prize-2021/test') )>5:\n",
    "    COMPUTE_VAL_SCORE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "\n",
    "# code for submission\n",
    "# ref) https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615?scriptVersionId=83230719\n",
    "test_names, test_texts = [], []\n",
    "for f in list(os.listdir('../input/feedback-prize-2021/test')):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    test_texts.append(open('../input/feedback-prize-2021/test/' + f, 'r').read())\n",
    "test_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\n",
    "\n",
    "test_names, train_texts = [], []\n",
    "for f in tqdm(list(os.listdir('../input/feedback-prize-2021/train'))):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    train_texts.append(open('../input/feedback-prize-2021/train/' + f, 'r').read())\n",
    "train_text_df = pd.DataFrame({'id': test_names, 'text': train_texts})\n",
    "\n",
    "\n",
    "# # TEST DATASET\n",
    "# test_texts_set = dataset(test_texts, tokenizer, config['max_length'], True)\n",
    "# test_texts_loader = DataLoader(test_texts_set, **test_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snu36",
   "language": "python",
   "name": "snu36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
