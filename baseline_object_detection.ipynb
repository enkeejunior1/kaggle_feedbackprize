{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat, reduce\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmain ref) https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615?scriptVersionId=83230719\\ndetr ref) https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb\\ndetr ref) https://www.kaggle.com/tanulsingh077/end-to-end-object-detection-with-transformers-detr\\n코드 실행 전 준비 사항 :\\n    1) 먼저 kaggle 에서 데이터를 다운받고, './input' 에 압축해제 시켜준다.\\n    2) detr 을 fork 해준다. # !git clone https://github.com/facebookresearch/detr.git\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "main ref) https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615?scriptVersionId=83230719\n",
    "detr ref) https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb\n",
    "detr ref) https://www.kaggle.com/tanulsingh077/end-to-end-object-detection-with-transformers-detr\n",
    "코드 실행 전 준비 사항 :\n",
    "    1) 먼저 kaggle 에서 데이터를 다운받고, './input' 에 압축해제 시켜준다.\n",
    "    2) detr 을 fork 해준다. # !git clone https://github.com/facebookresearch/detr.git\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object detection 문제로 전처리하기\n",
    "# objective : use DETR structure for sentence segmentation.\n",
    "PATH = os.path.join(os.getcwd(), 'input')\n",
    "TRAIN_NER_PATH_DETR = os.path.join(PATH, 'train_detr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "#!# test code for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER label 로 전처리한 데이터 불러오기\n",
    "# 만약 starting class 를 원하지 않는다면 이하 코드를 실행할 것.\n",
    "\n",
    "try:\n",
    "    from ast import literal_eval\n",
    "    train_text_df = pd.read_csv(TRAIN_NER_PATH_DETR)\n",
    "    \n",
    "    # pandas saves lists as string, we must convert back\n",
    "    from ast import literal_eval\n",
    "    train_text_df.segment_label = train_text_df.segment_label.apply(lambda x: literal_eval(x))\n",
    "    \n",
    "    original_train_df = pd.read_csv(os.path.join(PATH, 'train.csv'))\n",
    "    \n",
    "except:\n",
    "    print('this is 1st time to run this code...')\n",
    "    print('try to convert original text to DETR labels...')\n",
    "    # read original text files0\n",
    "    train_ids, train_texts = [], []\n",
    "    for f in tqdm(list(os.listdir(os.path.join(PATH, 'train')))):\n",
    "        train_ids.append(f.replace('.txt', ''))\n",
    "        train_texts.append(open(os.path.join(PATH, 'train', f), 'r').read())\n",
    "    train_text_df = pd.DataFrame({'id': train_ids, 'text': train_texts})\n",
    "\n",
    "    # convert segment label into object detection label : [segment_type, x, y]\n",
    "    original_train_df = pd.read_csv(os.path.join(PATH, 'train.csv'))\n",
    "    label_list = []\n",
    "    for i, text_df in tqdm(train_text_df.iterrows()):\n",
    "        total = text_df['text'].split().__len__()\n",
    "        segment_label_list = []\n",
    "        for j, segment_df in original_train_df[original_train_df['id'] == text_df['id']].iterrows():\n",
    "            segment_label = [\n",
    "                segment_df['discourse_type'],\n",
    "                int(segment_df['predictionstring'].split(' ')[0]), \n",
    "                int(segment_df['predictionstring'].split(' ')[-1])\n",
    "            ]\n",
    "            segment_label_list.append(segment_label)\n",
    "\n",
    "        label_list.append(segment_label_list)\n",
    "\n",
    "    train_text_df['segment_label'] = label_list\n",
    "    train_text_df.to_csv(TRAIN_NER_PATH_DETR, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DICTIONARIES THAT WE CAN USE DURING TRAIN AND INFER\n",
    "output_labels_detr = [\n",
    "    'O', # detr need dummy class for padding\n",
    "    'Lead', \n",
    "    'Position', \n",
    "    'Claim', \n",
    "    'Counterclaim', \n",
    "    'Rebuttal', \n",
    "    'Evidence', \n",
    "    'Concluding Statement'\n",
    "]\n",
    "\n",
    "labels_to_ids = {v:k for k,v in enumerate(output_labels_detr)}\n",
    "ids_to_labels = {k:v for k,v in enumerate(output_labels_detr)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15594 train texts. We will split 90% 10% for validation.\n",
      "FULL Dataset: (15594, 3)\n",
      "TRAIN Dataset: (14034, 2)\n",
      "VALID Dataset: (1560, 3)\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE VALIDATION INDEXES\n",
    "IDS = original_train_df.id.unique()\n",
    "print('There are',len(IDS),'train texts. We will split 90% 10% for validation.')\n",
    "\n",
    "# TRAIN VALID SPLIT 90% 10%\n",
    "train_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\n",
    "valid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\n",
    "\n",
    "# CREATE TRAIN SUBSET AND VALID SUBSET\n",
    "data_df = train_text_df[['id','text', 'segment_label']]\n",
    "train_df = data_df.loc[data_df['id'].isin(IDS[train_idx]),['text', 'segment_label']].reset_index(drop=True)\n",
    "valid_df = data_df.loc[data_df['id'].isin(IDS[valid_idx])].reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
    "print(\"VALID Dataset: {}\".format(valid_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Venus', 'is', 'a', 'worthy', 'pursuit', 'despite', 'the', 'dangers.']\n",
      "39834    Venus is a worthy pursuit despite the dangers \n",
      "Name: discourse_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "'''test code for preprocessing'''\n",
    "i = 1\n",
    "j = 0\n",
    "\n",
    "# pre-processed\n",
    "label, start_idx, end_idx = data_df['segment_label'][i][j]\n",
    "text_id = data_df['id'][i]\n",
    "print(data_df['text'][i].split()[start_idx:end_idx+1])\n",
    "\n",
    "# original\n",
    "original_text = original_train_df[original_train_df['id'] == text_id]\n",
    "print(original_text[original_text['discourse_type'] == label]['discourse_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>segment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7301B174090E</td>\n",
       "      <td>I believe that a B average would be a good thi...</td>\n",
       "      <td>[[Position, 0, 14], [Claim, 32, 47], [Counterc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3799E21B6EC3</td>\n",
       "      <td>Venus is a worthy pursuit despite the dangers....</td>\n",
       "      <td>[[Position, 0, 7], [Claim, 8, 28], [Evidence, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29C5DBB0A339</td>\n",
       "      <td>Limiting car usage will have many advantages. ...</td>\n",
       "      <td>[[Position, 0, 6], [Claim, 11, 12], [Claim, 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1613BD216385</td>\n",
       "      <td>\"Making Mona Lisa Smile\" is about a computer h...</td>\n",
       "      <td>[[Lead, 0, 39], [Position, 40, 64], [Evidence,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D4A3E7EC982E</td>\n",
       "      <td>In this essay i will be explaining the differe...</td>\n",
       "      <td>[[Lead, 0, 22], [Position, 23, 33], [Claim, 34...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  \\\n",
       "0  7301B174090E  I believe that a B average would be a good thi...   \n",
       "1  3799E21B6EC3  Venus is a worthy pursuit despite the dangers....   \n",
       "2  29C5DBB0A339  Limiting car usage will have many advantages. ...   \n",
       "3  1613BD216385  \"Making Mona Lisa Smile\" is about a computer h...   \n",
       "4  D4A3E7EC982E  In this essay i will be explaining the differe...   \n",
       "\n",
       "                                       segment_label  \n",
       "0  [[Position, 0, 14], [Claim, 32, 47], [Counterc...  \n",
       "1  [[Position, 0, 7], [Claim, 8, 28], [Evidence, ...  \n",
       "2  [[Position, 0, 6], [Claim, 11, 12], [Claim, 14...  \n",
       "3  [[Lead, 0, 39], [Position, 40, 64], [Evidence,...  \n",
       "4  [[Lead, 0, 22], [Position, 23, 33], [Claim, 34...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 이 잘 작동하는지 확인하는 코드\n",
    "# #!# 로 표지된 index 를 바꿔주면 해당 dataset_row 에 대해서 전처리된 라벨과 실제 라벨에서 다른 부분을 출력해준다.\n",
    "\n",
    "# data = data_df\n",
    "# is_train = True\n",
    "\n",
    "# index = 2 #!# 바꾸면서 다양한 시도 해보기\n",
    "\n",
    "# text = data.text[index]        \n",
    "# text_id = data.id[index]\n",
    "# segment_label_list = data.segment_label[index] if is_train else None\n",
    "\n",
    "# # TOKENIZE TEXT\n",
    "# encoding = tokenizer(\n",
    "#     text.split(),\n",
    "#     is_split_into_words=True,\n",
    "#     padding='max_length', #!# need to check exist seq s.t. longer than 4094\n",
    "#     truncation=True, #!# need to check exist seq s.t. longer than 4094\n",
    "#     max_length=500\n",
    "# )\n",
    "        \n",
    "# word_ids = encoding.word_ids()\n",
    "\n",
    "# segment_ids_list = [[labels_to_ids[label], start_idx, end_idx] for label, start_idx, end_idx in segment_label_list]\n",
    "\n",
    "# processed_list = []\n",
    "# for ids, start_idx, end_idx in segment_ids_list:\n",
    "#     start_word_ids = word_ids.index(start_idx)\n",
    "#     end_word_ids = word_ids.index(end_idx)\n",
    "    \n",
    "#     processed_list.append(tokenizer.decode(encoding.input_ids[start_word_ids:end_word_ids+1]))\n",
    "    \n",
    "# original_list = list(train_df[train_df['id'] == text_id]['discourse_text'])\n",
    "\n",
    "# is_same = True\n",
    "# for p_discourse, o_discourse in zip(processed_list, original_list):\n",
    "#     if p_discourse.split() == o_discourse.split():\n",
    "#         continue\n",
    "        \n",
    "#     else: \n",
    "#         is_same = False\n",
    "#         for p, o in zip(p_discourse.split(), o_discourse.split()):\n",
    "#             if p != o:\n",
    "#                 print(p, o)\n",
    "# if is_same:\n",
    "#     print('every token in the label is same.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''baseline : ignore \\n\\n, 문장 기호들'''\n",
    "#!# 문장 기호는 상당히 중요한 정보를 담고 있어서 처리해주고 싶은데.. \n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, is_train):\n",
    "        super(dataset, self).__init__()\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.is_train = is_train # if test (or validation) period, we won't use word label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        global max_segment\n",
    "        # GET TEXT AND WORD LABELS \n",
    "        text = self.data.text[index]        \n",
    "        segment_label_list = self.data.segment_label[index] if self.is_train else None\n",
    "\n",
    "        # TOKENIZE TEXT\n",
    "        encoding = self.tokenizer(\n",
    "            text.split(),\n",
    "            is_split_into_words=True,\n",
    "            return_offsets_mapping=False, #!# how to use it for enabling tokenizer to \"see\" \\n\\n?\n",
    "            padding='max_length', #!# need to check exist seq s.t. longer than 4094\n",
    "            truncation=True, #!# need to check exist seq s.t. longer than 4094\n",
    "            max_length=self.max_len\n",
    "        )\n",
    "        \n",
    "        word_ids = encoding.word_ids()\n",
    "        \n",
    "        # CREATE TARGETS\n",
    "        #!# detr label padding 구현 : x, y 정보는 어떻게 넣어주는가? 0이어도 되나, random 이 더 좋으려나\n",
    "        if self.is_train:\n",
    "            segment_ids_list = torch.as_tensor([[labels_to_ids[label], start_idx, end_idx] for label, start_idx, end_idx in segment_label_list]) # [num_seg, 3]\n",
    "            segment_ids_pad  = torch.zeros(max_segment - segment_ids_list.size(0), segment_ids_list.size(1)) # [max_seg - num_seg, 3]\n",
    "            segment_ids_list = torch.cat((segment_ids_list, segment_ids_pad), dim = 0) # [max_seg, 3]\n",
    "            encoding['labels'] = segment_ids_list #!# .type(torch.LongTensor) # class, bound box must be long tensor\n",
    "\n",
    "        # CONVERT TO TORCH TENSORS\n",
    "        item = {k: torch.as_tensor(v) for k, v in encoding.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref) https://github.com/facebookresearch/detr 를 참고했으나, review 필요함.\n",
    "class DetrHead(nn.Module):\n",
    "    def __init__(self, feature_extractor, transformer, prediction_head, pos_emb, max_seq, max_segment, d_model):\n",
    "        super(DetrHead, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transformer = transformer\n",
    "        self.prediction_head = prediction_head\n",
    "        \n",
    "        self.feature_pos = pos_emb # absolute positional encoding (sinusodial, attention is all you need)\n",
    "        self.query_pos = nn.Parameter(torch.rand(max_segment, d_model))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.feature_extractor(x) # x -> [b, s, d_model]\n",
    "        out = self.transformer(out + self.feature_pos(out), repeat(self.query_pos, 'i j -> b i j', b = out.size(0))) # [b, s, d_model]\n",
    "        out = self.prediction_head(out)\n",
    "        return out\n",
    "    \n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, lm):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.lm = lm\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.lm(input_ids = x['input_ids'], attention_mask = x['attention_mask']).last_hidden_state #!# todo : try other layer\n",
    "        return out\n",
    "    \n",
    "class Transformer(nn.Module): #!# todo : change transformer structure with user defined transformer structure\n",
    "    def __init__(self, d_model, nhead = 12, num_encoder_layers = 2, num_decoder_layers = 2, dim_feedforward = 256):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.transformer = nn.Transformer( # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "            d_model, \n",
    "            nhead = nhead, \n",
    "            num_encoder_layers = num_encoder_layers, \n",
    "            num_decoder_layers = num_decoder_layers, \n",
    "            dim_feedforward = dim_feedforward,\n",
    "            batch_first = True\n",
    "        ) \n",
    "        \n",
    "    def forward(self, f, q):\n",
    "        out = self.transformer(f, q)\n",
    "        return out\n",
    "\n",
    "class PredictionHead(nn.Module): #!# todo : try diff. prediction head\n",
    "    def __init__(self, d_model, num_class):\n",
    "        super(PredictionHead, self).__init__()\n",
    "        self.fc_layer_class = nn.Linear(d_model, num_class + 1) # +1 for null class\n",
    "        self.fc_layer_segment = nn.Linear(d_model, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        c = self.fc_layer_class(x)\n",
    "        b = self.fc_layer_segment(x)\n",
    "        return (c, b)\n",
    "    \n",
    "import math\n",
    "class PositionalEmbedding(nn.Module): #!# ref) https://github.com/codertimo/BERT-pytorch\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model, requires_grad = False).float()\n",
    "        pos = torch.arange(0, max_len).float()\n",
    "        div = (-(torch.arange(0, d_model, 2).float() / d_model) * math.log(10000.0)).exp()\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(torch.einsum('i,j->ij', pos, div))\n",
    "        pe[:, 1::2] = torch.cos(torch.einsum('i,j->ij', pos, div))\n",
    "        pe = rearrange(pe, 'i j -> () i j')\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test code (model)\n",
    "\n",
    "모든 부분을 믿지 마라. 확인할 수 있는 코드를 만드는 것도 능력이다.\n",
    "\n",
    "작업중..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76f5cbf445b443abdc088f87f4bc283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "lm = AutoModel.from_pretrained('bert-base-uncased') #!# 제출을 위해 local 에 다운받는 과정 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset(train_df, tokenizer, lm.config.max_position_embeddings, is_train = True)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model parameter'''\n",
    "max_seq = lm.config.max_position_embeddings\n",
    "d_model = lm.config.hidden_size\n",
    "\n",
    "max_segment = 20\n",
    "num_class = len(output_labels_detr)\n",
    "\n",
    "FEATURE_EXTRACTOR = FeatureExtractor(lm)\n",
    "TRANSFORMER = Transformer(d_model)\n",
    "PREDICTION_HEAD = PredictionHead(d_model, num_class)\n",
    "PE = PositionalEmbedding(d_model, max_seq)\n",
    "DETR_HEAD = DetrHead(FEATURE_EXTRACTOR, TRANSFORMER, PREDICTION_HEAD, PE, max_seq, max_segment, d_model)\n",
    "\n",
    "for batch in train_loader:\n",
    "    break\n",
    "\n",
    "batch = {k : v.to(device) for k, v in batch.items()}\n",
    "DETR_HEAD.to(device)\n",
    "\n",
    "out = DETR_HEAD(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "# fork ref) https://github.com/facebookresearch/detr/blob/091a817eca74b8b97e35e4531c1c39f89fbe38eb/models/detr.py#L83\n",
    "# code ref) https://www.kaggle.com/tanulsingh077/end-to-end-object-detection-with-transformers-detr\n",
    "\n",
    "import sys\n",
    "sys.path.append('./detr/')\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline 이 작동하는지 확인하기 위해 bert-base 활용\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "lm = AutoModel.from_pretrained('bert-base-uncased') #!# 제출을 위해 local 에 다운받는 과정 필요.\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('./model')\n",
    "# lm = AutoModel.from_pretrained('google/bigbird-roberta-base') #!# 제출을 위해 local 에 다운받는 과정 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model parameter'''\n",
    "max_seq = lm.config.max_position_embeddings\n",
    "d_model = lm.config.hidden_size\n",
    "\n",
    "max_segment = 20\n",
    "num_class = len(output_labels_detr)\n",
    "\n",
    "FEATURE_EXTRACTOR = FeatureExtractor(lm)\n",
    "TRANSFORMER = Transformer(d_model)\n",
    "PREDICTION_HEAD = PredictionHead(d_model, num_class)\n",
    "PE = PositionalEmbedding(d_model, max_seq)\n",
    "DETR_HEAD = DetrHead(FEATURE_EXTRACTOR, TRANSFORMER, PREDICTION_HEAD, PE, max_seq, max_segment, d_model)\n",
    "\n",
    "model = DETR_HEAD.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break\n",
    "\n",
    "batch = {k : v.to(device) for k, v in batch.items()}\n",
    "\n",
    "out = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_class, label_pos = batch['labels'][:, :, 0].to(device), batch['labels'][:, :, 1:].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detr.models.matcher import HungarianMatcher\n",
    "from detr.models.detr import SetCriterion\n",
    "\n",
    "matcher = HungarianMatcher()\n",
    "weight_dict = weight_dict = {'loss_ce': 1, 'loss_bbox': 1 , 'loss_giou': 1}\n",
    "null_class_coef = 0.5\n",
    "losses = ['labels', 'boxes', 'cardinality']\n",
    "\n",
    "criterion = SetCriterion(num_class-1, matcher, weight_dict, eos_coef = null_class_coef, losses=losses)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-5\n",
    "\n",
    "import torch.optim\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = 1e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "\n",
    "summary_loss = AverageMeter()\n",
    "\n",
    "model.train()\n",
    "criterion.train()\n",
    "\n",
    "for batch in train_loader:\n",
    "    break\n",
    "\n",
    "batch = {k : v.to(device) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, b = model(batch)\n",
    "out = {'pred_logits' : c, 'pred_boxes' : b}\n",
    "tgt = {'labels' : batch['labels'][:, :, 0], 'boxes' : batch['labels'][:, :, 1:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-26470dcc7bb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "outputs = out\n",
    "targets = tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!# 여기부터 facebook source code 를 변화시키고 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20, 2])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"pred_boxes\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_query = outputs[\"pred_logits\"].shape[:2]\n",
    "\n",
    "# We flatten to compute the cost matrices in a batch #!#\n",
    "out_prob = rearrange(outputs[\"pred_logits\"], 'b s c -> (b s) c').softmax(dim = -1)  # [batch_size * num_queries, num_classes]\n",
    "out_bbox = rearrange(outputs[\"pred_boxes\"], 'b s box -> (b s) box')  # [batch_size * num_queries, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also concat the target labels and boxes\n",
    "tgt_ids  = torch.cat([v for v in targets[\"labels\"].type(torch.LongTensor)]).to(device)\n",
    "tgt_bbox = torch.cat([v for v in targets[\"boxes\"]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
    "# but approximate it in 1 - proba[target class].\n",
    "# The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
    "cost_class = -out_prob[:, tgt_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the L1 cost between boxes\n",
    "#!# 이거 normalize 안돼서 너무 클거임. 1/n_seq 해주는게 좋을 듯\n",
    "cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1) # [batch_size * num_queries, batch_size * num_queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the giou cost betwen boxes\n",
    "#!# 분명 overflow 문제 발생함.. 어떻게 해결할 수 있을까. detr 은 assert 사용함\n",
    "def one_dim_iou(out_bbox, tgt_bbox):\n",
    "    max_matrix = torch.max(\n",
    "    repeat(out_bbox, 'bq i -> bq r i', r = out_bbox.size(0)), \n",
    "    repeat(tgt_bbox, 'bq i -> r bq i', r = out_bbox.size(0))\n",
    "    )\n",
    "\n",
    "    min_matrix = torch.min(\n",
    "        repeat(out_bbox, 'bq i -> bq r i', r = out_bbox.size(0)), \n",
    "        repeat(tgt_bbox, 'bq i -> r bq i', r = out_bbox.size(0))\n",
    "    )\n",
    "    \n",
    "    return (min_matrix[:, :, 1] - max_matrix[:, :, 0]) / (max_matrix[:, :, 1] - min_matrix[:, :, 0])\n",
    "    \n",
    "cost_giou = -one_dim_iou(out_bbox, tgt_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 40])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_giou.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cost matrix\n",
    "#!# need to change weight for loss\n",
    "C = 1 * cost_bbox + 1 * cost_class + 1 * cost_giou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 40])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = rearrange(C, '(b1 q1) (b2 q2) -> b1 q1 b2 q2', b1 = 2, q1 = 20, b2 = 2, q2 = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "C[0, :, 0, :] # b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C[1, :, 1, :] # b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in C.split(sizes, -1)[0]:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19]),\n",
       " array([ 6,  9, 10, 12,  0, 14, 15,  1, 16, 17, 18, 19,  8,  7,  3,  2,  4,\n",
       "        13, 11,  5]))"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_sum_assignment(C[0, :, 0, :].detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [linear_sum_assignment(C[b, :, b, :].detach().cpu()) for b in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-cc8e2fd19212>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# but approximate it in 1 - proba[target class].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# The 1 is a constant that doesn't change the matching, it can be ommitted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcost_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mout_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Compute the L1 cost between boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tensors used as indices must be long, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HungarianMatcher(nn.Module):\n",
    "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
    "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
    "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
    "    while the others are un-matched (and thus treated as non-objects).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cost_class: float = 1, cost_bbox: float = 1, cost_giou: float = 1):\n",
    "        \"\"\"Creates the matcher\n",
    "        Params:\n",
    "            cost_class: This is the relative weight of the classification error in the matching cost\n",
    "            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n",
    "            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_bbox = cost_bbox\n",
    "        self.cost_giou = cost_giou\n",
    "        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, \"all costs cant be 0\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" Performs the matching\n",
    "        Params:\n",
    "            outputs: This is a dict that contains at least these entries:\n",
    "                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
    "                 \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n",
    "            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
    "                 \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n",
    "                           objects in the target) containing the class labels\n",
    "                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n",
    "        Returns:\n",
    "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
    "                - index_i is the indices of the selected predictions (in order)\n",
    "                - index_j is the indices of the corresponding selected targets (in order)\n",
    "            For each batch element, it holds:\n",
    "                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
    "        \"\"\"\n",
    "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
    "\n",
    "        # We flatten to compute the cost matrices in a batch\n",
    "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
    "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
    "\n",
    "        # Also concat the target labels and boxes\n",
    "        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
    "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
    "\n",
    "        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
    "        # but approximate it in 1 - proba[target class].\n",
    "        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
    "        cost_class = -out_prob[:, tgt_ids]\n",
    "\n",
    "        # Compute the L1 cost between boxes\n",
    "        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
    "\n",
    "        # Compute the giou cost betwen boxes\n",
    "        cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))\n",
    "\n",
    "        # Final cost matrix\n",
    "        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou\n",
    "        C = C.view(bs, num_queries, -1).cpu()\n",
    "\n",
    "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
    "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-51aa34d32139>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle_feedbackprize/detr/models/matcher.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Also concat the target labels and boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mtgt_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mtgt_bbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle_feedbackprize/detr/models/matcher.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Also concat the target labels and boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mtgt_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mtgt_bbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "matcher(out, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 0, 0, 0, 5, 0, 0, 0, 0, 0, 8, 2, 0, 2, 0, 5, 2, 3, 5, 0],\n",
       "        [8, 5, 0, 5, 5, 2, 0, 2, 0, 5, 0, 8, 5, 2, 3, 2, 2, 3, 0, 8]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(c, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-86b363f3db1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle_feedbackprize/detr/models/detr.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m    220\u001b[0m                       \u001b[0mThe\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meach\u001b[0m \u001b[0mdict\u001b[0m \u001b[0mdepends\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msee\u001b[0m \u001b[0meach\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;31m'\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0moutputs_without_aux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'aux_outputs'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# Retrieve the matching between the outputs of the last layer and the targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "loss_dict = criterion(torch.argmax(c, dim = -1), label_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dict = criterion.weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN DATASET AND VALID DATASET\n",
    "train_params = {'batch_size': config['train_batch_size'], 'shuffle': True}\n",
    "\n",
    "train_loader = DataLoader(train_dataset, **train_params, \n",
    "                             # collate_fn = collate_batch\n",
    "                            )\n",
    "\n",
    "# valid_params = {'batch_size': config['valid_batch_size'],\n",
    "#                 'shuffle': False,\n",
    "#                 'num_workers': 1,\n",
    "#                 'pin_memory':True\n",
    "#                 }\n",
    "\n",
    "# validing_loader = DataLoader(valid_dataset, **valid_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in training_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    # tr_preds, tr_labels = [], []\n",
    "    \n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        \n",
    "        ids = batch['input_ids'].to(config['device'], dtype = torch.long)\n",
    "        mask = batch['attention_mask'].to(config['device'], dtype = torch.long)\n",
    "        labels = batch['labels'].to(config['device'], dtype = torch.long)\n",
    "\n",
    "        loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels, return_dict=False)\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "        \n",
    "        if idx % 200==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss after {idx:04d} training steps: {loss_step}\")\n",
    "           \n",
    "        # compute training accuracy\n",
    "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        \n",
    "        # only compute accuracy at active labels\n",
    "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
    "        \n",
    "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        #tr_labels.extend(labels)\n",
    "        #tr_preds.extend(predictions)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=config['max_grad_norm']\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE MODEL\n",
    "config_model = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'/config.json') \n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    DOWNLOADED_MODEL_PATH+'/pytorch_model.bin',\n",
    "    config=config_model\n",
    ")\n",
    "model.to(config['device'])\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=config['learning_rates'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOP TO TRAIN MODEL (or load model)\n",
    "if not LOAD_MODEL_FROM:\n",
    "    for epoch in range(config['epochs']):\n",
    "        \n",
    "        print(f\"### Training epoch: {epoch + 1}\")\n",
    "        for g in optimizer.param_groups: \n",
    "            g['lr'] = config['learning_rates'][epoch]\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'### LR = {lr}\\n')\n",
    "        \n",
    "        train(epoch)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    torch.save(model.state_dict(), f'bigbird_v{VER}.pt')\n",
    "    \n",
    "else:\n",
    "    model.load_state_dict(torch.load(f'{LOAD_MODEL_FROM}/bigbird_v{VER}.pt'))\n",
    "    print('Model loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for submission\n",
    "# ref) https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615?scriptVersionId=83230719\n",
    "test_names, test_texts = [], []\n",
    "for f in list(os.listdir('../input/feedback-prize-2021/test')):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    test_texts.append(open('../input/feedback-prize-2021/test/' + f, 'r').read())\n",
    "test_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\n",
    "\n",
    "test_names, train_texts = [], []\n",
    "for f in tqdm(list(os.listdir('../input/feedback-prize-2021/train'))):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    train_texts.append(open('../input/feedback-prize-2021/train/' + f, 'r').read())\n",
    "train_text_df = pd.DataFrame({'id': test_names, 'text': train_texts})\n",
    "\n",
    "\n",
    "# # TEST DATASET\n",
    "# test_texts_set = dataset(test_texts, tokenizer, config['max_length'], True)\n",
    "# test_texts_loader = DataLoader(test_texts_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define config\n",
    "config = {'model_name': MODEL_NAME,   \n",
    "          'max_length': 1024,\n",
    "          'train_batch_size':4,\n",
    "          'valid_batch_size':4,\n",
    "          'epochs':5,\n",
    "          'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n",
    "          'max_grad_norm':10,\n",
    "          'device': 'cuda' if cuda.is_available() else 'cpu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666cc845bdc54244acb9aff81cef5626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51674fa78df049babf1e80f7e1b66fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e6301eb1b248a7aa55f1f9062defcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/846k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055c9165c0c64092805d94e89973a1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/775 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9508107e9dd543fa9c6ed48270f8d929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdForTokenClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# if you are first running this code, please download LM.\n",
    "MODEL_NAME = 'google/bigbird-roberta-base' # choose which model to download\n",
    "DOWNLOADED_MODEL_PATH = 'model'            # choose where to download the model\n",
    "\n",
    "if DOWNLOADED_MODEL_PATH == 'model':\n",
    "    os.mkdir('model')\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True) #!# add_prefix_space?\n",
    "    tokenizer.save_pretrained('model')\n",
    "\n",
    "    config_model = AutoConfig.from_pretrained(MODEL_NAME) \n",
    "    config_model.num_labels = 15\n",
    "    config_model.save_pretrained('model')\n",
    "\n",
    "    backbone = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, \n",
    "                                                               config=config_model)\n",
    "    backbone.save_pretrained('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!# implement tokenizer \\n\\n, to see paragraph information\n",
    "# ref) https://github.com/huggingface/tokenizers/issues/247\n",
    "# ref) https://www.kaggle.com/c/feedback-prize-2021/discussion/296713\n",
    "tokenizer.decode(tokenizer(r'\\\\n\\\\n', return_offsets_mapping=True)['input_ids'])\n",
    "\n",
    "'''add new special token to model and tokenizer'''\n",
    "special_tokens_dict = {'additional_special_tokens': [r'\\\\n\\\\n']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i) \n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:                \n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            else:  \n",
    "                label_ids.append(label[word_idx])\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snu36",
   "language": "python",
   "name": "snu36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
