{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat, reduce\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmain ref) https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615?scriptVersionId=83230719\\ndetr ref) https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb\\ndetr ref) https://www.kaggle.com/tanulsingh077/end-to-end-object-detection-with-transformers-detr\\n코드 실행 전 준비 사항 :\\n    1) 먼저 kaggle 에서 데이터를 다운받고, './input' 에 압축해제 시켜준다.\\n    2) detr 을 fork 해준다. # !git clone https://github.com/facebookresearch/detr.git\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "main ref) https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615?scriptVersionId=83230719\n",
    "detr ref) https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb\n",
    "detr ref) https://www.kaggle.com/tanulsingh077/end-to-end-object-detection-with-transformers-detr\n",
    "코드 실행 전 준비 사항 :\n",
    "    1) 먼저 kaggle 에서 데이터를 다운받고, './input' 에 압축해제 시켜준다.\n",
    "    2) detr 을 fork 해준다. # !git clone https://github.com/facebookresearch/detr.git\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object detection 문제로 전처리하기\n",
    "# objective : use DETR structure for sentence segmentation.\n",
    "PATH = os.path.join(os.getcwd(), 'input')\n",
    "TRAIN_NER_PATH_DETR = os.path.join(PATH, 'train_detr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "#!# test code for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER label 로 전처리한 데이터 불러오기\n",
    "# 만약 starting class 를 원하지 않는다면 이하 코드를 실행할 것.\n",
    "\n",
    "try:\n",
    "    from ast import literal_eval\n",
    "    train_text_df = pd.read_csv(TRAIN_NER_PATH_DETR)\n",
    "    \n",
    "    # pandas saves lists as string, we must convert back\n",
    "    from ast import literal_eval\n",
    "    train_text_df.segment_label = train_text_df.segment_label.apply(lambda x: literal_eval(x))\n",
    "    \n",
    "    original_train_df = pd.read_csv(os.path.join(PATH, 'train.csv'))\n",
    "    \n",
    "except:\n",
    "    print('this is 1st time to run this code...')\n",
    "    print('try to convert original text to DETR labels...')\n",
    "    # read original text files0\n",
    "    train_ids, train_texts = [], []\n",
    "    for f in tqdm(list(os.listdir(os.path.join(PATH, 'train')))):\n",
    "        train_ids.append(f.replace('.txt', ''))\n",
    "        train_texts.append(open(os.path.join(PATH, 'train', f), 'r').read())\n",
    "    train_text_df = pd.DataFrame({'id': train_ids, 'text': train_texts})\n",
    "\n",
    "    # convert segment label into object detection label : [segment_type, x, y]\n",
    "    original_train_df = pd.read_csv(os.path.join(PATH, 'train.csv'))\n",
    "    label_list = []\n",
    "    for i, text_df in tqdm(train_text_df.iterrows()):\n",
    "        total = text_df['text'].split().__len__()\n",
    "        segment_label_list = []\n",
    "        for j, segment_df in original_train_df[original_train_df['id'] == text_df['id']].iterrows():\n",
    "            segment_label = [\n",
    "                segment_df['discourse_type'],\n",
    "                int(segment_df['predictionstring'].split(' ')[0]), \n",
    "                int(segment_df['predictionstring'].split(' ')[-1])\n",
    "            ]\n",
    "            segment_label_list.append(segment_label)\n",
    "\n",
    "        label_list.append(segment_label_list)\n",
    "\n",
    "    train_text_df['segment_label'] = label_list\n",
    "    train_text_df.to_csv(TRAIN_NER_PATH_DETR, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DICTIONARIES THAT WE CAN USE DURING TRAIN AND INFER\n",
    "output_labels_detr = [\n",
    "    'O', # detr need dummy class for padding\n",
    "    'Lead', \n",
    "    'Position', \n",
    "    'Claim', \n",
    "    'Counterclaim', \n",
    "    'Rebuttal', \n",
    "    'Evidence', \n",
    "    'Concluding Statement'\n",
    "]\n",
    "\n",
    "labels_to_ids = {v:k for k,v in enumerate(output_labels_detr)}\n",
    "ids_to_labels = {k:v for k,v in enumerate(output_labels_detr)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15594 train texts. We will split 90% 10% for validation.\n",
      "FULL Dataset: (15594, 3)\n",
      "TRAIN Dataset: (14034, 2)\n",
      "VALID Dataset: (1560, 3)\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE VALIDATION INDEXES\n",
    "IDS = original_train_df.id.unique()\n",
    "print('There are',len(IDS),'train texts. We will split 90% 10% for validation.')\n",
    "\n",
    "# TRAIN VALID SPLIT 90% 10%\n",
    "train_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\n",
    "valid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\n",
    "\n",
    "# CREATE TRAIN SUBSET AND VALID SUBSET\n",
    "data_df = train_text_df[['id','text', 'segment_label']]\n",
    "train_df = data_df.loc[data_df['id'].isin(IDS[train_idx]),['text', 'segment_label']].reset_index(drop=True)\n",
    "valid_df = data_df.loc[data_df['id'].isin(IDS[valid_idx])].reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
    "print(\"VALID Dataset: {}\".format(valid_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Venus', 'is', 'a', 'worthy', 'pursuit', 'despite', 'the', 'dangers.']\n",
      "39834    Venus is a worthy pursuit despite the dangers \n",
      "Name: discourse_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "'''test code for preprocessing'''\n",
    "i = 1\n",
    "j = 0\n",
    "\n",
    "# pre-processed\n",
    "label, start_idx, end_idx = data_df['segment_label'][i][j]\n",
    "text_id = data_df['id'][i]\n",
    "print(data_df['text'][i].split()[start_idx:end_idx+1])\n",
    "\n",
    "# original\n",
    "original_text = original_train_df[original_train_df['id'] == text_id]\n",
    "print(original_text[original_text['discourse_type'] == label]['discourse_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>segment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7301B174090E</td>\n",
       "      <td>I believe that a B average would be a good thi...</td>\n",
       "      <td>[[Position, 0, 14], [Claim, 32, 47], [Counterc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3799E21B6EC3</td>\n",
       "      <td>Venus is a worthy pursuit despite the dangers....</td>\n",
       "      <td>[[Position, 0, 7], [Claim, 8, 28], [Evidence, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29C5DBB0A339</td>\n",
       "      <td>Limiting car usage will have many advantages. ...</td>\n",
       "      <td>[[Position, 0, 6], [Claim, 11, 12], [Claim, 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1613BD216385</td>\n",
       "      <td>\"Making Mona Lisa Smile\" is about a computer h...</td>\n",
       "      <td>[[Lead, 0, 39], [Position, 40, 64], [Evidence,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D4A3E7EC982E</td>\n",
       "      <td>In this essay i will be explaining the differe...</td>\n",
       "      <td>[[Lead, 0, 22], [Position, 23, 33], [Claim, 34...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  \\\n",
       "0  7301B174090E  I believe that a B average would be a good thi...   \n",
       "1  3799E21B6EC3  Venus is a worthy pursuit despite the dangers....   \n",
       "2  29C5DBB0A339  Limiting car usage will have many advantages. ...   \n",
       "3  1613BD216385  \"Making Mona Lisa Smile\" is about a computer h...   \n",
       "4  D4A3E7EC982E  In this essay i will be explaining the differe...   \n",
       "\n",
       "                                       segment_label  \n",
       "0  [[Position, 0, 14], [Claim, 32, 47], [Counterc...  \n",
       "1  [[Position, 0, 7], [Claim, 8, 28], [Evidence, ...  \n",
       "2  [[Position, 0, 6], [Claim, 11, 12], [Claim, 14...  \n",
       "3  [[Lead, 0, 39], [Position, 40, 64], [Evidence,...  \n",
       "4  [[Lead, 0, 22], [Position, 23, 33], [Claim, 34...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 이 잘 작동하는지 확인하는 코드\n",
    "# #!# 로 표지된 index 를 바꿔주면 해당 dataset_row 에 대해서 전처리된 라벨과 실제 라벨에서 다른 부분을 출력해준다.\n",
    "\n",
    "# data = data_df\n",
    "# is_train = True\n",
    "\n",
    "# index = 2 #!# 바꾸면서 다양한 시도 해보기\n",
    "\n",
    "# text = data.text[index]        \n",
    "# text_id = data.id[index]\n",
    "# segment_label_list = data.segment_label[index] if is_train else None\n",
    "\n",
    "# # TOKENIZE TEXT\n",
    "# encoding = tokenizer(\n",
    "#     text.split(),\n",
    "#     is_split_into_words=True,\n",
    "#     padding='max_length', #!# need to check exist seq s.t. longer than 4094\n",
    "#     truncation=True, #!# need to check exist seq s.t. longer than 4094\n",
    "#     max_length=500\n",
    "# )\n",
    "        \n",
    "# word_ids = encoding.word_ids()\n",
    "\n",
    "# segment_ids_list = [[labels_to_ids[label], start_idx, end_idx] for label, start_idx, end_idx in segment_label_list]\n",
    "\n",
    "# processed_list = []\n",
    "# for ids, start_idx, end_idx in segment_ids_list:\n",
    "#     start_word_ids = word_ids.index(start_idx)\n",
    "#     end_word_ids = word_ids.index(end_idx)\n",
    "    \n",
    "#     processed_list.append(tokenizer.decode(encoding.input_ids[start_word_ids:end_word_ids+1]))\n",
    "    \n",
    "# original_list = list(train_df[train_df['id'] == text_id]['discourse_text'])\n",
    "\n",
    "# is_same = True\n",
    "# for p_discourse, o_discourse in zip(processed_list, original_list):\n",
    "#     if p_discourse.split() == o_discourse.split():\n",
    "#         continue\n",
    "        \n",
    "#     else: \n",
    "#         is_same = False\n",
    "#         for p, o in zip(p_discourse.split(), o_discourse.split()):\n",
    "#             if p != o:\n",
    "#                 print(p, o)\n",
    "# if is_same:\n",
    "#     print('every token in the label is same.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''baseline : ignore \\n\\n, 문장 기호들'''\n",
    "#!# 문장 기호는 상당히 중요한 정보를 담고 있어서 처리해주고 싶은데.. \n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, is_train):\n",
    "        super(dataset, self).__init__()\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.is_train = is_train # if test (or validation) period, we won't use word label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        global max_segment\n",
    "        # GET TEXT AND WORD LABELS \n",
    "        text = self.data.text[index]        \n",
    "        segment_label_list = self.data.segment_label[index] if self.is_train else None\n",
    "\n",
    "        # TOKENIZE TEXT\n",
    "        encoding = self.tokenizer(\n",
    "            text.split(),\n",
    "            is_split_into_words=True,\n",
    "            return_offsets_mapping=False, #!# how to use it for enabling tokenizer to \"see\" \\n\\n?\n",
    "            padding='max_length', #!# need to check exist seq s.t. longer than 4094\n",
    "            truncation=True, #!# need to check exist seq s.t. longer than 4094\n",
    "            max_length=self.max_len\n",
    "        )\n",
    "        \n",
    "        word_ids = encoding.word_ids()\n",
    "        \n",
    "        # CREATE TARGETS\n",
    "        #!# detr label padding 구현 : x, y 정보는 어떻게 넣어주는가? 0이어도 되나, random 이 더 좋으려나\n",
    "        #!# 결론 : padding 은 구현 안함. loss 계산에서 누락. 필요하면 이하 코드 활용.\n",
    "        #!# 근데, 또 null_object weight 같은걸 보면 아예 없지는 않은듯.... 어렵네\n",
    "        if self.is_train:\n",
    "            segment_ids_list = torch.as_tensor([[labels_to_ids[label], start_idx, end_idx] for label, start_idx, end_idx in segment_label_list]) # [num_seg, 3]\n",
    "            segment_ids_pad  = torch.zeros(max_segment - segment_ids_list.size(0), segment_ids_list.size(1)) # [max_seg - num_seg, 3]\n",
    "            segment_ids_list = torch.cat((segment_ids_list, segment_ids_pad), dim = 0) # [max_seg, 3]\n",
    "            encoding['labels'] = segment_ids_list #!# .type(torch.LongTensor) # class, bound box must be long tensor\n",
    "\n",
    "        # CONVERT TO TORCH TENSORS\n",
    "        item = {k: torch.as_tensor(v) for k, v in encoding.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref) https://github.com/facebookresearch/detr 를 참고했으나, review 필요함.\n",
    "class DetrHead(nn.Module):\n",
    "    def __init__(self, feature_extractor, transformer, prediction_head, pos_emb, max_seq, max_segment, d_model):\n",
    "        super(DetrHead, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transformer = transformer\n",
    "        self.prediction_head = prediction_head\n",
    "        \n",
    "        self.feature_pos = pos_emb # absolute positional encoding (sinusodial, attention is all you need)\n",
    "        self.query_pos = nn.Parameter(torch.rand(max_segment, d_model))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.feature_extractor(x) # x -> [b, s, d_model]\n",
    "        out = self.transformer(out + self.feature_pos(out), repeat(self.query_pos, 'i j -> b i j', b = out.size(0))) # [b, s, d_model]\n",
    "        out = self.prediction_head(out)\n",
    "        return out\n",
    "    \n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, lm):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.lm = lm\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.lm(input_ids = x['input_ids'], attention_mask = x['attention_mask']).last_hidden_state #!# todo : try other layer\n",
    "        return out\n",
    "    \n",
    "class Transformer(nn.Module): #!# todo : change transformer structure with user defined transformer structure\n",
    "    def __init__(self, d_model, nhead = 8, num_encoder_layers = 6, num_decoder_layers = 6, dim_feedforward = 256):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.transformer = nn.Transformer( # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "            d_model, \n",
    "            nhead = nhead, \n",
    "            num_encoder_layers = num_encoder_layers, \n",
    "            num_decoder_layers = num_decoder_layers, \n",
    "            dim_feedforward = dim_feedforward,\n",
    "            batch_first = True\n",
    "        ) \n",
    "        \n",
    "    def forward(self, f, q):\n",
    "        out = self.transformer(f, q)\n",
    "        return out\n",
    "\n",
    "class PredictionHead(nn.Module): #!# todo : try diff. prediction head\n",
    "    def __init__(self, d_model, num_class):\n",
    "        super(PredictionHead, self).__init__()\n",
    "        self.fc_layer_class = nn.Linear(d_model, num_class + 1) # +1 for null class\n",
    "        self.fc_layer_segment = nn.Linear(d_model, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        c = self.fc_layer_class(x)\n",
    "        b = self.fc_layer_segment(x)\n",
    "        return (c, b)\n",
    "    \n",
    "import math\n",
    "class PositionalEmbedding(nn.Module): #!# ref) https://github.com/codertimo/BERT-pytorch\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model, requires_grad = False).float()\n",
    "        pos = torch.arange(0, max_len).float()\n",
    "        div = (-(torch.arange(0, d_model, 2).float() / d_model) * math.log(10000.0)).exp()\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(torch.einsum('i,j->ij', pos, div))\n",
    "        pe[:, 1::2] = torch.cos(torch.einsum('i,j->ij', pos, div))\n",
    "        pe = rearrange(pe, 'i j -> () i j')\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test code (model)\n",
    "\n",
    "모든 부분을 믿지 마라. 확인할 수 있는 코드를 만드는 것도 능력이다.\n",
    "\n",
    "작업중..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "lm = AutoModel.from_pretrained('bert-base-uncased') #!# 제출을 위해 local 에 다운받는 과정 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset(train_df, tokenizer, lm.config.max_position_embeddings, is_train = True)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 2, shuffle = True)\n",
    "\n",
    "valid_dataset = dataset(valid_df, tokenizer, lm.config.max_position_embeddings, is_train = True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model parameter'''\n",
    "max_seq = lm.config.max_position_embeddings\n",
    "d_model = lm.config.hidden_size\n",
    "\n",
    "max_segment = 20\n",
    "num_class = len(output_labels_detr)\n",
    "\n",
    "FEATURE_EXTRACTOR = FeatureExtractor(lm)\n",
    "TRANSFORMER = Transformer(d_model)\n",
    "PREDICTION_HEAD = PredictionHead(d_model, num_class)\n",
    "PE = PositionalEmbedding(d_model, max_seq)\n",
    "DETR_HEAD = DetrHead(FEATURE_EXTRACTOR, TRANSFORMER, PREDICTION_HEAD, PE, max_seq, max_segment, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break\n",
    "\n",
    "batch = {k : v.to(device) for k, v in batch.items()}\n",
    "DETR_HEAD.to(device)\n",
    "\n",
    "out = DETR_HEAD(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "# fork ref) https://github.com/facebookresearch/detr/blob/091a817eca74b8b97e35e4531c1c39f89fbe38eb/models/detr.py#L83\n",
    "# code ref) https://www.kaggle.com/tanulsingh077/end-to-end-object-detection-with-transformers-detr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# baseline 이 작동하는지 확인하기 위해 bert-base 활용\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "lm = AutoModel.from_pretrained('bert-base-uncased') #!# 제출을 위해 local 에 다운받는 과정 필요.\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('./model')\n",
    "# lm = AutoModel.from_pretrained('google/bigbird-roberta-base') #!# 제출을 위해 local 에 다운받는 과정 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset(train_df, tokenizer, lm.config.max_position_embeddings, is_train = True)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 2, shuffle = True)\n",
    "\n",
    "valid_dataset = dataset(valid_df, tokenizer, lm.config.max_position_embeddings, is_train = True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model parameter'''\n",
    "max_seq = lm.config.max_position_embeddings\n",
    "d_model = lm.config.hidden_size\n",
    "\n",
    "max_segment = 20\n",
    "num_class = len(output_labels_detr) - 1 # null-class\n",
    "FEATURE_EXTRACTOR = FeatureExtractor(lm)\n",
    "TRANSFORMER = Transformer(d_model)\n",
    "PREDICTION_HEAD = PredictionHead(d_model, num_class)\n",
    "PE = PositionalEmbedding(d_model, max_seq)\n",
    "DETR_HEAD = DetrHead(FEATURE_EXTRACTOR, TRANSFORMER, PREDICTION_HEAD, PE, max_seq, max_segment, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''hyper parameter'''\n",
    "EPOCH = 5\n",
    "LR = 2e-6\n",
    "max_norm = 0.1 # gradient clipping\n",
    "\n",
    "scheduler = None\n",
    "\n",
    "# 이하는 작업중인 hyper-parameters\n",
    "# null_class_coef = 0.5 #!# null class 학습이 전혀 안이뤄지는것도 문제가 있지... 지금은 전부 버리는데 이게 맞는 것 같지는 않아."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!# 여기부터 facebook source code 를 바꾸고 있음.\n",
    "#!# ref) https://github.com/facebookresearch/detr/blob/eb9f7e03ed8e2ed2cd55528989fe7df890bc3fc0/models/matcher.py#L12\n",
    "#!# 각 코드가 잘 작동하는지 *매우* 유의해서 살펴봐야 함.\n",
    "#!# facebook 에서는 @torch.no_grad() 를 사용해서 메모리를 적게 사용함. 나는 loss 를 compute_loss 에서 직접 계산하기 때문에 그게 안됨.\n",
    "#!# 메모리 문제가 아슬아슬해지면 고려가 필요함.\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def compute_loss(outputs : dict, targets : dict, weight_bbox = 1, weight_class = 1, weight_giou = 1):\n",
    "    '''Compute loss. \n",
    "    모델이 예측한 class 와 bounding box 를 토대로 target 과 비교해서 loss 를 계산한다.\n",
    "    DETR 코드와 마찬가지로 target 에는 no-object class 가 없다. 즉, no-object 에 대해서는 loss 를 계산하지 않는다.\n",
    "    # ref) https://github.com/facebookresearch/detr/blob/eb9f7e03ed8e2ed2cd55528989fe7df890bc3fc0/models/matcher.py#L12\n",
    "    \n",
    "    Parameters\n",
    "        outputs:  \n",
    "            - 'pred_logits': 각 class 에 대한 logit. 이후 softmax 를 통해 class 에 대한 확률 계산으로 사용된다.\n",
    "            - 'pred_boxes' :  예측한 bounding box. [x, y] 로 이뤄진다.\n",
    "        target:\n",
    "            - 'labels': 해당 segment 의 class\n",
    "            - 'boxes' : 해당 segment 의 bounding box [x, y]\n",
    "            \n",
    "    Returns\n",
    "        bbox 에 대한 2가지 loss (L1 거리, iou) 와 cross entropy 를 합한 loss 를 뱉어준다.\n",
    "        : cost_bbox + cost_class + cost_giou\n",
    "    '''\n",
    "    batch_size, num_query, num_tgt = outputs[\"pred_logits\"].size(0), outputs[\"pred_logits\"].size(1), targets['labels'].size(-1)\n",
    "\n",
    "    # We flatten to compute the cost matrices in a batch\n",
    "    out_prob = rearrange(outputs[\"pred_logits\"], 'b s c -> (b s) c').softmax(dim = -1)  # [batch_size * num_queries, num_classes]\n",
    "    out_bbox = rearrange(outputs[\"pred_boxes\"], 'b s box -> (b s) box')  # [batch_size * num_queries, 4]\n",
    "\n",
    "    # Also concat the target labels and boxes\n",
    "    tgt_ids  = torch.cat([v for v in targets[\"labels\"].type(torch.LongTensor)]).to(device)\n",
    "    tgt_bbox = torch.cat([v for v in targets[\"boxes\"]]).to(device)\n",
    "\n",
    "    # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
    "    # but approximate it in 1 - proba[target class].\n",
    "    # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
    "    cost_class = -out_prob[:, tgt_ids]\n",
    "\n",
    "    # Compute the L1 cost between boxes\n",
    "    #!# 이거 normalize 안돼서 너무 클거임. 1/n_seq 해주는게 좋을 듯\n",
    "    cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1) # [batch_size * num_queries, batch_size * num_queries]\n",
    "\n",
    "    # Compute the giou cost betwen boxes\n",
    "    #!# 분명 overflow 문제 발생함.. 어떻게 해결할 수 있을까. detr 은 assert 사용함\n",
    "    cost_giou = -one_dim_iou(out_bbox, tgt_bbox)\n",
    "\n",
    "    #!# Final cost matrix\n",
    "    #!# need to change weight for loss\n",
    "    C = weight_bbox * cost_bbox + weight_class * cost_class + weight_giou * cost_giou\n",
    "\n",
    "    C = rearrange(C, '(b1 q1) (b2 q2) -> b1 q1 b2 q2', # [batch_size, num_query, batch_size, num_query]\n",
    "                  b1 = batch_size, q1 = num_query, b2 = batch_size, q2 = num_tgt) \n",
    "\n",
    "    match_list = [linear_sum_assignment(C[b, :, b, :].detach().cpu()) for b in range(batch_size)]\n",
    "\n",
    "    match_loss = torch.zeros(batch_size, num_query, num_query)\n",
    "    for b in range(batch_size):\n",
    "        for i, j in zip(match_list[b][0], match_list[b][1]): # index = (i, j)\n",
    "            match_loss[b, i, j] = torch.as_tensor(C[b, i, b, j])\n",
    "    \n",
    "    return torch.sum(match_loss)\n",
    "\n",
    "#!# 높은 확률로 div zero 로 인한 overflow 예상됨.\n",
    "def one_dim_iou(out_bbox, tgt_bbox):\n",
    "    max_matrix = torch.max(\n",
    "        repeat(out_bbox, 'bq i -> bq r i', r = tgt_bbox.size(0)), \n",
    "        repeat(tgt_bbox, 'bq i -> r bq i', r = out_bbox.size(0))\n",
    "    )\n",
    "\n",
    "    min_matrix = torch.min(\n",
    "        repeat(out_bbox, 'bq i -> bq r i', r = tgt_bbox.size(0)), \n",
    "        repeat(tgt_bbox, 'bq i -> r bq i', r = out_bbox.size(0))\n",
    "    )\n",
    "\n",
    "    return (min_matrix[:, :, 1] - max_matrix[:, :, 0]) / (max_matrix[:, :, 1] - min_matrix[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matrix contains invalid numeric entries",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5adec6c035c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'labels'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boxes'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mweight_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-8de0425dd314>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(outputs, targets, weight_bbox, weight_class, weight_giou)\u001b[0m\n\u001b[1;32m     55\u001b[0m                   b1 = batch_size, q1 = num_query, b2 = batch_size, q2 = num_tgt) \n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mmatch_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlinear_sum_assignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mmatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-8de0425dd314>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m                   b1 = batch_size, q1 = num_query, b2 = batch_size, q2 = num_tgt) \n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mmatch_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlinear_sum_assignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mmatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda-3-2020.02/envs/snu36/lib/python3.6/site-packages/scipy/optimize/_lsap.py\u001b[0m in \u001b[0;36mlinear_sum_assignment\u001b[0;34m(cost_matrix, maximize)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misneginf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_matrix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"matrix contains invalid numeric entries\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mcost_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matrix contains invalid numeric entries"
     ]
    }
   ],
   "source": [
    "weight_dict = {'weight_bbox' : 1/max_seq, 'weight_class' : 1, 'weight_giou' : 1/max_seq}\n",
    "optimizer = torch.optim.AdamW(DETR_HEAD.parameters(), lr=LR)\n",
    "\n",
    "loss_traj = []\n",
    "\n",
    "model = DETR_HEAD.to(device)\n",
    "model.train()\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    batch = {k : v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    c, b = DETR_HEAD(batch) # c : class, b : bounding box\n",
    "    out = {'pred_logits' : c, 'pred_boxes' : b}\n",
    "    tgt = {'labels' : batch['labels'][:, :, 0], 'boxes' : batch['labels'][:, :, 1:]}\n",
    "\n",
    "    loss = compute_loss(out, tgt, **weight_dict)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    loss_traj.append(float(loss))\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f12d2bdd438>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for submission\n",
    "# ref) https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615?scriptVersionId=83230719\n",
    "test_names, test_texts = [], []\n",
    "for f in list(os.listdir('../input/feedback-prize-2021/test')):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    test_texts.append(open('../input/feedback-prize-2021/test/' + f, 'r').read())\n",
    "test_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\n",
    "\n",
    "test_names, train_texts = [], []\n",
    "for f in tqdm(list(os.listdir('../input/feedback-prize-2021/train'))):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    train_texts.append(open('../input/feedback-prize-2021/train/' + f, 'r').read())\n",
    "train_text_df = pd.DataFrame({'id': test_names, 'text': train_texts})\n",
    "\n",
    "\n",
    "# # TEST DATASET\n",
    "# test_texts_set = dataset(test_texts, tokenizer, config['max_length'], True)\n",
    "# test_texts_loader = DataLoader(test_texts_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define config\n",
    "config = {'model_name': MODEL_NAME,   \n",
    "          'max_length': 1024,\n",
    "          'train_batch_size':4,\n",
    "          'valid_batch_size':4,\n",
    "          'epochs':5,\n",
    "          'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n",
    "          'max_grad_norm':10,\n",
    "          'device': 'cuda' if cuda.is_available() else 'cpu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666cc845bdc54244acb9aff81cef5626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51674fa78df049babf1e80f7e1b66fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e6301eb1b248a7aa55f1f9062defcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/846k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055c9165c0c64092805d94e89973a1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/775 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9508107e9dd543fa9c6ed48270f8d929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdForTokenClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# if you are first running this code, please download LM.\n",
    "MODEL_NAME = 'google/bigbird-roberta-base' # choose which model to download\n",
    "DOWNLOADED_MODEL_PATH = 'model'            # choose where to download the model\n",
    "\n",
    "if DOWNLOADED_MODEL_PATH == 'model':\n",
    "    os.mkdir('model')\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True) #!# add_prefix_space?\n",
    "    tokenizer.save_pretrained('model')\n",
    "\n",
    "    config_model = AutoConfig.from_pretrained(MODEL_NAME) \n",
    "    config_model.num_labels = 15\n",
    "    config_model.save_pretrained('model')\n",
    "\n",
    "    backbone = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, \n",
    "                                                               config=config_model)\n",
    "    backbone.save_pretrained('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!# implement tokenizer \\n\\n, to see paragraph information\n",
    "# ref) https://github.com/huggingface/tokenizers/issues/247\n",
    "# ref) https://www.kaggle.com/c/feedback-prize-2021/discussion/296713\n",
    "tokenizer.decode(tokenizer(r'\\\\n\\\\n', return_offsets_mapping=True)['input_ids'])\n",
    "\n",
    "'''add new special token to model and tokenizer'''\n",
    "special_tokens_dict = {'additional_special_tokens': [r'\\\\n\\\\n']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i) \n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:                \n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            else:  \n",
    "                label_ids.append(label[word_idx])\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snu36",
   "language": "python",
   "name": "snu36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
