{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook baseline_object_detection.ipynb to script\n",
      "[NbConvertApp] Writing 34757 bytes to baseline_object_detection.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script baseline_object_detection.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat, reduce\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmain ref) https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615?scriptVersionId=83230719\\ndetr ref) https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb\\ndetr ref) https://www.kaggle.com/tanulsingh077/end-to-end-object-detection-with-transformers-detr\\n코드 실행 전 준비 사항 :\\n    1) 먼저 kaggle 에서 데이터를 다운받고, './input' 에 압축해제 시켜준다.\\n    2) detr 을 fork 해준다. # !git clone https://github.com/facebookresearch/detr.git\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "main ref) https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615?scriptVersionId=83230719\n",
    "detr ref) https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb\n",
    "detr ref) https://www.kaggle.com/tanulsingh077/end-to-end-object-detection-with-transformers-detr\n",
    "코드 실행 전 준비 사항 :\n",
    "    1) 먼저 kaggle 에서 데이터를 다운받고, './input' 에 압축해제 시켜준다.\n",
    "    2) detr 을 fork 해준다. # !git clone https://github.com/facebookresearch/detr.git\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object detection 문제로 전처리하기\n",
    "# objective : use DETR structure for sentence segmentation.\n",
    "PATH = os.path.join(os.getcwd(), 'input')\n",
    "TRAIN_NER_PATH_DETR = os.path.join(PATH, 'train_detr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "#!# test code for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER label 로 전처리한 데이터 불러오기\n",
    "# 만약 starting class 를 원하지 않는다면 이하 코드를 실행할 것.\n",
    "\n",
    "try:\n",
    "    from ast import literal_eval\n",
    "    train_text_df = pd.read_csv(TRAIN_NER_PATH_DETR)\n",
    "    \n",
    "    # pandas saves lists as string, we must convert back\n",
    "    from ast import literal_eval\n",
    "    train_text_df.segment_label = train_text_df.segment_label.apply(lambda x: literal_eval(x))\n",
    "    \n",
    "    original_train_df = pd.read_csv(os.path.join(PATH, 'train.csv'))\n",
    "    \n",
    "except:\n",
    "    print('this is 1st time to run this code...')\n",
    "    print('try to convert original text to DETR labels...')\n",
    "    # read original text files0\n",
    "    train_ids, train_texts = [], []\n",
    "    for f in tqdm(list(os.listdir(os.path.join(PATH, 'train')))):\n",
    "        train_ids.append(f.replace('.txt', ''))\n",
    "        train_texts.append(open(os.path.join(PATH, 'train', f), 'r').read())\n",
    "    train_text_df = pd.DataFrame({'id': train_ids, 'text': train_texts})\n",
    "\n",
    "    # convert segment label into object detection label : [segment_type, x, y]\n",
    "    original_train_df = pd.read_csv(os.path.join(PATH, 'train.csv'))\n",
    "    label_list = []\n",
    "    for i, text_df in tqdm(train_text_df.iterrows()):\n",
    "        total = text_df['text'].split().__len__()\n",
    "        segment_label_list = []\n",
    "        for j, segment_df in original_train_df[original_train_df['id'] == text_df['id']].iterrows():\n",
    "            segment_label = [\n",
    "                segment_df['discourse_type'],\n",
    "                int(segment_df['predictionstring'].split(' ')[0]), \n",
    "                int(segment_df['predictionstring'].split(' ')[-1])\n",
    "            ]\n",
    "            segment_label_list.append(segment_label)\n",
    "\n",
    "        label_list.append(segment_label_list)\n",
    "\n",
    "    train_text_df['segment_label'] = label_list\n",
    "    train_text_df.to_csv(TRAIN_NER_PATH_DETR, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DICTIONARIES THAT WE CAN USE DURING TRAIN AND INFER\n",
    "output_labels_detr = [\n",
    "    'O', # detr need dummy class for padding\n",
    "    'Lead', \n",
    "    'Position', \n",
    "    'Claim', \n",
    "    'Counterclaim', \n",
    "    'Rebuttal', \n",
    "    'Evidence', \n",
    "    'Concluding Statement'\n",
    "]\n",
    "\n",
    "labels_to_ids = {v:k for k,v in enumerate(output_labels_detr)}\n",
    "ids_to_labels = {k:v for k,v in enumerate(output_labels_detr)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15594 train texts. We will split 90% 10% for validation.\n",
      "FULL Dataset: (15594, 3)\n",
      "TRAIN Dataset: (14034, 3)\n",
      "VALID Dataset: (1560, 3)\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE VALIDATION INDEXES\n",
    "IDS = original_train_df.id.unique()\n",
    "print('There are',len(IDS),'train texts. We will split 90% 10% for validation.')\n",
    "\n",
    "# TRAIN VALID SPLIT 90% 10%\n",
    "train_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\n",
    "valid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\n",
    "\n",
    "# CREATE TRAIN SUBSET AND VALID SUBSET\n",
    "data_df = train_text_df[['id','text', 'segment_label']]\n",
    "train_df = data_df.loc[data_df['id'].isin(IDS[train_idx]),['id', 'text', 'segment_label']].reset_index(drop=True)\n",
    "valid_df = data_df.loc[data_df['id'].isin(IDS[valid_idx])].reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
    "print(\"VALID Dataset: {}\".format(valid_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Venus', 'is', 'a', 'worthy', 'pursuit', 'despite', 'the', 'dangers.']\n",
      "39834    Venus is a worthy pursuit despite the dangers \n",
      "Name: discourse_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "'''test code for preprocessing'''\n",
    "i = 1\n",
    "j = 0\n",
    "\n",
    "# pre-processed\n",
    "label, start_idx, end_idx = data_df['segment_label'][i][j]\n",
    "text_id = data_df['id'][i]\n",
    "print(data_df['text'][i].split()[start_idx:end_idx+1])\n",
    "\n",
    "# original\n",
    "original_text = original_train_df[original_train_df['id'] == text_id]\n",
    "print(original_text[original_text['discourse_type'] == label]['discourse_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>segment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7301B174090E</td>\n",
       "      <td>I believe that a B average would be a good thi...</td>\n",
       "      <td>[[Position, 0, 14], [Claim, 32, 47], [Counterc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3799E21B6EC3</td>\n",
       "      <td>Venus is a worthy pursuit despite the dangers....</td>\n",
       "      <td>[[Position, 0, 7], [Claim, 8, 28], [Evidence, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29C5DBB0A339</td>\n",
       "      <td>Limiting car usage will have many advantages. ...</td>\n",
       "      <td>[[Position, 0, 6], [Claim, 11, 12], [Claim, 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1613BD216385</td>\n",
       "      <td>\"Making Mona Lisa Smile\" is about a computer h...</td>\n",
       "      <td>[[Lead, 0, 39], [Position, 40, 64], [Evidence,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D4A3E7EC982E</td>\n",
       "      <td>In this essay i will be explaining the differe...</td>\n",
       "      <td>[[Lead, 0, 22], [Position, 23, 33], [Claim, 34...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  \\\n",
       "0  7301B174090E  I believe that a B average would be a good thi...   \n",
       "1  3799E21B6EC3  Venus is a worthy pursuit despite the dangers....   \n",
       "2  29C5DBB0A339  Limiting car usage will have many advantages. ...   \n",
       "3  1613BD216385  \"Making Mona Lisa Smile\" is about a computer h...   \n",
       "4  D4A3E7EC982E  In this essay i will be explaining the differe...   \n",
       "\n",
       "                                       segment_label  \n",
       "0  [[Position, 0, 14], [Claim, 32, 47], [Counterc...  \n",
       "1  [[Position, 0, 7], [Claim, 8, 28], [Evidence, ...  \n",
       "2  [[Position, 0, 6], [Claim, 11, 12], [Claim, 14...  \n",
       "3  [[Lead, 0, 39], [Position, 40, 64], [Evidence,...  \n",
       "4  [[Lead, 0, 22], [Position, 23, 33], [Claim, 34...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 이 잘 작동하는지 확인하는 코드\n",
    "# #!# 로 표지된 index 를 바꿔주면 해당 dataset_row 에 대해서 전처리된 라벨과 실제 라벨에서 다른 부분을 출력해준다.\n",
    "\n",
    "# data = data_df\n",
    "# is_train = True\n",
    "\n",
    "# index = 2 #!# 바꾸면서 다양한 시도 해보기\n",
    "\n",
    "# text = data.text[index]        \n",
    "# text_id = data.id[index]\n",
    "# segment_label_list = data.segment_label[index] if is_train else None\n",
    "\n",
    "# # TOKENIZE TEXT\n",
    "# encoding = tokenizer(\n",
    "#     text.split(),\n",
    "#     is_split_into_words=True,\n",
    "#     padding='max_length', #!# need to check exist seq s.t. longer than 4094\n",
    "#     truncation=True, #!# need to check exist seq s.t. longer than 4094\n",
    "#     max_length=500\n",
    "# )\n",
    "        \n",
    "# word_ids = encoding.word_ids()\n",
    "\n",
    "# segment_ids_list = [[labels_to_ids[label], start_idx, end_idx] for label, start_idx, end_idx in segment_label_list]\n",
    "\n",
    "# processed_list = []\n",
    "# for ids, start_idx, end_idx in segment_ids_list:\n",
    "#     start_word_ids = word_ids.index(start_idx)\n",
    "#     end_word_ids = word_ids.index(end_idx)\n",
    "    \n",
    "#     processed_list.append(tokenizer.decode(encoding.input_ids[start_word_ids:end_word_ids+1]))\n",
    "    \n",
    "# original_list = list(train_df[train_df['id'] == text_id]['discourse_text'])\n",
    "\n",
    "# is_same = True\n",
    "# for p_discourse, o_discourse in zip(processed_list, original_list):\n",
    "#     if p_discourse.split() == o_discourse.split():\n",
    "#         continue\n",
    "        \n",
    "#     else: \n",
    "#         is_same = False\n",
    "#         for p, o in zip(p_discourse.split(), o_discourse.split()):\n",
    "#             if p != o:\n",
    "#                 print(p, o)\n",
    "# if is_same:\n",
    "#     print('every token in the label is same.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''baseline : ignore \\n\\n, 문장 기호들'''\n",
    "#!# 문장 기호는 상당히 중요한 정보를 담고 있어서 처리해주고 싶은데.. \n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, is_train):\n",
    "        super(dataset, self).__init__()\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.is_train = is_train # if test (or validation) period, we won't use word label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        global max_segment\n",
    "        # GET TEXT AND WORD LABELS \n",
    "        text = self.data.text[index]        \n",
    "        segment_label_list = self.data.segment_label[index] if self.is_train else None\n",
    "\n",
    "        # TOKENIZE TEXT\n",
    "        encoding = self.tokenizer(\n",
    "            text.split(),\n",
    "            is_split_into_words=True,\n",
    "            return_offsets_mapping=False, #!# how to use it for enabling tokenizer to \"see\" \\n\\n?\n",
    "            padding='max_length', #!# need to check exist seq s.t. longer than 4094\n",
    "            truncation=True, #!# need to check exist seq s.t. longer than 4094\n",
    "            max_length=self.max_len\n",
    "        )\n",
    "        \n",
    "        word_ids = encoding.word_ids()\n",
    "        \n",
    "        # CREATE TARGETS\n",
    "        #!# detr label padding 구현 : x, y 정보는 어떻게 넣어주는가? 0이어도 되나, random 이 더 좋으려나\n",
    "        #!# 결론 : padding 은 구현 안함. loss 계산에서 누락. 필요하면 이하 코드 활용.\n",
    "        #!# 근데, 또 null_object weight 같은걸 보면 아예 없지는 않은듯.... 어렵네\n",
    "        if self.is_train:\n",
    "            segment_ids_list = torch.as_tensor([[labels_to_ids[label], start_idx, end_idx] for label, start_idx, end_idx in segment_label_list]) # [num_seg, 3]\n",
    "            segment_ids_pad  = torch.zeros(max_segment - segment_ids_list.size(0), segment_ids_list.size(1)) # [max_seg - num_seg, 3]\n",
    "            segment_ids_list = torch.cat((segment_ids_list, segment_ids_pad), dim = 0) # [max_seg, 3]\n",
    "            encoding['labels'] = segment_ids_list #!# .type(torch.LongTensor) # class, bound box must be long tensor\n",
    "\n",
    "        # CONVERT TO TORCH TENSORS\n",
    "        item = {k: torch.as_tensor(v) for k, v in encoding.items()}\n",
    "        \n",
    "        # id 를 넣어주면 마무리\n",
    "        item['id'] = self.data.id[index]        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref) https://github.com/facebookresearch/detr 를 참고했으나, review 필요함.\n",
    "class DetrHead(nn.Module):\n",
    "    def __init__(self, feature_extractor, transformer, prediction_head, pos_emb, max_seq, max_segment, d_model,device_0, device_1, split_gpus = True):\n",
    "        super(DetrHead, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transformer = transformer\n",
    "        self.prediction_head = prediction_head\n",
    "        \n",
    "        self.feature_pos = torch.rand(max_seq, d_model).cuda(device_1) #!# pos_emb # absolute positional encoding (sinusodial, attention is all you need)\n",
    "        self.query_pos = torch.rand(max_segment, d_model).cuda(device_1)\n",
    "        \n",
    "#         self.split_gpus = split_gpus\n",
    "        \n",
    "#         if split_gpus:\n",
    "#             print('split model for parallel processing')\n",
    "#             self.feature_extractor.cuda(0) # LM\n",
    "#             self.query_pos.cuda(1)\n",
    "#             self.feature_pos.cuda(1)\n",
    "#             self.transformer.cuda(1)\n",
    "#             self.prediction_head.cuda(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.feature_extractor(x) # x -> [b, s, d_model]\n",
    "        out = out.cuda(device_1)\n",
    "        #!# absolute pos : out = self.transformer(out + self.feature_pos(out), repeat(self.query_pos, 'i j -> b i j', b = out.size(0))) # [b, s, d_model]\n",
    "        out = self.transformer(out + repeat(self.feature_pos, 'i j -> b i j', b = out.size(0)), repeat(self.query_pos, 'i j -> b i j', b = out.size(0))) # [b, s, d_model]\n",
    "        out = self.prediction_head(out)\n",
    "        return out\n",
    "    \n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, lm):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.lm = lm\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.lm(input_ids = x['input_ids'], attention_mask = x['attention_mask']).last_hidden_state #!# todo : try other layer\n",
    "        return out\n",
    "    \n",
    "class Transformer(nn.Module): #!# todo : change transformer structure with user defined transformer structure\n",
    "    def __init__(self, d_model, nhead = 8, num_encoder_layers = 6, num_decoder_layers = 6, dim_feedforward = 2048):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.transformer = nn.Transformer( # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "            d_model, \n",
    "            nhead = nhead, \n",
    "            num_encoder_layers = num_encoder_layers, \n",
    "            num_decoder_layers = num_decoder_layers, \n",
    "            dim_feedforward = dim_feedforward,\n",
    "            batch_first = True\n",
    "        ) \n",
    "        \n",
    "    def forward(self, f, q):\n",
    "        out = self.transformer(f, q)\n",
    "        return out\n",
    "\n",
    "class PredictionHead(nn.Module): #!# todo : try diff. prediction head\n",
    "    def __init__(self, d_model, num_class, num_class_layer = 10):\n",
    "        super(PredictionHead, self).__init__()\n",
    "        self.fc_layer_class = nn.ModuleList(\n",
    "            [nn.Linear(d_model, d_model) for _ in range(num_class_layer)]\n",
    "        )\n",
    "        \n",
    "        self.fc_layer_segment = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model // 2, 2) # [x, y]\n",
    "        )\n",
    "        \n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        c = x\n",
    "        for fc_layer in self.fc_layer_class:\n",
    "            c = fc_layer(c) + c\n",
    "            c = self.activation(c)\n",
    "            \n",
    "        b = self.fc_layer_segment(x)\n",
    "        return (c, b)\n",
    "    \n",
    "import math\n",
    "class PositionalEmbedding(nn.Module): #!# ref) https://github.com/codertimo/BERT-pytorch\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model, requires_grad = False).float()\n",
    "        pos = torch.arange(0, max_len).float()\n",
    "        div = (-(torch.arange(0, d_model, 2).float() / d_model) * math.log(10000.0)).exp()\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(torch.einsum('i,j->ij', pos, div))\n",
    "        pe[:, 1::2] = torch.cos(torch.einsum('i,j->ij', pos, div))\n",
    "        pe = rearrange(pe, 'i j -> () i j')\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting\n",
    "\n",
    "* build dataloader\n",
    "* model\n",
    "* loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "# fork ref) https://github.com/facebookresearch/detr/blob/091a817eca74b8b97e35e4531c1c39f89fbe38eb/models/detr.py#L83\n",
    "# code ref) https://www.kaggle.com/tanulsingh077/end-to-end-object-detection-with-transformers-detr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# baseline 이 작동하는지 확인하기 위해 bert-base 활용\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/bigbird-roberta-base')\n",
    "lm = AutoModel.from_pretrained('google/bigbird-roberta-base') #!# 제출을 위해 local 에 다운받는 과정 필요.\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('./model')\n",
    "# lm = AutoModel.from_pretrained('google/bigbird-roberta-base') #!# 제출을 위해 local 에 다운받는 과정 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!#\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "train_dataset_minibatch = dataset(train_df.iloc[:100], tokenizer, lm.config.max_position_embeddings, is_train = True)\n",
    "train_loader_minibatch = DataLoader(train_dataset_minibatch, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "train_dataset = dataset(train_df, tokenizer, lm.config.max_position_embeddings, is_train = True)\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model parameter'''\n",
    "max_seq = lm.config.max_position_embeddings\n",
    "d_model = lm.config.hidden_size\n",
    "max_segment = 50\n",
    "num_class = len(output_labels_detr) - 1 # null-class\n",
    "\n",
    "device_0, device_1 = 1, 0\n",
    "\n",
    "FEATURE_EXTRACTOR = FeatureExtractor(lm).cuda(device_0)\n",
    "TRANSFORMER = Transformer(d_model).cuda(device_1)\n",
    "PREDICTION_HEAD = PredictionHead(d_model, num_class).cuda(device_1)\n",
    "PE = PositionalEmbedding(d_model, max_seq).cuda(device_1)\n",
    "DETR_HEAD = DetrHead(FEATURE_EXTRACTOR, TRANSFORMER, PREDICTION_HEAD, PE, max_seq, max_segment, d_model, device_0, device_1)\n",
    "\n",
    "model = DETR_HEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''hyper parameter'''\n",
    "EPOCH = 1\n",
    "LR = 1e-5\n",
    "max_norm = 2 # gradient clipping\n",
    "\n",
    "weight_dict = {'weight_bbox' : 1, 'weight_class' : 0, 'weight_giou' : 1} #!# it is important to make sure the model learns bbox faster than class\n",
    "\n",
    "if_grad_clip = False\n",
    "if_scheduler = False\n",
    "\n",
    "class_loss = nn.CrossEntropyLoss()\n",
    "box_loss = nn.L1Loss()\n",
    "\n",
    "# 이하는 작업중인 hyper-parameters\n",
    "# null_class_coef = 0.5 #!# null class 학습이 전혀 안이뤄지는것도 문제가 있지... 지금은 전부 버리는데 이게 맞는 것 같지는 않아."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!# 여기부터 facebook source code 를 바꾸고 있음.\n",
    "#!# ref) https://github.com/facebookresearch/detr/blob/eb9f7e03ed8e2ed2cd55528989fe7df890bc3fc0/models/matcher.py#L12\n",
    "#!# 각 코드가 잘 작동하는지 *매우* 유의해서 살펴봐야 함.\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "#!# 높은 확률로 div zero 로 인한 overflow 예상됨. -> 아마 여기서 gradient exploding 문제가 발생했을듯\n",
    "#!# matching 이 초기에 이뤄지지 않으면 아예 학습이 안되는 알고리즘임에 주의하자. \n",
    "def one_dim_iou(out_bbox, tgt_bbox):\n",
    "    # assert (tgt_bbox[:, 1] >= tgt_bbox[:, 0]).all() #!# 이게 있어야 학습이 잘 이뤄질 것 같긴해... 근데 초기부터 이뤄지는게 가능한건가?\n",
    "    # assert (out_bbox[:, 1] >= out_bbox[:, 0]).all()\n",
    "    \n",
    "    max_matrix = torch.max(\n",
    "        repeat(out_bbox, 'bq i -> bq r i', r = tgt_bbox.size(0)), \n",
    "        repeat(tgt_bbox, 'bq i -> r bq i', r = out_bbox.size(0))\n",
    "    )\n",
    "\n",
    "    min_matrix = torch.min(\n",
    "        repeat(out_bbox, 'bq i -> bq r i', r = tgt_bbox.size(0)), \n",
    "        repeat(tgt_bbox, 'bq i -> r bq i', r = out_bbox.size(0))\n",
    "    )\n",
    "\n",
    "    return (min_matrix[:, :, 1] - max_matrix[:, :, 0]) / (max_matrix[:, :, 1] - min_matrix[:, :, 0])\n",
    "\n",
    "@torch.no_grad()\n",
    "def match(outputs : dict, targets : dict, weight_bbox = 1, weight_class = 1, weight_giou = 1):\n",
    "    '''scipy 의 linear_sum_assignment 를 활용해서 Hungarian matching 을 수행한다. \n",
    "    모델이 예측한 class 와 bounding box 를 토대로 target 과 비교해서\n",
    "    cost 를 최소화하는 matching 을 찾아낸다.\n",
    "    \n",
    "    #!# DETR 코드와 마찬가지로 target 에는 no-object class 가 없다. 즉, no-object 에 대해서는 loss 를 계산하지 않는다.\n",
    "    # ref) https://github.com/facebookresearch/detr/blob/eb9f7e03ed8e2ed2cd55528989fe7df890bc3fc0/models/matcher.py#L12\n",
    "    \n",
    "    Parameters\n",
    "        outputs: dict\n",
    "            - 'pred_logits': 각 class 에 대한 logit. 이후 softmax 를 통해 class 에 대한 확률 계산으로 사용된다.\n",
    "            - 'pred_boxes' :  예측한 bounding box. [x, y] 로 이뤄진다.\n",
    "        target: dict\n",
    "            - 'labels': 해당 segment 의 class\n",
    "            - 'boxes' : 해당 segment 의 bounding box [x, y]\n",
    "            \n",
    "    Returns\n",
    "        match_list: list [batch_size, [row_ind, col_ind]]\n",
    "            - 각 batch 에 대해서 row 와 col 의 matching index 정보를 담은 array 를 반환한다.\n",
    "    '''\n",
    "    \n",
    "    batch_size, num_query, num_tgt = outputs[\"pred_logits\"].size(0), outputs[\"pred_logits\"].size(1), targets['labels'].size(-1)\n",
    "\n",
    "    # We flatten to compute the cost matrices in a batch\n",
    "    out_prob = rearrange(outputs[\"pred_logits\"], 'b s c -> (b s) c').softmax(dim = -1)  # [batch_size * num_queries, num_classes]\n",
    "    out_bbox = rearrange(outputs[\"pred_boxes\"], 'b s box -> (b s) box')  # [batch_size * num_queries, 2]\n",
    "\n",
    "    # Also concat the target labels and boxes\n",
    "    tgt_ids  = torch.cat([v for v in targets[\"labels\"].type(torch.LongTensor)]).to(device)\n",
    "    tgt_bbox = torch.cat([v for v in targets[\"boxes\"]]).to(device)\n",
    "\n",
    "    # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
    "    # but approximate it in 1 - proba[target class].\n",
    "    # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
    "    cost_class = -out_prob[:, tgt_ids]\n",
    "\n",
    "    # Compute the L1 cost between boxes\n",
    "    #!# 이거 normalize 안돼서 너무 클거임. 1/n_seq 해주는게 좋을 듯\n",
    "    cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1) # [batch_size * num_queries, batch_size * num_queries]\n",
    "\n",
    "    # Compute the giou cost betwen boxes\n",
    "    #!# 분명 overflow 문제 발생함.. 어떻게 해결할 수 있을까. detr 은 assert 사용함\n",
    "    cost_giou = -one_dim_iou(out_bbox, tgt_bbox)\n",
    "\n",
    "    #!# Final cost matrix\n",
    "    #!# need to change weight for loss\n",
    "    C = weight_bbox * cost_bbox + weight_class * cost_class + weight_giou * cost_giou\n",
    "    C = torch.nan_to_num(C, nan = 1e9)\n",
    "    \n",
    "    if 1e9 in C:\n",
    "        print(float(torch.sum(C == 1e9))) #!# to track how many segments are discarded.\n",
    "        \n",
    "    C = rearrange(C, '(b1 q1) (b2 q2) -> b1 q1 b2 q2', # [batch_size, num_query, batch_size, num_query]\n",
    "                  b1 = batch_size, q1 = num_query, b2 = batch_size, q2 = num_tgt) \n",
    "\n",
    "    match_list = [linear_sum_assignment(C[b, :, b, :].detach().cpu()) for b in range(batch_size)]\n",
    "    return match_list\n",
    "\n",
    "@torch.no_grad()\n",
    "def _index_select_for_batch(x : torch.tensor, index : torch.tensor) -> torch.tensor:\n",
    "    '''Unfold batch and select element\n",
    "    Parameters\n",
    "        x : [batch, ..., index]\n",
    "        index : [batch, index]\n",
    "        \n",
    "    Returns\n",
    "        x (selected) : [batch, ..., index]\n",
    "    '''\n",
    "    x_shape = x.shape\n",
    "    \n",
    "    # unfold batch\n",
    "    index = torch.cat(tuple([torch.as_tensor(index_batch) + batch_idx * x_shape[1] for batch_idx, index_batch in enumerate(index)]), dim = 0) #\n",
    "    x = torch.cat(tuple([x_batch for x_batch in x]), dim = 0)\n",
    "    x = torch.index_select(x, 0, index)\n",
    "    \n",
    "    return x.view(x_shape)\n",
    "\n",
    "def _unfold_batch(x): # batch_first = True\n",
    "    return torch.cat(tuple([x_batch for x_batch in x]), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DETR_HEAD\n",
    "\n",
    "loss_traj = []\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer,\n",
    "    lr_lambda=lambda i: 0.95 ** i,\n",
    "    last_epoch=-1,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/opt/anaconda-3-2020.02/envs/snu36/lib/python3.6/site-packages/transformers/models/big_bird/modeling_big_bird.py:978: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  * num_indices_to_pick_from\n",
      "100%|██████████| 100/100 [01:38<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for _ in range(1):\n",
    "    for batch in tqdm(train_loader_minibatch): #!# train_loader\n",
    "        text_id_list = batch.pop('id')\n",
    "        batch = {k : v.cuda(device_0) for k, v in batch.items()}\n",
    "\n",
    "        c, b = model(batch) # c : class, b : bounding box\n",
    "        out = {'pred_logits' : c, 'pred_boxes' : b}\n",
    "        tgt = {'labels' : batch['labels'][:, :, 0], 'boxes' : batch['labels'][:, :, 1:] / max_seq} #!# input normalization\n",
    "\n",
    "        match_list = match(out, tgt, **weight_dict)\n",
    "        match_list = [list(index_batch[1]) for index_batch in match_list]\n",
    "        \n",
    "        loss = \\\n",
    "            0 * class_loss( #!# learn only segmentation\n",
    "                _unfold_batch(out['pred_logits'].softmax(dim = -1)).cuda(device_1),\n",
    "                _unfold_batch(_index_select_for_batch(tgt['labels'].type(torch.LongTensor), match_list).cuda(device_1))\n",
    "            ) + \\\n",
    "            weight_dict['weight_bbox'] * box_loss(\n",
    "                _unfold_batch(out['pred_boxes']).cuda(device_1),\n",
    "                _unfold_batch(_index_select_for_batch(tgt['boxes'].type(torch.LongTensor), match_list).cuda(device_1))\n",
    "            )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        if if_grad_clip:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if if_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        loss_traj.append(float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:45<00:00,  2.20it/s]\n"
     ]
    }
   ],
   "source": [
    "for param in model.feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
    "\n",
    "for _ in range(1):\n",
    "    for batch in tqdm(train_loader_minibatch): #!# train_loader\n",
    "        text_id_list = batch.pop('id')\n",
    "        batch = {k : v.cuda(device_0) for k, v in batch.items()}\n",
    "\n",
    "        c, b = model(batch) # c : class, b : bounding box\n",
    "        out = {'pred_logits' : c, 'pred_boxes' : b}\n",
    "        tgt = {'labels' : batch['labels'][:, :, 0], 'boxes' : batch['labels'][:, :, 1:] / max_seq} #!# input normalization\n",
    "\n",
    "        match_list = match(out, tgt, **weight_dict)\n",
    "        match_list = [list(index_batch[1]) for index_batch in match_list]\n",
    "        \n",
    "        loss = \\\n",
    "            1 * class_loss( #!# learn classification module\n",
    "                _unfold_batch(out['pred_logits'].softmax(dim = -1)).cuda(device_1),\n",
    "                _unfold_batch(_index_select_for_batch(tgt['labels'].type(torch.LongTensor), match_list).cuda(device_1))\n",
    "            ) + \\\n",
    "            weight_dict['weight_bbox'] * box_loss(\n",
    "                _unfold_batch(out['pred_boxes']).cuda(device_1),\n",
    "                _unfold_batch(_index_select_for_batch(tgt['boxes'].type(torch.LongTensor), match_list).cuda(device_1))\n",
    "            )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        if if_grad_clip:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if if_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        loss_traj.append(float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f59870932e8>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_traj[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f39800ae6a0>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_traj[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!# why classification mal-functioning?\n",
    "# 1. positional encoding (query, feature)\n",
    "# 2. fc_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[160, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160,\n",
       "         160, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160,\n",
       "         160, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160,\n",
       "         160, 160, 160, 160, 160, 160, 160, 160]], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['pred_logits'].argmax(dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0042],\n",
       "         [0.0044, 0.0081],\n",
       "         [0.0083, 0.0105],\n",
       "         [0.0107, 0.0139],\n",
       "         [0.0142, 0.0151],\n",
       "         [0.0154, 0.0232],\n",
       "         [0.0627, 0.0688],\n",
       "         [0.0691, 0.0796],\n",
       "         [0.0833, 0.0957],\n",
       "         [0.0959, 0.1023],\n",
       "         [0.1025, 0.1062],\n",
       "         [0.1064, 0.1187],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000]]], device='cuda:1')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt['boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  21.3308,  148.8575],\n",
       "         [  73.1937,   41.8354],\n",
       "         [ -41.9145,  239.2831],\n",
       "         [  59.4469,   50.6216],\n",
       "         [  42.1644,   88.9807],\n",
       "         [ 239.6809,   55.3452],\n",
       "         [ -80.9757,  238.6915],\n",
       "         [ -72.4200,   47.7628],\n",
       "         [ 217.4932,  258.3589],\n",
       "         [ 212.7906, -103.6397],\n",
       "         [ 238.9761,  -52.8346],\n",
       "         [  23.2336,   90.0844],\n",
       "         [  52.7408, -150.0342],\n",
       "         [ 213.1765,  532.8762],\n",
       "         [ 291.7162,   26.0797],\n",
       "         [ -15.9739, -261.7321],\n",
       "         [ 239.1922,  277.3680],\n",
       "         [ 281.9629,   60.3299],\n",
       "         [  45.1648,  308.1915],\n",
       "         [ -65.3509,   97.2841],\n",
       "         [ -54.8236,  -90.8115],\n",
       "         [   1.8124,  -93.1089],\n",
       "         [ 198.7298,  -25.1922],\n",
       "         [ 169.7078,  -10.2121],\n",
       "         [ 255.2095,  -49.1883],\n",
       "         [ -14.4000,  -39.0859],\n",
       "         [  92.6680,  173.7189],\n",
       "         [-216.4725,  295.3028],\n",
       "         [ -51.8181,  101.8259],\n",
       "         [  88.1491,  219.7425],\n",
       "         [  82.5210,  -22.6537],\n",
       "         [  -3.5932,  271.1028],\n",
       "         [ 246.0753, -159.7444],\n",
       "         [  56.2286,   13.6156],\n",
       "         [ 309.1305,  152.5842],\n",
       "         [ -47.0182,  -57.1242],\n",
       "         [ 220.0634,  187.0221],\n",
       "         [  91.2293,   47.8300],\n",
       "         [ 222.8022,  -44.7552],\n",
       "         [ 454.5429,  -51.8591],\n",
       "         [ 129.2440,  -37.3549],\n",
       "         [ 481.1373,  287.9130],\n",
       "         [ 254.3487,  107.8847],\n",
       "         [ -21.9229,  108.6833],\n",
       "         [  73.5723,  108.2238],\n",
       "         [ 115.3093,   53.6571],\n",
       "         [-122.5216,  417.4773],\n",
       "         [ 116.5518,   28.5759],\n",
       "         [  84.1414, -119.0342],\n",
       "         [  -8.0417,  237.4301]]], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['pred_boxes'] * max_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 : matching function\n",
    "# 2 : loss function\n",
    "\n",
    "#!# todo : create error output to check whether model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument x2 in method wrapper___cdist_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2563a8e369e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mex_tgt_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_tgt_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex_tgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mex_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex_out_true_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex_tgt_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-c38ae19d1896>\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(outputs, targets, weight_bbox, weight_class, weight_giou)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Compute the L1 cost between boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m#!# 이거 normalize 안돼서 너무 클거임. 1/n_seq 해주는게 좋을 듯\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mcost_bbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size * num_queries, batch_size * num_queries]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Compute the giou cost betwen boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mcdist\u001b[0;34m(x1, x2, p, compute_mode)\u001b[0m\n\u001b[1;32m   1151\u001b[0m             cdist, (x1, x2), x1, x2, p=p, compute_mode=compute_mode)\n\u001b[1;32m   1152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'use_mm_for_euclid_dist_if_necessary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcompute_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'use_mm_for_euclid_dist'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument x2 in method wrapper___cdist_forward)"
     ]
    }
   ],
   "source": [
    "'''matching function'''\n",
    "# create exemplar batch\n",
    "BATCH_SIZE, num_class, max_segment = 2, 7, 5\n",
    "\n",
    "ex_tgt = torch.cat((torch.randint(1, num_class+1, (BATCH_SIZE, max_segment, 1)), torch.rand(BATCH_SIZE, max_segment, 2)), dim = -1) # [class, x, y]\n",
    "ex_indices = torch.stack([torch.randperm(max_segment) for _ in range(BATCH_SIZE)], dim = 0)\n",
    "ex_out_true = _index_select_for_batch(ex_tgt, ex_indices)\n",
    "\n",
    "convert_out_dict = lambda x : {'pred_logits' : 0.9 * F.one_hot(x[:, :, 0].type(torch.LongTensor), num_classes = num_class+1), 'pred_boxes' : x[:, :, 1:]}\n",
    "convert_tgt_dict = lambda x : {'labels' : x[:, :, 0], 'boxes' : x[:, :, 1:]}\n",
    "\n",
    "ex_out_true_dict = convert_out_dict(ex_out_true)\n",
    "ex_tgt_dict = convert_tgt_dict(ex_tgt)\n",
    "\n",
    "ex_indices == torch.as_tensor([list(tgt_idx) for out_idx, tgt_idx in match(ex_out_true_dict, ex_tgt_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''loss function'''\n",
    "tgt = ex_tgt_dict\n",
    "out = ex_out_true_dict\n",
    "# out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_loss(\n",
    "    _unfold_batch(out['pred_logits'].softmax(dim = -1)).cuda(device_1),\n",
    "    _unfold_batch(_index_select_for_batch(tgt['labels'].type(torch.LongTensor), match_list).cuda(device_1))\n",
    ")\n",
    "\n",
    "box_loss(\n",
    "    _unfold_batch(out['pred_boxes']).cuda(device_1),\n",
    "    _unfold_batch(_index_select_for_batch(tgt['boxes'].type(torch.LongTensor), match_list).cuda(device_1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!# Final cost matrix\n",
    "#!# need to change weight for loss\n",
    "C = weight_bbox * cost_bbox + weight_class * cost_class + weight_giou * cost_giou\n",
    "C = torch.nan_to_num(C, nan = 1e9) #!# linear_sum_assignment 는 nan 을 처리 못함.\n",
    "C = rearrange(C, '(b1 q1) (b2 q2) -> b1 q1 b2 q2', # [batch_size, num_query, batch_size, num_query]\n",
    "              b1 = batch_size, q1 = num_query, b2 = batch_size, q2 = num_tgt) \n",
    "\n",
    "match_list = [linear_sum_assignment(C[b, :, b, :].detach().cpu()) for b in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_indices = match(ex_out_true_dict, ex_tgt_dict) #!# awkard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_indices = torch.as_tensor([list(tgt_idx) for out_idx, tgt_idx in match_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_tgt_matched = _index_select_for_batch(ex_tgt, match_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.0000, 0.5725, 0.4980],\n",
       "         [4.0000, 0.6826, 0.3051],\n",
       "         [5.0000, 0.4635, 0.4550],\n",
       "         [4.0000, 0.9371, 0.6556],\n",
       "         [1.0000, 0.6387, 0.5247]],\n",
       "\n",
       "        [[4.0000, 0.9371, 0.6556],\n",
       "         [5.0000, 0.4635, 0.4550],\n",
       "         [1.0000, 0.4162, 0.2843],\n",
       "         [7.0000, 0.3138, 0.1980],\n",
       "         [4.0000, 0.5725, 0.4980]]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_out_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.6387, 0.5247],\n",
       "         [5.0000, 0.4635, 0.4550],\n",
       "         [4.0000, 0.6826, 0.3051],\n",
       "         [4.0000, 0.9371, 0.6556],\n",
       "         [4.0000, 0.5725, 0.4980]],\n",
       "\n",
       "        [[7.0000, 0.3138, 0.1980],\n",
       "         [4.0000, 0.9371, 0.6556],\n",
       "         [5.0000, 0.4635, 0.4550],\n",
       "         [4.0000, 0.5725, 0.4980],\n",
       "         [1.0000, 0.4162, 0.2843]]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_tgt_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_tgt_dict = {'labels' : out_true[:, :, 0], 'boxes' : out_true[:, :, 1:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 'pred_logits': 각 class 에 대한 logit. 이후 softmax 를 통해 class 에 대한 확률 계산으로 사용된다.\n",
    "            - 'pred_boxes' :  예측한 bounding box. [x, y] 로 이뤄진다.\n",
    "        target: dict\n",
    "            - 'labels': 해당 segment 의 class\n",
    "            - 'boxes' : 해당 segment 의 bounding box [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 3])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_tgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-d022bffc8354>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mex_out_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex_tgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "ex_out_true = torch.index_select(ex_tgt, 0, true_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. with true output, can matcher find true indices?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = DETR_HEAD.to(device)\n",
    "loss_traj = []\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer,\n",
    "    lr_lambda=lambda i: 0.95 ** i,\n",
    "    last_epoch=-1,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "model.train()\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for i, batch in enumerate(train_loader_minibatch):\n",
    "    batch = {k : v.to(device) for k, v in batch.items()}\n",
    "    break\n",
    "    \n",
    "class_pred, box_pred = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_pred.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2528],\n",
       "         [261.0119, 253.2528],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2528],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2530],\n",
       "         [261.0119, 253.2528],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0120, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2528],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529]],\n",
       "\n",
       "        [[261.0118, 253.2530],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2528],\n",
       "         [261.0118, 253.2528],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2528],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2528],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529]]], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq * (0.5 - box_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = {'id' : [], 'class' : [], 'predictionstring' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_overlap(row):\n",
    "    \"\"\"\n",
    "    Calculates the overlap between prediction and\n",
    "    ground truth and overlap percentages used for determining\n",
    "    true positives.\n",
    "    \"\"\"\n",
    "    set_pred = set(row.predictionstring_pred.split(' '))\n",
    "    set_gt = set(row.predictionstring_gt.split(' '))\n",
    "    # Length of each and intersection\n",
    "    len_gt = len(set_gt)\n",
    "    len_pred = len(set_pred)\n",
    "    inter = len(set_gt.intersection(set_pred))\n",
    "    overlap_1 = inter / len_gt\n",
    "    overlap_2 = inter/ len_pred\n",
    "    return [overlap_1, overlap_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_pred, b = model(batch)\n",
    "c_pred = torch.argmax(c_pred, dim = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "= max_seq * (0.5 - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(batch):\n",
    "    # MOVE BATCH TO GPU AND INFER\n",
    "    ids = batch[\"input_ids\"].to(config['device'])\n",
    "    mask = batch[\"attention_mask\"].to(config['device'])\n",
    "    outputs = model(ids, attention_mask=mask, return_dict=False)\n",
    "    all_preds = torch.argmax(outputs[0], axis=-1).cpu().numpy() \n",
    "\n",
    "    # INTERATE THROUGH EACH TEXT AND GET PRED\n",
    "    predictions = []\n",
    "    for k,text_preds in enumerate(all_preds):\n",
    "        token_preds = [ids_to_labels[i] for i in text_preds]\n",
    "\n",
    "        prediction = []\n",
    "        word_ids = batch['wids'][k].numpy()  \n",
    "        previous_word_idx = -1\n",
    "        for idx,word_idx in enumerate(word_ids):                            \n",
    "            if word_idx == -1:\n",
    "                pass\n",
    "            elif word_idx != previous_word_idx:              \n",
    "                prediction.append(token_preds[idx])\n",
    "                previous_word_idx = word_idx\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/zzy990106/pytorch-ner-infer\n",
    "# changed a little bit (용현)\n",
    "\n",
    "df=valid_dataset\n",
    "loader=valid_loader\n",
    "\n",
    "# put model in eval mode\n",
    "model.eval()\n",
    "\n",
    "# calc prediction from model\n",
    "model_predict = []\n",
    "for batch in loader:\n",
    "    labels = inference(batch)\n",
    "    y_pred2.extend(labels)\n",
    "\n",
    "final_preds2 = []\n",
    "for i in range(len(df)):\n",
    "\n",
    "    idx = df.id.values[i]\n",
    "    #pred = [x.replace('B-','').replace('I-','') for x in y_pred2[i]]\n",
    "    pred = y_pred2[i] # Leave \"B\" and \"I\"\n",
    "    preds = []\n",
    "    j = 0\n",
    "    while j < len(pred):\n",
    "        cls = pred[j]\n",
    "        if cls == 'O': j += 1\n",
    "        else: cls = cls.replace('B','I') # spans start with B\n",
    "        end = j + 1\n",
    "        while end < len(pred) and pred[end] == cls:\n",
    "            end += 1\n",
    "\n",
    "        if cls != 'O' and cls != '' and end - j > 7:\n",
    "            final_preds2.append((idx, cls.replace('I-',''),\n",
    "                                 ' '.join(map(str, list(range(j, end))))))\n",
    "\n",
    "        j = end\n",
    "\n",
    "oof = pd.DataFrame(final_preds2)\n",
    "oof.columns = ['id','class','predictionstring']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def score_feedback_comp(pred_df, gt_df):\n",
    "    \"\"\"\n",
    "    A function that scores for the kaggle\n",
    "        Student Writing Competition\n",
    "        \n",
    "    Uses the steps in the evaluation page here:\n",
    "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
    "    \"\"\"\n",
    "    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df = pred_df[['id','class','predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df['pred_id'] = pred_df.index\n",
    "    gt_df['gt_id'] = gt_df.index\n",
    "    # Step 1. all ground truths and predictions for a given class are compared.\n",
    "    joined = pred_df.merge(gt_df,\n",
    "                           left_on=['id','class'],\n",
    "                           right_on=['id','discourse_type'],\n",
    "                           how='outer',\n",
    "                           suffixes=('_pred','_gt')\n",
    "                          )\n",
    "    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n",
    "    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n",
    "\n",
    "    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n",
    "\n",
    "    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n",
    "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
    "    # the prediction is a match and considered a true positive.\n",
    "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n",
    "    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n",
    "\n",
    "\n",
    "    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n",
    "    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n",
    "    tp_pred_ids = joined.query('potential_TP') \\\n",
    "        .sort_values('max_overlap', ascending=False) \\\n",
    "        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n",
    "\n",
    "    # 3. Any unmatched ground truths are false negatives\n",
    "    # and any unmatched predictions are false positives.\n",
    "    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n",
    "\n",
    "    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n",
    "    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n",
    "\n",
    "    # Get numbers of each type\n",
    "    TP = len(tp_pred_ids)\n",
    "    FP = len(fp_pred_ids)\n",
    "    FN = len(unmatched_gt_ids)\n",
    "    #calc microf1\n",
    "    my_f1_score = TP / (TP + 0.5*(FP+FN))\n",
    "    return my_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALID TARGETS\n",
    "valid = train_df.loc[train_df['id'].isin(IDS[valid_idx])]\n",
    "\n",
    "# OOF PREDICTIONS\n",
    "oof = get_predictions(test_dataset, testing_loader)\n",
    "\n",
    "# COMPUTE F1 SCORE\n",
    "f1s = []\n",
    "CLASSES = oof['class'].unique()\n",
    "print()\n",
    "for c in CLASSES:\n",
    "    pred_df = oof.loc[oof['class']==c].copy()\n",
    "    gt_df = valid.loc[valid['discourse_type']==c].copy()\n",
    "    f1 = score_feedback_comp(pred_df, gt_df)\n",
    "    print(c,f1)\n",
    "    f1s.append(f1)\n",
    "print()\n",
    "print('Overall',np.mean(f1s))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for submission\n",
    "# ref) https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615?scriptVersionId=83230719\n",
    "test_names, test_texts = [], []\n",
    "for f in list(os.listdir('../input/feedback-prize-2021/test')):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    test_texts.append(open('../input/feedback-prize-2021/test/' + f, 'r').read())\n",
    "test_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\n",
    "\n",
    "test_names, train_texts = [], []\n",
    "for f in tqdm(list(os.listdir('../input/feedback-prize-2021/train'))):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    train_texts.append(open('../input/feedback-prize-2021/train/' + f, 'r').read())\n",
    "train_text_df = pd.DataFrame({'id': test_names, 'text': train_texts})\n",
    "\n",
    "\n",
    "# # TEST DATASET\n",
    "# test_texts_set = dataset(test_texts, tokenizer, config['max_length'], True)\n",
    "# test_texts_loader = DataLoader(test_texts_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define config\n",
    "config = {'model_name': MODEL_NAME,   \n",
    "          'max_length': 1024,\n",
    "          'train_batch_size':4,\n",
    "          'valid_batch_size':4,\n",
    "          'epochs':5,\n",
    "          'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n",
    "          'max_grad_norm':10,\n",
    "          'device': 'cuda' if cuda.is_available() else 'cpu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666cc845bdc54244acb9aff81cef5626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51674fa78df049babf1e80f7e1b66fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e6301eb1b248a7aa55f1f9062defcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/846k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055c9165c0c64092805d94e89973a1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/775 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9508107e9dd543fa9c6ed48270f8d929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdForTokenClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# if you are first running this code, please download LM.\n",
    "MODEL_NAME = 'google/bigbird-roberta-base' # choose which model to download\n",
    "DOWNLOADED_MODEL_PATH = 'model'            # choose where to download the model\n",
    "\n",
    "if DOWNLOADED_MODEL_PATH == 'model':\n",
    "    os.mkdir('model')\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True) #!# add_prefix_space?\n",
    "    tokenizer.save_pretrained('model')\n",
    "\n",
    "    config_model = AutoConfig.from_pretrained(MODEL_NAME) \n",
    "    config_model.num_labels = 15\n",
    "    config_model.save_pretrained('model')\n",
    "\n",
    "    backbone = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, \n",
    "                                                               config=config_model)\n",
    "    backbone.save_pretrained('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!# implement tokenizer \\n\\n, to see paragraph information\n",
    "# ref) https://github.com/huggingface/tokenizers/issues/247\n",
    "# ref) https://www.kaggle.com/c/feedback-prize-2021/discussion/296713\n",
    "tokenizer.decode(tokenizer(r'\\\\n\\\\n', return_offsets_mapping=True)['input_ids'])\n",
    "\n",
    "'''add new special token to model and tokenizer'''\n",
    "special_tokens_dict = {'additional_special_tokens': [r'\\\\n\\\\n']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i) \n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:                \n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            else:  \n",
    "                label_ids.append(label[word_idx])\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snu36",
   "language": "python",
   "name": "snu36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
