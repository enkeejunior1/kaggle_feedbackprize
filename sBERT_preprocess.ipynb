{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd #!#\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from einops import rearrange, repeat, reduce\n",
    "\n",
    "from joblib import Parallel, delayed # https://www.notion.so/joblib-da8f5ee8dbd44da19b36da04bd657bb1\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import tez # https://www.notion.so/Tez-093f1f31cba646e3963108294563ddd1\n",
    "from tez.callbacks import Callback\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "from sklearn import metrics\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_JOBS = 12 #1# hyper parameter setting\n",
    "args = set_args() #1# hyper parameter setting\n",
    "seed_everything(args.seed) #2# fix seed for reproducability\n",
    "os.makedirs(args.output_path, exist_ok = True)\n",
    "\n",
    "df = pd.read_csv(os.path.join(os.getcwd(), f'input/train_{5}folds.csv')) #3# read kfold data\n",
    "\n",
    "args.model = './model' #!# big bird. 이후에 long-former 로 바꿔주세요.\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
    "samples = prepare_training_data(df.iloc[:100], tokenizer, args, num_jobs=NUM_JOBS)\n",
    "collate = Collate(tokenizer)\n",
    "\n",
    "dataset = FeedbackDataset(samples, args.max_len, tokenizer)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size = args.batch_size, collate_fn = collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model were not used when initializing BigBirdModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "CONFIG = AutoConfig.from_pretrained(args.model, output_hidden_states = True) # ref) https://github.com/huggingface/transformers/issues/1827\n",
    "LM = AutoModel.from_pretrained(args.model, config = CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(target_id_map)\n",
    "d_model = LM.config.hidden_size\n",
    "\n",
    "HEAD = Head(d_model, num_labels)\n",
    "FEED_BACK_MODEL = FeedbackModel(LM, HEAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "model = FEED_BACK_MODEL.to(device)\n",
    "\n",
    "EPOCH = 5\n",
    "LR = 2e-5\n",
    "optimizer = AdamW(model.parameters(), lr = LR)\n",
    "\n",
    "loss = 0\n",
    "loss_traj = []\n",
    "\n",
    "model.train()\n",
    "for _ in tqdm(range(EPOCH)):\n",
    "    for batch in dataloader:\n",
    "        batch_device = dict()\n",
    "        batch_device['ids'] = batch['ids'].to(device)\n",
    "        batch_device['type'] = [type_.to(device) for type_ in batch['type']]\n",
    "        batch_device['mask'] = batch['mask'].to(device)\n",
    "        batch_device['targets'] = batch['targets'].to(device)\n",
    "\n",
    "        pred_logits = model(batch_device)\n",
    "        loss = calc_loss(pred_logits, batch_device['targets'], batch_device['mask'].gt(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_traj.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_traj)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(logits, labels, mask, num_labels = 16): #!# test code\n",
    "    '''CrossEntropy 를 계산한다.\n",
    "    '''\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    mask_label = repeat(mask, 'b s -> b s c', c = num_labels)\n",
    "    unfold_logits = torch.masked_select(logits, mask_label).view(-1, num_labels).softmax(dim = -1)\n",
    "    unfold_labels = torch.masked_select(labels, mask)\n",
    "    return loss_func(unfold_logits, unfold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackModel(nn.Module):\n",
    "    def __init__(self, lm, head):\n",
    "        super(FeedbackModel, self).__init__()\n",
    "        self.lm = lm\n",
    "        self.head = head\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.lm(input_ids = x['ids'], attention_mask = x['mask'])[0] # out \n",
    "        out = self.head(x['type'], out)\n",
    "        return out\n",
    "\n",
    "# s : sentence, t : token, b : batch, d : d_model\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, d_model, num_labels):\n",
    "        super(Head, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.num_labels_sent = (num_labels - 2) / 2\n",
    "        \n",
    "        self.fc_layer_start = nn.Linear(d_model, self.num_labels)\n",
    "        self.fc_layer_sent  = nn.Linear(d_model, self.num_labels) # I-label, O, PAD #!# \n",
    "        \n",
    "    def forward(self, type_list:list, out:torch.tensor) -> torch.tensor:\n",
    "        global device\n",
    "        batch_size, seq_len, d_model = out.shape\n",
    "        pred_class = torch.zeros(batch_size, seq_len, self.num_labels).to(device)\n",
    "        \n",
    "        start_list, sent_list = self._get_sent_idx(type_list, seq_len)\n",
    "        for i, (start, sent) in enumerate(zip(start_list, sent_list)):\n",
    "            start_token = self._pool_by_mask(out[i], start.type(torch.float))\n",
    "            sent_token = self._pool_by_mask(out[i], sent.type(torch.float))\n",
    "            \n",
    "            pred_class[i] = pred_class[i] + self.fc_layer_start(start_token)\n",
    "            pred_class[i] = pred_class[i] + self.fc_layer_sent(sent_token) #!# sent 문장 성분은 num_label 말고 num_labels_sent만 활용해보기\n",
    "            \n",
    "        return pred_class\n",
    "    \n",
    "    def _get_sent_idx(self, type_list:list, seq_len):\n",
    "        start_list = []\n",
    "        sent_list = []\n",
    "        \n",
    "        for type_ in type_list:\n",
    "            _, i = torch.max(type_, dim = 1)\n",
    "            start = F.one_hot(i, num_classes = type_.size(-1))\n",
    "            sent = type_ - start\n",
    "            start_list.append(start)\n",
    "            sent_list.append(sent)\n",
    "        \n",
    "        return start_list, sent_list\n",
    "    \n",
    "    def _pool_by_mask(self, x, mask): #!# todo : mean pooling 적용해보기 #!# torch.masked_select 사용해보기\n",
    "        '''Parameters\n",
    "            x : BERT 를 지나온 encoded vector\n",
    "            mask : 유효한 token 을 1 로 표기한 mask.\n",
    "                ex) [0,0,0,1,1,1,0,0,0]\n",
    "        '''\n",
    "        assert mask.dtype == torch.float # for einsum operation\n",
    "        x = torch.einsum('st,td->sd', mask, x) # extract encoded vector : [sentence, seq_len] * [seq_len, d_model] -> [sentence, d_model]\n",
    "        x = torch.einsum('si,sj->sij', mask, x).sum(dim = 0) # combine encoded vector : [sentence, d_model]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class set_args: #1#\n",
    "    seed: int = 42\n",
    "    fold: int = 0\n",
    "    kfold: int = 5\n",
    "    model = 'allenai/longformer-base-4096' #!# 모델 이름\n",
    "    lr: float = 1e-5\n",
    "    output_path = os.path.join(os.getcwd(), 'model') # '../model'\n",
    "    input_path = os.path.join(os.getcwd(), 'input') # '../input'\n",
    "    max_len: int = 1024\n",
    "    batch_size: int = 8\n",
    "    valid_batch_size: int = 8\n",
    "    epochs: int = 20\n",
    "    accumulation_steps = 1 #!# 이게 뭐지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "class Collate:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n",
    "        output[\"type\"] = [sample[\"type\"] for sample in batch] # sample[\"type\"] = [num_sent, seq_len]\n",
    "        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n",
    "        output['targets'] = [sample[\"targets\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"ids\"]])\n",
    "\n",
    "        output['ids'] = pad_sequence(output['ids'], batch_first = True)\n",
    "        output['mask'] = pad_sequence(output['mask'], batch_first = True)\n",
    "        output['targets'] = pad_sequence(output['targets'], batch_first = True)\n",
    "        \n",
    "        # add padding... #!# readability\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"type\"] = [torch.cat((s, torch.full((s.size(-2), batch_max - s.size(-1)), 0)), dim = -1) for s in output[\"type\"]]\n",
    "        else:\n",
    "            output[\"type\"] = [torch.cat((torch.full((s.size(-2), batch_max - s.size(-1)), 0), s), dim = -1) for s in output[\"type\"]]\n",
    "        \n",
    "        # add padding... #!# readability\n",
    "#         if self.tokenizer.padding_side == \"right\":\n",
    "#             output[\"ids\"] = [torch.cat((s, torch.full((batch_max - s.size(-1),), self.tokenizer.pad_token_id)), dim = -1) for s in output[\"ids\"]]\n",
    "#             output[\"type\"] = [torch.cat((s, torch.full((s.size(-2), batch_max - s.size(-1)), 0)), dim = -1) for s in output[\"type\"]]\n",
    "#             output[\"mask\"] = [torch.cat((s, torch.full((batch_max - s.size(-1),), 0)), dim = -1) for s in output[\"mask\"]]\n",
    "#         else:\n",
    "#             output[\"ids\"] = [torch.cat((torch.full((batch_max - s.size(-1),), self.tokenizer.pad_token_id), s), dim = -1) for s in output[\"ids\"]]\n",
    "#             output[\"type\"] = [torch.cat((torch.full((s.size(-2), batch_max - s.size(-1)), 0), s), dim = -1) for s in output[\"type\"]]\n",
    "#             output[\"mask\"] = [torch.cat((torch.full((batch_max - s.size(-1),), 0), s), dim = -1) for s in output[\"mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"ids\"] = output[\"ids\"].type(torch.long)\n",
    "        output[\"mask\"] = output[\"mask\"].type(torch.long)\n",
    "        output[\"targets\"] = output[\"targets\"].type(torch.long)\n",
    "        output[\"type\"] = [token_type.type(torch.long) for token_type in output[\"type\"]]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackDataset(Dataset):\n",
    "    def __init__(self, samples, max_len, tokenizer):\n",
    "        super(FeedbackDataset, self).__init__() #!#\n",
    "        self.samples = samples\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.length = len(samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.samples[idx][\"input_ids\"]\n",
    "        input_labels = [target_id_map[x] for x in self.samples[idx][\"input_labels\"]]\n",
    "        input_type_ids = self.samples[idx][\"token_type_ids\"]\n",
    "        other_label_id = target_id_map[\"O\"]\n",
    "        padding_label_id = target_id_map[\"PAD\"]\n",
    "\n",
    "        # add start token id to the input_ids\n",
    "        input_ids = [self.tokenizer.cls_token_id] + input_ids\n",
    "        input_labels = [other_label_id] + input_labels\n",
    "        input_type_ids = [[0] + type_ids for type_ids in input_type_ids]\n",
    "        \n",
    "        # truncate the input if the text is longer than max_len\n",
    "        if len(input_ids) > self.max_len - 1:\n",
    "            input_ids = input_ids[: self.max_len - 1]\n",
    "            input_labels = input_labels[: self.max_len - 1]\n",
    "            input_type_ids = [type_ids[: self.max_len - 1] for type_ids in input_type_ids]\n",
    "\n",
    "        # add end token id to the input_ids\n",
    "        input_ids = input_ids + [self.tokenizer.sep_token_id]\n",
    "        input_labels = input_labels + [other_label_id]\n",
    "        input_type_ids = [type_ids + [0] for type_ids in input_type_ids]\n",
    "\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        \n",
    "        # padding\n",
    "        #!# 굳이 padding 을 max_len 까지 전부 다 해야하나..?\n",
    "#         padding_length = self.max_len - len(input_ids)\n",
    "#         if padding_length > 0:\n",
    "#             if self.tokenizer.padding_side == \"right\":\n",
    "#                 input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n",
    "#                 input_labels = input_labels + [padding_label_id] * padding_length\n",
    "#                 input_type_ids = input_type_ids + [0] * padding_length\n",
    "#                 attention_mask = attention_mask + [0] * padding_length\n",
    "                \n",
    "#             else:\n",
    "#                 input_ids = [self.tokenizer.pad_token_id] * padding_length + input_ids\n",
    "#                 input_labels = [padding_label_id] * padding_length + input_labels\n",
    "#                 input_type_ids = [0] * padding_length + input_type_ids\n",
    "#                 attention_mask = [0] * padding_length + attention_mask\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"type\": torch.tensor(input_type_ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"targets\": torch.tensor(input_labels, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(df: pd.DataFrame, tokenizer, args, num_jobs):\n",
    "    '''_prepare_training_data_helper 를 병렬처리\n",
    "        Parameters\n",
    "            tokenizer : 전처리에 활용되는 tokenizer\n",
    "            num_jobs : number of 병렬 처리 workers\n",
    "        \n",
    "        Returns\n",
    "            training_samples(list) : \n",
    "    '''\n",
    "    training_samples = []\n",
    "    train_ids = df[\"id\"].unique()\n",
    "\n",
    "    train_ids_splits = np.array_split(train_ids, num_jobs)\n",
    "\n",
    "    results = Parallel(n_jobs=num_jobs, backend=\"multiprocessing\")(\n",
    "        delayed(_prepare_training_data_helper)(df, tokenizer, args, idx) for idx in train_ids_splits\n",
    "    )\n",
    "    for result in results:\n",
    "        training_samples.extend(result)\n",
    "\n",
    "    return training_samples\n",
    "\n",
    "def _prepare_training_data_helper(df, tokenizer, args, train_ids = None):\n",
    "    training_samples = []\n",
    "    for idx in df['id'].unique(): #!# replace df['id'].unique() -> train_ids\n",
    "        filename = os.path.join('./input', \"train\", idx + \".txt\")\n",
    "        with open(filename, \"r\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "\n",
    "        input_ids = encoded_text[\"input_ids\"]\n",
    "        offset_mapping = encoded_text[\"offset_mapping\"]\n",
    "\n",
    "        sample = {\n",
    "            \"id\": idx,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"text\": text,\n",
    "            \"offset_mapping\": offset_mapping,\n",
    "        }\n",
    "\n",
    "        # token_type_ids 만들기\n",
    "        #!# test code\n",
    "        token_type_ids_list = []\n",
    "        processed_text, processed_idx_list = _replace_awkend(text) # nltk 를 위한 preprocessing\n",
    "        start_idx_list, end_idx_list = _extract_sentence_idx(processed_text) # 문장 index 추출하기\n",
    "        start_idx_list = _postprocess_sent_idx(start_idx_list, processed_idx_list) # preprocessing 이전의 index 로 되돌리기\n",
    "        end_idx_list = _postprocess_sent_idx(end_idx_list, processed_idx_list) # preprocessing 이전의 index 로 되돌리기\n",
    "        \n",
    "        for start_idx, end_idx in zip(start_idx_list, end_idx_list):\n",
    "            text_type_ids = [0] * len(text)\n",
    "            text_type_ids[start_idx:end_idx] = [1] * (end_idx - start_idx) #!# start_idx:end_idx 로 잘 추출되는지 확인\n",
    "\n",
    "            token_type_ids = []\n",
    "            for i, (offset1, offset2) in enumerate(encoded_text[\"offset_mapping\"]):\n",
    "                if any(text_type_ids[offset1:offset2]): # 1개의 text 라도 0 이외의 값\n",
    "                    if len(text[offset1:offset2].split()) > 0: #!# 1개의 token 은 include\n",
    "                        token_type_ids.append(1)\n",
    "                    else:\n",
    "                        token_type_ids.append(0)\n",
    "                else:\n",
    "                    token_type_ids.append(0)\n",
    "                        \n",
    "            assert len(token_type_ids) == len(encoded_text[\"offset_mapping\"])\n",
    "            token_type_ids_list.append(token_type_ids)\n",
    "        sample[\"token_type_ids\"] = token_type_ids_list\n",
    "        \n",
    "        # input_labels\n",
    "        temp_df = df[df['id'] == idx]\n",
    "        input_labels = copy.deepcopy(input_ids)\n",
    "        for k in range(len(input_labels)):\n",
    "            input_labels[k] = \"O\"\n",
    "            \n",
    "        for _, row in temp_df.iterrows():\n",
    "            text_labels = [0] * len(text)\n",
    "            discourse_start = int(row[\"discourse_start\"])\n",
    "            discourse_end = int(row[\"discourse_end\"])\n",
    "            prediction_label = row[\"discourse_type\"]\n",
    "            text_labels[discourse_start:discourse_end] = [1] * (discourse_end - discourse_start)\n",
    "            target_idx = []\n",
    "            for map_idx, (offset1, offset2) in enumerate(encoded_text[\"offset_mapping\"]):\n",
    "                if sum(text_labels[offset1:offset2]) > 0:\n",
    "                    if len(text[offset1:offset2].split()) > 0:\n",
    "                        target_idx.append(map_idx)\n",
    "\n",
    "            targets_start = target_idx[0]\n",
    "            targets_end = target_idx[-1]\n",
    "            pred_start = \"B-\" + prediction_label\n",
    "            pred_end = \"I-\" + prediction_label\n",
    "            input_labels[targets_start] = pred_start\n",
    "            input_labels[targets_start + 1 : targets_end + 1] = [pred_end] * (targets_end - targets_start)\n",
    "        sample[\"input_ids\"] = input_ids\n",
    "        sample[\"input_labels\"] = input_labels\n",
    "\n",
    "        training_samples.append(sample)\n",
    "    return training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _replace_awkend(text):\n",
    "    '''\"문장.문장\", \"문장 .문장\" 을 \"문장. 문장\" 으로 바꿔준다.\n",
    "        Parameters\n",
    "            - text (str) : \"문장 .문장\", \"문장.문장\"\n",
    "        Return\n",
    "            - text (str) : \"문장. 문장\"\n",
    "        \n",
    "    nltk 의 nltk.sent_tokenize() 는 문장    \n",
    "    cf) \"U.S. gov\" 를 \"U. S. gov\" 로 바꾸지만, nltk 는 다행히 후자를 하나의 문장으로 취급한다.\n",
    "    '''\n",
    "    # \"문장 .문장\"\n",
    "    text = re.sub(r' \\.', r'. ', text) \n",
    "    \n",
    "    # \"문장.문장\"\n",
    "    replace_token = re.findall(r'\\w\\.\\w', text) \n",
    "    replace_idx = [text.index(token) + i + 1 for i, token in enumerate(replace_token)]\n",
    "    for idx in replace_idx:\n",
    "        text = text[:idx] + '. ' + text[idx+1:]        \n",
    "    \n",
    "    return text, replace_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sentence index\n",
    "def _extract_sentence_idx(text):\n",
    "    '''nltk 를 활용해서 문장을 추출한다.\n",
    "        Parameters\n",
    "            - text : nltk 가 오작동하지 않도록 전처리된 자료\n",
    "        Returns\n",
    "            - start_idx_list (list) : 해당 문장이 시작하는 index\n",
    "            - end_idx_list (list) : 해당 문장이 끝나는 index\n",
    "        \n",
    "        Assert\n",
    "            - 각 i 에 대해서 text[start_idx_list[i]:end_idx_list[i]] 는 \n",
    "                하나의 문장에 대응한다.\n",
    "    '''\n",
    "    sent_list = nltk.sent_tokenize(text)\n",
    "    \n",
    "    start_idx_list = []\n",
    "    end_idx_list = []\n",
    "    for i, sent in enumerate(sent_list):\n",
    "        start_idx_list.append(text.index(sent))\n",
    "        end_idx_list.append(start_idx_list[-1] + len(sent))\n",
    "\n",
    "    for i, _ in enumerate(sent_list):\n",
    "        assert text[start_idx_list[i]:end_idx_list[i]] == sent_list[i]\n",
    "        \n",
    "    return start_idx_list, end_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import warnings\n",
    "\n",
    "def _postprocess_sent_idx(sent_idx_list, processed_idx_list):\n",
    "    postprocess_sent_idx_list = copy.deepcopy(sent_idx_list)\n",
    "    for i, sent_idx in enumerate(sent_idx_list):\n",
    "        for processed_idx in processed_idx_list:\n",
    "            if sent_idx > processed_idx:\n",
    "                postprocess_sent_idx_list[i] -= 1\n",
    "        \n",
    "    return postprocess_sent_idx_list\n",
    "\n",
    "# original_start_idx_list = _postprocess_sent_idx(start_idx_list, processed_idx_list)\n",
    "# original_end_idx_list = _postprocess_sent_idx(end_idx_list, processed_idx_list)\n",
    "\n",
    "# # test code : 추출 목표가 추출 결과와 일치하는가?\n",
    "# diff_list = []\n",
    "# for i in range(len(start_idx_list)):\n",
    "#     start_idx = start_idx_list[i]\n",
    "#     end_idx = end_idx_list[i]\n",
    "#     original_start_idx = original_start_idx_list[i]\n",
    "#     original_end_idx = original_end_idx_list[i]\n",
    "    \n",
    "#     if processed_text[start_idx:end_idx] != text[original_start_idx:original_end_idx]:\n",
    "#         diff_list.append([processed_text[start_idx:end_idx], text[original_start_idx:original_end_idx]])\n",
    "    \n",
    "# if len(diff_list) != 0:\n",
    "#     for diff in diff_list:\n",
    "#         print(f\"{diff[0]}\\n{diff[1]}\")\n",
    "#     warnings.warn(\"exist different sentence (processed, original)\"+f\"{diff_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id_map = {\n",
    "    \"B-Lead\": 0,\n",
    "    \"I-Lead\": 1,\n",
    "    \"B-Position\": 2,\n",
    "    \"I-Position\": 3,\n",
    "    \"B-Evidence\": 4,\n",
    "    \"I-Evidence\": 5,\n",
    "    \"B-Claim\": 6,\n",
    "    \"I-Claim\": 7,\n",
    "    \"B-Concluding Statement\": 8,\n",
    "    \"I-Concluding Statement\": 9,\n",
    "    \"B-Counterclaim\": 10,\n",
    "    \"I-Counterclaim\": 11,\n",
    "    \"B-Rebuttal\": 12,\n",
    "    \"I-Rebuttal\": 13,\n",
    "    \"O\": 14,\n",
    "    \"PAD\": -100,\n",
    "}\n",
    "\n",
    "id_target_map = {v: k for k, v in target_id_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testcode_prepare_training_data_helper(samples:list):\n",
    "    '''문장이 제대로 추출됐는지 눈으로 확인한다. \n",
    "        Parameters\n",
    "            - samples (dict)\n",
    "                > input_ids : 각 token 들의 index. \n",
    "                > token_type_ids : 문장에 대응하는 token 위치를 1 로 저장해둔 list.\n",
    "                    ex) [0,0,0,1,1,1,1,0,0,0,...]\n",
    "        Returns\n",
    "            - decoded_samples (list) : 자연어로 변환된 결과를 담은 list.\n",
    "    '''\n",
    "    decoded_samples = []\n",
    "    for sample in samples:\n",
    "        extracted_input_ids = torch.tensor(sample['input_ids']) * torch.tensor(sample['token_type_ids'])\n",
    "        for ext_ids in extracted_input_ids:\n",
    "            decoded_samples.append(tokenizer.decode(ext_ids))\n",
    "    \n",
    "    return decoded_samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snu36",
   "language": "python",
   "name": "snu36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
