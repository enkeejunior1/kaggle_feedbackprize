{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd #!#\n",
    "import random\n",
    "from einops import rearrange, repeat, reduce\n",
    "\n",
    "from joblib import Parallel, delayed # https://www.notion.so/joblib-da8f5ee8dbd44da19b36da04bd657bb1\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import tez # https://www.notion.so/Tez-093f1f31cba646e3963108294563ddd1\n",
    "from tez.callbacks import Callback\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "from sklearn import metrics\n",
    "\n",
    "gc.enable()\n",
    "sys.path.append(\"../input/tez-lib/\") #!#\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(os.getcwd(), f'input/train_{5}folds.csv')) \n",
    "\n",
    "training_samples = []\n",
    "\n",
    "for idx in df['id'].unique():\n",
    "    filename = os.path.join('./input', \"train\", idx + \".txt\")\n",
    "    with open(filename, \"r\") as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    encoded_text = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=False,\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded_text[\"input_ids\"]\n",
    "    offset_mapping = encoded_text[\"offset_mapping\"]\n",
    "    \n",
    "    sample = {\n",
    "        \"id\": idx,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"text\": text,\n",
    "        \"offset_mapping\": offset_mapping,\n",
    "    }\n",
    "    \n",
    "    # token_type_ids 만들기\n",
    "    token_type_ids_list = []\n",
    "    processed_text, processed_idx_list = _replace_awkend(text) # nltk 를 위한 preprocessing\n",
    "    start_idx_list, end_idx_list = _extract_sentence_idx(processed_text) # 문장 index 추출하기\n",
    "    for start_idx, end_idx in zip(start_idx_list, end_idx_list):\n",
    "        text_type_ids = [0] * len(text)\n",
    "        text_type_ids[start_idx:end_idx] = [1] * (start_idx - end_idx) #!# start_idx:end_idx 로 잘 추출되는지 확인\n",
    "        \n",
    "        token_type_ids = []\n",
    "        for i, (offset1, offset2) in enumerate(encoded_text[\"offset_mapping\"]):\n",
    "            if any(text_type_ids[offset1:offset2]): # 1개의 text 라도 0 이외의 값\n",
    "                if len(text[offset1:offset2].split()) > 0:\n",
    "                    token_type_ids.append(1)\n",
    "                    \n",
    "            else:\n",
    "                token_type_ids\n",
    "        \n",
    "        token_type_ids_list.append()\n",
    "        \n",
    "    for _, row in temp_df.iterrows():\n",
    "        text_labels = [0] * len(text)\n",
    "        discourse_start = int(row[\"discourse_start\"])\n",
    "        discourse_end = int(row[\"discourse_end\"])\n",
    "        prediction_label = row[\"discourse_type\"]\n",
    "        text_labels[discourse_start:discourse_end] = [1] * (discourse_end - discourse_start)\n",
    "        target_idx = []\n",
    "        for map_idx, (offset1, offset2) in enumerate(encoded_text[\"offset_mapping\"]):\n",
    "            if sum(text_labels[offset1:offset2]) > 0:\n",
    "                if len(text[offset1:offset2].split()) > 0:\n",
    "                    target_idx.append(map_idx)\n",
    "\n",
    "        targets_start = target_idx[0]\n",
    "        targets_end = target_idx[-1]\n",
    "        pred_start = \"B-\" + prediction_label\n",
    "        pred_end = \"I-\" + prediction_label\n",
    "        input_labels[targets_start] = pred_start\n",
    "        input_labels[targets_start + 1 : targets_end + 1] = [pred_end] * (targets_end - targets_start)\n",
    "        sample[\"input_ids\"] = input_ids\n",
    "        sample[\"input_labels\"] = input_labels\n",
    "        training_samples.append(sample)\n",
    "        \n",
    "        \n",
    "    if len(processed_idx_list) != 0:\n",
    "        start_idx_list = _postprocess_sent_idx(start_idx_list, processed_idx_list)\n",
    "        end_idx_list = _postprocess_sent_idx(end_idx_list, processed_idx_list)\n",
    "    \n",
    "    # input_label 만들기\n",
    "    input_labels = copy.deepcopy(input_ids)\n",
    "    for i, _ in enumerate(input_labels):\n",
    "        input_labels[i] = \"O\"\n",
    "        \n",
    "    temp_df = df[df['id'] == idx]\n",
    "    for _, row in temp_df.iterrows():\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "        temp_df = df[df[\"id\"] == idx]\n",
    "        for _, row in temp_df.iterrows():\n",
    "            text_labels = [0] * len(text)\n",
    "            discourse_start = int(row[\"discourse_start\"])\n",
    "            discourse_end = int(row[\"discourse_end\"])\n",
    "            prediction_label = row[\"discourse_type\"]\n",
    "            text_labels[discourse_start:discourse_end] = [1] * (discourse_end - discourse_start)\n",
    "            target_idx = []\n",
    "            for map_idx, (offset1, offset2) in enumerate(encoded_text[\"offset_mapping\"]):\n",
    "                if sum(text_labels[offset1:offset2]) > 0:\n",
    "                    if len(text[offset1:offset2].split()) > 0:\n",
    "                        target_idx.append(map_idx)\n",
    "\n",
    "            targets_start = target_idx[0]\n",
    "            targets_end = target_idx[-1]\n",
    "            pred_start = \"B-\" + prediction_label\n",
    "            pred_end = \"I-\" + prediction_label\n",
    "            input_labels[targets_start] = pred_start\n",
    "            input_labels[targets_start + 1 : targets_end + 1] = [pred_end] * (targets_end - targets_start)\n",
    "        sample[\"input_ids\"] = input_ids\n",
    "        sample[\"input_labels\"] = input_labels\n",
    "        training_samples.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-233-e5d58e3a23d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# text = text_list[3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprocessed_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_idx_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_replace_awkend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_idx_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for text in text_list:\n",
    "    # text = text_list[3]\n",
    "    processed_text, processed_idx_list = _replace_awkend(text)\n",
    "    assert len(processed_idx_list) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _replace_awkend(text):\n",
    "    '''\"문장.문장\", \"문장 .문장\" 을 \"문장. 문장\" 으로 바꿔준다.\n",
    "        Parameters\n",
    "            - text (str) : \"문장 .문장\", \"문장.문장\"\n",
    "        Return\n",
    "            - text (str) : \"문장. 문장\"\n",
    "        \n",
    "    nltk 의 nltk.sent_tokenize() 는 문장    \n",
    "    cf) \"U.S. gov\" 를 \"U. S. gov\" 로 바꾸지만, nltk 는 다행히 후자를 하나의 문장으로 취급한다.\n",
    "    '''\n",
    "    # \"문장 .문장\"\n",
    "    text = re.sub(r' \\.', r'. ', text) \n",
    "    \n",
    "    # \"문장.문장\"\n",
    "    replace_token = re.findall(r'\\w\\.\\w', text) \n",
    "    replace_idx = [text.index(token) + i + 1 for i, token in enumerate(replace_token)]\n",
    "    for idx in replace_idx:\n",
    "        text = text[:idx] + '. ' + text[idx+1:]        \n",
    "    \n",
    "    return text, replace_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sentence index\n",
    "def _extract_sentence_idx(text):\n",
    "    '''nltk 를 활용해서 문장을 추출한다.\n",
    "        Parameters\n",
    "            - text : nltk 가 오작동하지 않도록 전처리된 자료\n",
    "        Returns\n",
    "            - start_idx_list (list) : 해당 문장이 시작하는 index\n",
    "            - end_idx_list (list) : 해당 문장이 끝나는 index\n",
    "        \n",
    "        Assert\n",
    "            - 각 i 에 대해서 text[start_idx_list[i]:end_idx_list[i]] 는 \n",
    "                하나의 문장에 대응한다.\n",
    "    '''\n",
    "    sent_list = nltk.sent_tokenize(text)\n",
    "    \n",
    "    start_idx_list = []\n",
    "    end_idx_list = []\n",
    "    for i, sent in enumerate(sent_list):\n",
    "        start_idx_list.append(text.index(sent))\n",
    "        end_idx_list.append(start_idx_list[-1] + len(sent))\n",
    "\n",
    "    for i, _ in enumerate(sent_list):\n",
    "        assert text[start_idx_list[i]:end_idx_list[i]] == sent_list[i]\n",
    "        \n",
    "    return start_idx_list, end_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a recent article I read it said \"2017 U. S. Cell Phone and Driving Statistics.\n",
      "In a recent article I read it said \"2017 U.S. Cell Phone and Driving Statistics.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import warnings\n",
    "\n",
    "def _postprocess_sent_idx(sent_idx_list, processed_idx_list):\n",
    "    postprocess_sent_idx_list = copy.deepcopy(sent_idx_list)\n",
    "    for i, sent_idx in enumerate(sent_idx_list):\n",
    "        for processed_idx in processed_idx_list:\n",
    "            if sent_idx > processed_idx:\n",
    "                postprocess_sent_idx_list[i] -= 1\n",
    "                \n",
    "    # test code : 추출 목표가 추출 결과와 일치하지 않으면 그 부분을 반환한다.\n",
    "    diff_list = []\n",
    "    for i in range(len(start_idx_list)):\n",
    "        start_idx = start_idx_list[i]\n",
    "        end_idx = end_idx_list[i]\n",
    "        original_start_idx = original_start_idx_list[i]\n",
    "        original_end_idx = original_end_idx_list[i]\n",
    "\n",
    "        if processed_text[start_idx:end_idx] != text[original_start_idx:original_end_idx]:\n",
    "            diff_list.append([processed_text[start_idx:end_idx], text[original_start_idx:original_end_idx]])\n",
    "\n",
    "    if len(diff_list) != 0:\n",
    "        for diff in diff_list:\n",
    "            print(f\"{diff[0]}\\n{diff[1]}\")\n",
    "        \n",
    "    return postprocess_sent_idx_list\n",
    "\n",
    "original_start_idx_list = _postprocess_sent_idx(start_idx_list, processed_idx_list)\n",
    "original_end_idx_list = _postprocess_sent_idx(end_idx_list, processed_idx_list)\n",
    "\n",
    "# test code : 추출 목표가 추출 결과와 일치하는가?\n",
    "diff_list = []\n",
    "for i in range(len(start_idx_list)):\n",
    "    start_idx = start_idx_list[i]\n",
    "    end_idx = end_idx_list[i]\n",
    "    original_start_idx = original_start_idx_list[i]\n",
    "    original_end_idx = original_end_idx_list[i]\n",
    "    \n",
    "    if processed_text[start_idx:end_idx] != text[original_start_idx:original_end_idx]:\n",
    "        diff_list.append([processed_text[start_idx:end_idx], text[original_start_idx:original_end_idx]])\n",
    "    \n",
    "if len(diff_list) != 0:\n",
    "    for diff in diff_list:\n",
    "        print(f\"{diff[0]}\\n{diff[1]}\")\n",
    "    warnings.warn(\"exist different sentence (processed, original)\"+f\"{diff_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id_map = {\n",
    "    \"B-Lead\": 0,\n",
    "    \"I-Lead\": 1,\n",
    "    \"B-Position\": 2,\n",
    "    \"I-Position\": 3,\n",
    "    \"B-Evidence\": 4,\n",
    "    \"I-Evidence\": 5,\n",
    "    \"B-Claim\": 6,\n",
    "    \"I-Claim\": 7,\n",
    "    \"B-Concluding Statement\": 8,\n",
    "    \"I-Concluding Statement\": 9,\n",
    "    \"B-Counterclaim\": 10,\n",
    "    \"I-Counterclaim\": 11,\n",
    "    \"B-Rebuttal\": 12,\n",
    "    \"I-Rebuttal\": 13,\n",
    "    \"O\": 14,\n",
    "    \"PAD\": -100,\n",
    "}\n",
    "\n",
    "id_target_map = {v: k for k, v in target_id_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token offset -> start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randint(0, 14, (1, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [id_target_map[int(p)] for p in a[0][1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lead'"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1914"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "phrase_preds = []\n",
    "while idx < len(b):\n",
    "    start, _ = offset_mapping[idx]\n",
    "    if preds[idx] != \"O\":\n",
    "        label = preds[idx][2:] #!# [2:] 는 왜 붙은걸까\n",
    "    else:\n",
    "        label = \"O\"\n",
    "    phrase_scores = []\n",
    "    phrase_scores.append(sample_pred_scores[idx])\n",
    "    \n",
    "    # label을 추출... \n",
    "    \n",
    "    idx += 1\n",
    "    while idx < len(offset_mapping):\n",
    "        if label == \"O\":\n",
    "            matching_label = \"O\"\n",
    "        else:\n",
    "            matching_label = f\"I-{label}\"\n",
    "        if preds[idx] == matching_label:\n",
    "            _, end = offset_mapping[idx]\n",
    "            phrase_scores.append(sample_pred_scores[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            break # 해당 token 이 끝났기때문에 멈춘다.\n",
    "\n",
    "    if \"end\" in locals(): # _, end = offset_mapping[idx]\n",
    "        phrase = sample_text[start:end]\n",
    "        phrase_preds.append((phrase, start, end, label, phrase_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start, end -> split index\n",
    "temp_df = []\n",
    "for phrase_idx, (phrase, start, end, label, phrase_scores) in enumerate(phrase_preds):\n",
    "    word_start = len(sample_text[:start].split())\n",
    "    word_end = word_start + len(sample_text[start:end].split())\n",
    "    word_end = min(word_end, len(sample_text.split()))\n",
    "    ps = \" \".join([str(x) for x in range(word_start, word_end)])\n",
    "    \n",
    "    # label 이 O 가 아닌 친구들만 temp_df 에 넣는다.\n",
    "    if label != \"O\":\n",
    "        if sum(phrase_scores) / len(phrase_scores) >= proba_thresh[label]:\n",
    "            temp_df.append((sample_id, label, ps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for processed_idx in processed_idx_list:\n",
    "        if sent_idx > processed_idx:\n",
    "            a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1136"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1136]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "            sent_idx_list[i] -= 1\n",
    "return sent_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1136 0\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(processed_idx_list, start_idx_list):\n",
    "    if i != j:\n",
    "        print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Texting and driving\\n\\nOver half of drivers in today's society have this horrible habit of texting and driving. \""
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text[0:0+110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pro_text in nltk.sent_tokenize(processed_text):\n",
    "    if pro_text not in processed_text:\n",
    "        print(pro_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Texting and driving\\n\\nOver half of drivers in today's society have this horrible habit of texting and driving.\",\n",
       " \"This has to stop immediately, texting and driving puts people's lives at risk.\",\n",
       " 'This simple act has caused many fatalities over that last years.',\n",
       " 'Fortunately, the government have put a new law to deal with this problem with our generations problem of texting and driving.',\n",
       " 'I strongly feel that texting and driving is dangerous and should be prohibited.',\n",
       " 'Diving more into detail, texting and driving is hazardous and life threatening.',\n",
       " \"Just imagine you're on the road driving to your friend's house and someone shoots you a text.\",\n",
       " 'You look down not even for 5 seconds; then you accidents swerve into the next lane hitting a car.',\n",
       " 'There are so many different scenarios that could go wrong in that situation.',\n",
       " \"Your life is over just because you couldn't wait to text your friend.\",\n",
       " \"The consequences that texting and driving comes with just isn't worth it.\",\n",
       " \"The cost of a life shouldn't be worth a text message, Snapchat or Instagram DM (Direct Message).\",\n",
       " 'Texting and driving has taken many lives.',\n",
       " 'In a recent article I read it said \"2017 U. S. Cell Phone and Driving Statistics.',\n",
       " 'In 2012, 3,328 people were killed in distraction-related crashes.',\n",
       " 'About 421,000 people were injured in crashes involving a distracted driver.',\n",
       " 'In 2017, 11% of drivers under age 20 involved in fatal accidents were reported to be distracted at the time of the crash.\"',\n",
       " 'This cannot go on any longer we must do something to stop these numbers from increasing.',\n",
       " 'Luckily the government has enforced a brand new law called \"Hands Free\".',\n",
       " 'This law states Drivers cannot have a phone in their hand or touching any part of their body while talking on their phone while driving.',\n",
       " \"If you don't follow these laws they can fine you fifty to one hundred and fifty dollars.\",\n",
       " 'I feel that this action was necessary and justified to better protect our society.',\n",
       " 'Over all texting and driving is dangerous, has killed many people and is now on the decline thanks to the new \" Hands Down\" law.']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1834"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(awk_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15594"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "for i, j in zip(' '.join(nltk.sent_tokenize(text)), text):\n",
    "    if i != j:\n",
    "        print(k)\n",
    "        break\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _replace_awkend(text):\n",
    "    '''\"문장.문장\", \"문장 .문장\" 을 \"문장. 문장\" 으로 바꿔준다.\n",
    "        Parameters\n",
    "            - text (str) : \"문장 .문장\", \"문장.문장\"\n",
    "        Return\n",
    "            - text (str) : \"문장. 문장\"\n",
    "        \n",
    "    nltk 의 nltk.sent_tokenize() 는 문장    \n",
    "    cf) \"U.S. gov\" 를 \"U. S. gov\" 로 바꾸지만, nltk 는 다행히 후자를 하나의 문장으로 취급한다.\n",
    "    '''\n",
    "    # \"문장 .문장\"\n",
    "    text = re.sub(r' \\.', r'. ', text) \n",
    "    \n",
    "    # \"문장.문장\"\n",
    "    replace_token = re.findall(r'\\w\\.\\w', text) \n",
    "    replace_idx = [text.index(token) + i + 1 for i, token in enumerate(replace_token)]\n",
    "    for idx in replace_idx:\n",
    "        text = text[:idx] + '. ' + text[idx+1:]        \n",
    "    \n",
    "    return text, replace_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_text_list = list(map(nltk.sent_tokenize, text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Phones\\n\\nModern humans today are always on their phone.']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(sentence_text_list[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text_list = list(map(_replace_awkend, text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for processed_text, text in zip(processed_text_list, text_list):\n",
    "    if len(processed_text.split()) != len(text.split()):\n",
    "        print(i)\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for processed_text, text in zip(processed_text_list[4], text_list[4]):\n",
    "    if processed_text != text:\n",
    "        print(i)\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d \"2017 U. S. Cell P']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(processed_text_list[4][i-10:i+10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rect Message). Texting and driving has taken many lives. In a recent article I read it said \"2017 U.S. Cell Phone and Driving Statistics. In 2012, 3,328 people were killed in distraction-related crash'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list[4][i-100:i+100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d \"2017 U.S.', 'Cell Ph']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(text_list[4][i-10:i+10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '5 hours a day no stop.All they do'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['5', 'hours', 'a', 'day', 'no', 'stop.All', 'they', 'do'],\n",
       " ['5', 'hours', 'a', 'day', 'no', 'stop.', 'All', 'they', 'do'])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.split(), _replace_awkend(a).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def _replace_awkend1(text):\n",
    "    '''nltk 는 \"문장 .문장\" 에서 오작동한다. 바꿔주자.\n",
    "        Parameters\n",
    "            - text (str) : \"문장 .문장\"\n",
    "        Return\n",
    "            - text (str) : \"문장. 문장\"\n",
    "    '''\n",
    "    text = re.sub(r' \\.', r'. ', text)\n",
    "    assert ' .' not in text\n",
    "    return text\n",
    "\n",
    "def _replace_awkend2(text):\n",
    "    '''nltk 는 \"문장.문장\" 에서 오작동한다. 바꿔주자.\n",
    "        Parameters\n",
    "            - text (str) : \"문장.문장\"\n",
    "        Return\n",
    "            - text (str) : \"문장. 문장\"\n",
    "    '''\n",
    "    replace_token = re.findall(r'\\w\\.\\w', text) \n",
    "    replace_idx = [text.index(token) + i + 1 for i, token in enumerate(replace_token)]\n",
    "    \n",
    "    for idx in replace_idx:\n",
    "        text = text[:idx] + '. ' + text[idx+1:]\n",
    "        \n",
    "    assert len(re.findall(r'\\w\\.\\w', text)) is 0\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'문장. 문장'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_replace_awkend('문장 .문장')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'Modern humans today are always on their phone .\\\n",
    "They are always on their phone more than 5 hours a day no stop.\\\n",
    "All they do is text back and forward and just have group Chats on social media.\\\n",
    "They even do it while driving.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "len(re.findall(r'\\w\\.\\w', a)) is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = a.index(b[0]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Modern humans today are always on their phone .They are always on their phone more than 5 hours a day no stop. All they do is text back and forward and just have group Chats on social media.They even do it while driving.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:f] + '. ' + a[f+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pat in b:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r' \\.', r'. ', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a. k. a.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize('a. k. a.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Modern humans today are always on their phone. They are always on their phone more than 5 hours a day no stop .All they do is text back and forward and just have group Chats on social media. They even do it while driving.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['discourse_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'문장. 문장'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r' \\.', r'. ', '문장 .문장')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Modern humans today are always on their phone.',\n",
       " 'They are always on their phone more than 5 hours a day no stop .All they do is text back and forward and just have group Chats on social media.',\n",
       " 'They even do it while driving.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.sent_tokenize(df['discourse_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4# preprocess and wrap into dataset class\n",
    "args.model = './model'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model) #!# 나중에 local 에서 불러오도록 코드 수정 \n",
    "training_samples = prepare_training_data(train_df, tokenizer, args, num_jobs=NUM_JOBS)\n",
    "valid_samples = prepare_training_data(valid_df, tokenizer, args, num_jobs=NUM_JOBS)\n",
    "\n",
    "train_dataset = FeedbackDataset(training_samples, args.max_len, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snu36",
   "language": "python",
   "name": "snu36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
