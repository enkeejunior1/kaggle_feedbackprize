{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd #!#\n",
    "import random\n",
    "from einops import rearrange, repeat, reduce\n",
    "\n",
    "from joblib import Parallel, delayed # https://www.notion.so/joblib-da8f5ee8dbd44da19b36da04bd657bb1\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import tez # https://www.notion.so/Tez-093f1f31cba646e3963108294563ddd1\n",
    "from tez.callbacks import Callback\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "from sklearn import metrics\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_JOBS = 12 #1# hyper parameter setting\n",
    "args = set_args() #1# hyper parameter setting\n",
    "seed_everything(args.seed) #2# fix seed for reproducability\n",
    "os.makedirs(args.output_path, exist_ok = True)\n",
    "\n",
    "df = pd.read_csv(os.path.join(os.getcwd(), f'input/train_{5}folds.csv')) #3# read kfold data\n",
    "\n",
    "args.model = './model' #!# big bird. 이후에 long-former 로 바꿔주세요.\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
    "samples = prepare_training_data(df.iloc[:100], tokenizer, args, num_jobs=NUM_JOBS)\n",
    "collate = Collate(tokenizer)\n",
    "\n",
    "dataset = FeedbackDataset(samples, args.max_len, tokenizer)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size = args.batch_size, collate_fn = collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required (got type list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-55a9d71005c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-129-cdf7408e2968>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;34m\"ids\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;34m\"mask\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         }\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required (got type list)"
     ]
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required (got type list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-b1ebf928df39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-129-cdf7408e2968>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;34m\"ids\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;34m\"mask\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         }\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required (got type list)"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackModel(nn.Module):\n",
    "    def __init__(self, lm, head):\n",
    "        super(FeedbackModel, self).__init__()\n",
    "        self.lm = lm\n",
    "        self.head = head\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class set_args: #1#\n",
    "    seed: int = 42\n",
    "    fold: int = 0\n",
    "    kfold: int = 5\n",
    "    model = 'allenai/longformer-base-4096' #!# 모델 이름\n",
    "    lr: float = 1e-5\n",
    "    output_path = os.path.join(os.getcwd(), 'model') # '../model'\n",
    "    input_path = os.path.join(os.getcwd(), 'input') # '../input'\n",
    "    max_len: int = 1024\n",
    "    batch_size: int = 8\n",
    "    valid_batch_size: int = 8\n",
    "    epochs: int = 20\n",
    "    accumulation_steps = 1 #!# 이게 뭐지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n",
    "        output[\"type\"] = [sample[\"type\"] for sample in batch] # sample[\"type\"] = [num_sent, seq_len]\n",
    "        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n",
    "            output[\"type\"] = [s + (batch_max - len(s)) * [0] for s in output[\"type\"]]\n",
    "            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n",
    "        else:\n",
    "            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n",
    "            output[\"type\"] = [(batch_max - len(s)) * [0] + s for s in output[\"type\"]]\n",
    "            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n",
    "        output[\"type\"] = torch.tensor(output[\"type\"], dtype=torch.long)\n",
    "        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_type_ids = [0] + input_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackDataset(Dataset):\n",
    "    def __init__(self, samples, max_len, tokenizer):\n",
    "        super(FeedbackDataset, self).__init__() #!#\n",
    "        self.samples = samples\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.length = len(samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.samples[idx][\"input_ids\"]\n",
    "        input_labels = [target_id_map[x] for x in self.samples[idx][\"input_labels\"]]\n",
    "        input_type_ids = self.samples[idx][\"token_type_ids\"]\n",
    "        other_label_id = target_id_map[\"O\"]\n",
    "        padding_label_id = target_id_map[\"PAD\"]\n",
    "\n",
    "        # add start token id to the input_ids\n",
    "        input_ids = [self.tokenizer.cls_token_id] + input_ids\n",
    "        input_labels = [other_label_id] + input_labels\n",
    "        input_type_ids = [[0] + type_ids for type_ids in input_type_ids]\n",
    "        \n",
    "        # truncate the input if the text is longer than max_len\n",
    "        if len(input_ids) > self.max_len - 1:\n",
    "            input_ids = input_ids[: self.max_len - 1]\n",
    "            input_labels = input_labels[: self.max_len - 1]\n",
    "            input_type_ids = [type_ids[: self.max_len - 1] for type_ids in input_type_ids]\n",
    "\n",
    "        # add end token id to the input_ids\n",
    "        input_ids = input_ids + [self.tokenizer.sep_token_id]\n",
    "        input_labels = input_labels + [other_label_id]\n",
    "        input_type_ids = [type_ids + [0] for type_ids in input_type_ids]\n",
    "\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        \n",
    "        # padding\n",
    "        #!# 굳이 padding 을 max_len 까지 전부 다 해야하나..?\n",
    "#         padding_length = self.max_len - len(input_ids)\n",
    "#         if padding_length > 0:\n",
    "#             if self.tokenizer.padding_side == \"right\":\n",
    "#                 input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n",
    "#                 input_labels = input_labels + [padding_label_id] * padding_length\n",
    "#                 input_type_ids = input_type_ids + [0] * padding_length\n",
    "#                 attention_mask = attention_mask + [0] * padding_length\n",
    "                \n",
    "#             else:\n",
    "#                 input_ids = [self.tokenizer.pad_token_id] * padding_length + input_ids\n",
    "#                 input_labels = [padding_label_id] * padding_length + input_labels\n",
    "#                 input_type_ids = [0] * padding_length + input_type_ids\n",
    "#                 attention_mask = [0] * padding_length + attention_mask\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"type\": torch.tensor(input_type_ids, dtype=torch.long),\n",
    "            \"targets\": torch.tensor(input_labels, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(df: pd.DataFrame, tokenizer, args, num_jobs):\n",
    "    '''_prepare_training_data_helper 를 병렬처리\n",
    "        Parameters\n",
    "            tokenizer : 전처리에 활용되는 tokenizer\n",
    "            num_jobs : number of 병렬 처리 workers\n",
    "        \n",
    "        Returns\n",
    "            training_samples(list) : \n",
    "    '''\n",
    "    training_samples = []\n",
    "    train_ids = df[\"id\"].unique()\n",
    "\n",
    "    train_ids_splits = np.array_split(train_ids, num_jobs)\n",
    "\n",
    "    results = Parallel(n_jobs=num_jobs, backend=\"multiprocessing\")(\n",
    "        delayed(_prepare_training_data_helper)(df, tokenizer, args, idx) for idx in train_ids_splits\n",
    "    )\n",
    "    for result in results:\n",
    "        training_samples.extend(result)\n",
    "\n",
    "    return training_samples\n",
    "\n",
    "def _prepare_training_data_helper(df, tokenizer, args, train_ids = None):\n",
    "    training_samples = []\n",
    "    for idx in df['id'].unique(): #!# replace df['id'].unique() -> train_ids\n",
    "        filename = os.path.join('./input', \"train\", idx + \".txt\")\n",
    "        with open(filename, \"r\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "\n",
    "        input_ids = encoded_text[\"input_ids\"]\n",
    "        offset_mapping = encoded_text[\"offset_mapping\"]\n",
    "\n",
    "        sample = {\n",
    "            \"id\": idx,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"text\": text,\n",
    "            \"offset_mapping\": offset_mapping,\n",
    "        }\n",
    "\n",
    "        # token_type_ids 만들기\n",
    "        #!# test code\n",
    "        token_type_ids_list = []\n",
    "        processed_text, processed_idx_list = _replace_awkend(text) # nltk 를 위한 preprocessing\n",
    "        start_idx_list, end_idx_list = _extract_sentence_idx(processed_text) # 문장 index 추출하기\n",
    "        start_idx_list = _postprocess_sent_idx(start_idx_list, processed_idx_list) # preprocessing 이전의 index 로 되돌리기\n",
    "        end_idx_list = _postprocess_sent_idx(end_idx_list, processed_idx_list) # preprocessing 이전의 index 로 되돌리기\n",
    "        \n",
    "        for start_idx, end_idx in zip(start_idx_list, end_idx_list):\n",
    "            text_type_ids = [0] * len(text)\n",
    "            text_type_ids[start_idx:end_idx] = [1] * (end_idx - start_idx) #!# start_idx:end_idx 로 잘 추출되는지 확인\n",
    "\n",
    "            token_type_ids = []\n",
    "            for i, (offset1, offset2) in enumerate(encoded_text[\"offset_mapping\"]):\n",
    "                if any(text_type_ids[offset1:offset2]): # 1개의 text 라도 0 이외의 값\n",
    "                    if len(text[offset1:offset2].split()) > 0: #!# 1개의 token 은 include\n",
    "                        token_type_ids.append(1)\n",
    "                    else:\n",
    "                        token_type_ids.append(0)\n",
    "                else:\n",
    "                    token_type_ids.append(0)\n",
    "                        \n",
    "            assert len(token_type_ids) == len(encoded_text[\"offset_mapping\"])\n",
    "            token_type_ids_list.append(token_type_ids)\n",
    "        sample[\"token_type_ids\"] = token_type_ids_list\n",
    "        \n",
    "        # input_labels\n",
    "        temp_df = df[df['id'] == idx]\n",
    "        input_labels = copy.deepcopy(input_ids)\n",
    "        for k in range(len(input_labels)):\n",
    "            input_labels[k] = \"O\"\n",
    "            \n",
    "        for _, row in temp_df.iterrows():\n",
    "            text_labels = [0] * len(text)\n",
    "            discourse_start = int(row[\"discourse_start\"])\n",
    "            discourse_end = int(row[\"discourse_end\"])\n",
    "            prediction_label = row[\"discourse_type\"]\n",
    "            text_labels[discourse_start:discourse_end] = [1] * (discourse_end - discourse_start)\n",
    "            target_idx = []\n",
    "            for map_idx, (offset1, offset2) in enumerate(encoded_text[\"offset_mapping\"]):\n",
    "                if sum(text_labels[offset1:offset2]) > 0:\n",
    "                    if len(text[offset1:offset2].split()) > 0:\n",
    "                        target_idx.append(map_idx)\n",
    "\n",
    "            targets_start = target_idx[0]\n",
    "            targets_end = target_idx[-1]\n",
    "            pred_start = \"B-\" + prediction_label\n",
    "            pred_end = \"I-\" + prediction_label\n",
    "            input_labels[targets_start] = pred_start\n",
    "            input_labels[targets_start + 1 : targets_end + 1] = [pred_end] * (targets_end - targets_start)\n",
    "        sample[\"input_ids\"] = input_ids\n",
    "        sample[\"input_labels\"] = input_labels\n",
    "\n",
    "        training_samples.append(sample)\n",
    "    return training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _replace_awkend(text):\n",
    "    '''\"문장.문장\", \"문장 .문장\" 을 \"문장. 문장\" 으로 바꿔준다.\n",
    "        Parameters\n",
    "            - text (str) : \"문장 .문장\", \"문장.문장\"\n",
    "        Return\n",
    "            - text (str) : \"문장. 문장\"\n",
    "        \n",
    "    nltk 의 nltk.sent_tokenize() 는 문장    \n",
    "    cf) \"U.S. gov\" 를 \"U. S. gov\" 로 바꾸지만, nltk 는 다행히 후자를 하나의 문장으로 취급한다.\n",
    "    '''\n",
    "    # \"문장 .문장\"\n",
    "    text = re.sub(r' \\.', r'. ', text) \n",
    "    \n",
    "    # \"문장.문장\"\n",
    "    replace_token = re.findall(r'\\w\\.\\w', text) \n",
    "    replace_idx = [text.index(token) + i + 1 for i, token in enumerate(replace_token)]\n",
    "    for idx in replace_idx:\n",
    "        text = text[:idx] + '. ' + text[idx+1:]        \n",
    "    \n",
    "    return text, replace_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sentence index\n",
    "def _extract_sentence_idx(text):\n",
    "    '''nltk 를 활용해서 문장을 추출한다.\n",
    "        Parameters\n",
    "            - text : nltk 가 오작동하지 않도록 전처리된 자료\n",
    "        Returns\n",
    "            - start_idx_list (list) : 해당 문장이 시작하는 index\n",
    "            - end_idx_list (list) : 해당 문장이 끝나는 index\n",
    "        \n",
    "        Assert\n",
    "            - 각 i 에 대해서 text[start_idx_list[i]:end_idx_list[i]] 는 \n",
    "                하나의 문장에 대응한다.\n",
    "    '''\n",
    "    sent_list = nltk.sent_tokenize(text)\n",
    "    \n",
    "    start_idx_list = []\n",
    "    end_idx_list = []\n",
    "    for i, sent in enumerate(sent_list):\n",
    "        start_idx_list.append(text.index(sent))\n",
    "        end_idx_list.append(start_idx_list[-1] + len(sent))\n",
    "\n",
    "    for i, _ in enumerate(sent_list):\n",
    "        assert text[start_idx_list[i]:end_idx_list[i]] == sent_list[i]\n",
    "        \n",
    "    return start_idx_list, end_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import warnings\n",
    "\n",
    "def _postprocess_sent_idx(sent_idx_list, processed_idx_list):\n",
    "    postprocess_sent_idx_list = copy.deepcopy(sent_idx_list)\n",
    "    for i, sent_idx in enumerate(sent_idx_list):\n",
    "        for processed_idx in processed_idx_list:\n",
    "            if sent_idx > processed_idx:\n",
    "                postprocess_sent_idx_list[i] -= 1\n",
    "        \n",
    "    return postprocess_sent_idx_list\n",
    "\n",
    "# original_start_idx_list = _postprocess_sent_idx(start_idx_list, processed_idx_list)\n",
    "# original_end_idx_list = _postprocess_sent_idx(end_idx_list, processed_idx_list)\n",
    "\n",
    "# # test code : 추출 목표가 추출 결과와 일치하는가?\n",
    "# diff_list = []\n",
    "# for i in range(len(start_idx_list)):\n",
    "#     start_idx = start_idx_list[i]\n",
    "#     end_idx = end_idx_list[i]\n",
    "#     original_start_idx = original_start_idx_list[i]\n",
    "#     original_end_idx = original_end_idx_list[i]\n",
    "    \n",
    "#     if processed_text[start_idx:end_idx] != text[original_start_idx:original_end_idx]:\n",
    "#         diff_list.append([processed_text[start_idx:end_idx], text[original_start_idx:original_end_idx]])\n",
    "    \n",
    "# if len(diff_list) != 0:\n",
    "#     for diff in diff_list:\n",
    "#         print(f\"{diff[0]}\\n{diff[1]}\")\n",
    "#     warnings.warn(\"exist different sentence (processed, original)\"+f\"{diff_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id_map = {\n",
    "    \"B-Lead\": 0,\n",
    "    \"I-Lead\": 1,\n",
    "    \"B-Position\": 2,\n",
    "    \"I-Position\": 3,\n",
    "    \"B-Evidence\": 4,\n",
    "    \"I-Evidence\": 5,\n",
    "    \"B-Claim\": 6,\n",
    "    \"I-Claim\": 7,\n",
    "    \"B-Concluding Statement\": 8,\n",
    "    \"I-Concluding Statement\": 9,\n",
    "    \"B-Counterclaim\": 10,\n",
    "    \"I-Counterclaim\": 11,\n",
    "    \"B-Rebuttal\": 12,\n",
    "    \"I-Rebuttal\": 13,\n",
    "    \"O\": 14,\n",
    "    \"PAD\": -100,\n",
    "}\n",
    "\n",
    "id_target_map = {v: k for k, v in target_id_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token offset -> start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randint(0, 14, (1, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [id_target_map[int(p)] for p in a[0][1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lead'"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1914"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "phrase_preds = []\n",
    "while idx < len(b):\n",
    "    start, _ = offset_mapping[idx]\n",
    "    if preds[idx] != \"O\":\n",
    "        label = preds[idx][2:] #!# [2:] 는 왜 붙은걸까\n",
    "    else:\n",
    "        label = \"O\"\n",
    "    phrase_scores = []\n",
    "    phrase_scores.append(sample_pred_scores[idx])\n",
    "    \n",
    "    # label을 추출... \n",
    "    \n",
    "    idx += 1\n",
    "    while idx < len(offset_mapping):\n",
    "        if label == \"O\":\n",
    "            matching_label = \"O\"\n",
    "        else:\n",
    "            matching_label = f\"I-{label}\"\n",
    "        if preds[idx] == matching_label:\n",
    "            _, end = offset_mapping[idx]\n",
    "            phrase_scores.append(sample_pred_scores[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            break # 해당 token 이 끝났기때문에 멈춘다.\n",
    "\n",
    "    if \"end\" in locals(): # _, end = offset_mapping[idx]\n",
    "        phrase = sample_text[start:end]\n",
    "        phrase_preds.append((phrase, start, end, label, phrase_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start, end -> split index\n",
    "temp_df = []\n",
    "for phrase_idx, (phrase, start, end, label, phrase_scores) in enumerate(phrase_preds):\n",
    "    word_start = len(sample_text[:start].split())\n",
    "    word_end = word_start + len(sample_text[start:end].split())\n",
    "    word_end = min(word_end, len(sample_text.split()))\n",
    "    ps = \" \".join([str(x) for x in range(word_start, word_end)])\n",
    "    \n",
    "    # label 이 O 가 아닌 친구들만 temp_df 에 넣는다.\n",
    "    if label != \"O\":\n",
    "        if sum(phrase_scores) / len(phrase_scores) >= proba_thresh[label]:\n",
    "            temp_df.append((sample_id, label, ps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for processed_idx in processed_idx_list:\n",
    "        if sent_idx > processed_idx:\n",
    "            a = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testcode_prepare_training_data_helper(samples:list):\n",
    "    '''문장이 제대로 추출됐는지 눈으로 확인한다. \n",
    "        Parameters\n",
    "            - samples (dict)\n",
    "                > input_ids : 각 token 들의 index. \n",
    "                > token_type_ids : 문장에 대응하는 token 위치를 1 로 저장해둔 list.\n",
    "                    ex) [0,0,0,1,1,1,1,0,0,0,...]\n",
    "        Returns\n",
    "            - decoded_samples (list) : 자연어로 변환된 결과를 담은 list.\n",
    "    '''\n",
    "    decoded_samples = []\n",
    "    for sample in samples:\n",
    "        extracted_input_ids = torch.tensor(sample['input_ids']) * torch.tensor(sample['token_type_ids'])\n",
    "        for ext_ids in extracted_input_ids:\n",
    "            decoded_samples.append(tokenizer.decode(ext_ids))\n",
    "    \n",
    "    return decoded_samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snu36",
   "language": "python",
   "name": "snu36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
