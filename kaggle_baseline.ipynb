{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[discussion 에 공유된 코드](https://www.kaggle.com/abhishek/two-longformers-are-better-than-1) 를 재생산하는 것을 목표로 한다.ds\n",
    "\n",
    "[github 링크](https://github.com/abhishekkrthakur/long-text-token-classification)\n",
    "\n",
    "#!# : todo list\n",
    "\n",
    "# todo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/anaconda-3-2020.02/envs/snu36/bin/python'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook kaggle_baseline.ipynb to script\n",
      "[NbConvertApp] Writing 25942 bytes to kaggle_baseline.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script kaggle_baseline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from einops import rearrange, repeat, reduce\n",
    "\n",
    "from joblib import Parallel, delayed # https://www.notion.so/joblib-da8f5ee8dbd44da19b36da04bd657bb1\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import tez # https://www.notion.so/Tez-093f1f31cba646e3963108294563ddd1\n",
    "from tez.callbacks import Callback\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "from sklearn import metrics\n",
    "\n",
    "gc.enable()\n",
    "sys.path.append(\"../input/tez-lib/\") #!#\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id_map = {\n",
    "    \"B-Lead\": 0,\n",
    "    \"I-Lead\": 1,\n",
    "    \"B-Position\": 2,\n",
    "    \"I-Position\": 3,\n",
    "    \"B-Evidence\": 4,\n",
    "    \"I-Evidence\": 5,\n",
    "    \"B-Claim\": 6,\n",
    "    \"I-Claim\": 7,\n",
    "    \"B-Concluding Statement\": 8,\n",
    "    \"I-Concluding Statement\": 9,\n",
    "    \"B-Counterclaim\": 10,\n",
    "    \"I-Counterclaim\": 11,\n",
    "    \"B-Rebuttal\": 12,\n",
    "    \"I-Rebuttal\": 13,\n",
    "    \"O\": 14,\n",
    "    \"PAD\": -100,\n",
    "}\n",
    "\n",
    "\n",
    "id_target_map = {v: k for k, v in target_id_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproduce 마무리한 뒤에는 parser 함수로 바꿔주기\n",
    "# https://github.com/abhishekkrthakur/long-text-token-classification/blob/8f636ea23b7e1842583581d9cbdbe9f0f54d3191/train.py#L30\n",
    "class set_args: #1#\n",
    "    seed: int = 42\n",
    "    fold: int = 0\n",
    "    kfold: int = 5\n",
    "    model = 'allenai/longformer-base-4096' #!# 모델 이름\n",
    "    lr: float = 1e-5\n",
    "    output_path = os.path.join(os.getcwd(), 'model') # '../model'\n",
    "    input_path = os.path.join(os.getcwd(), 'input') # '../input'\n",
    "    max_len: int = 1024\n",
    "    batch_size: int = 8\n",
    "    valid_batch_size: int = 8\n",
    "    epochs: int = 20\n",
    "    accumulation_steps = 1 #!# 이게 뭐지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int): #2#\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "def get_stratified_fold(df:pd.DataFrame, args): #3# ref : https://www.kaggle.com/abhishek/creating-folds-properly-hopefully-p/\n",
    "    '''iterstrat 를 활용해서 multi-label 데이터에 대한 StratifiedKFold 를 진행한다.\n",
    "        Parameters\n",
    "            df : need to have 'id', 'discourse_type' columns.\n",
    "            args : kfold 와 seed number 정보를 담고 있는 class.\n",
    "        \n",
    "        Returns\n",
    "            df : 'kfold' column 이 추가된 df 를 반환한다.\n",
    "    '''\n",
    "    dfx = pd.get_dummies(df, columns=[\"discourse_type\"]) # discourse_type 에 대한 one-hot vector 로 표현\n",
    "    dfx = dfx.groupby([\"id\"], as_index=False).sum() # 각 text 에서의 discourse_type 갯수 계산하기\n",
    "    \n",
    "    cols = [c for c in dfx.columns if c.startswith(\"discourse_type_\") or c == \"id\" and c != \"discourse_type_num\"] # discourse_type 열 추출\n",
    "    dfx = dfx[cols]\n",
    "\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=args.kfold, shuffle=True, random_state=args.seed) # kfold 를 수행해주는 class 선언\n",
    "    labels = [c for c in dfx.columns if c != \"id\"]\n",
    "    dfx_labels = dfx[labels]\n",
    "    dfx[\"kfold\"] = -1 \n",
    "\n",
    "    for fold, (trn_idx_array, val_idx_array) in enumerate(mskf.split(dfx, dfx_labels)): # mskf.split(dfx, dfx_labels) 는 (trn:array, val:array) index 를 내뱉어주는 generater.\n",
    "        dfx.loc[val_idx_array, f\"{args.kfold}fold\"] = fold\n",
    "\n",
    "    df = df.merge(dfx[[\"id\", f\"{args.kfold}fold\"]], on=\"id\", how=\"left\") # on : id 를 기준으로 merge, how : matching 되는 id 가 없어도 남겨둔다 (left)\n",
    "    df.to_csv(f\"input/train_{args.kfold}folds.csv\", index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4# preprocessing\n",
    "def prepare_training_data(df: pd.DataFrame, tokenizer, args, num_jobs):\n",
    "    '''_prepare_training_data_helper 를 병렬처리\n",
    "        Parameters\n",
    "            tokenizer : 전처리에 활용되는 tokenizer\n",
    "            num_jobs : number of 병렬 처리 workers\n",
    "        \n",
    "        Returns\n",
    "            training_samples(list) : \n",
    "    '''\n",
    "    training_samples = []\n",
    "    train_ids = df[\"id\"].unique()\n",
    "\n",
    "    train_ids_splits = np.array_split(train_ids, num_jobs)\n",
    "\n",
    "    results = Parallel(n_jobs=num_jobs, backend=\"multiprocessing\")(\n",
    "        delayed(_prepare_training_data_helper)(df, tokenizer, args, idx) for idx in train_ids_splits\n",
    "    )\n",
    "    for result in results:\n",
    "        training_samples.extend(result)\n",
    "\n",
    "    return training_samples\n",
    "\n",
    "import copy\n",
    "def _prepare_training_data_helper(df, tokenizer, args, train_ids):\n",
    "    training_samples = []\n",
    "    for idx in train_ids:\n",
    "        filename = os.path.join(args.input_path, \"train\", idx + \".txt\")\n",
    "        with open(filename, \"r\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        input_ids = encoded_text[\"input_ids\"]\n",
    "        input_labels = copy.deepcopy(input_ids)\n",
    "        offset_mapping = encoded_text[\"offset_mapping\"]\n",
    "\n",
    "        for k in range(len(input_labels)):\n",
    "            input_labels[k] = \"O\"\n",
    "\n",
    "        sample = {\n",
    "            \"id\": idx,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"text\": text,\n",
    "            \"offset_mapping\": offset_mapping,\n",
    "        }\n",
    "\n",
    "        temp_df = df[df[\"id\"] == idx]\n",
    "        for _, row in temp_df.iterrows():\n",
    "            text_labels = [0] * len(text)\n",
    "            discourse_start = int(row[\"discourse_start\"])\n",
    "            discourse_end = int(row[\"discourse_end\"])\n",
    "            prediction_label = row[\"discourse_type\"]\n",
    "            text_labels[discourse_start:discourse_end] = [1] * (discourse_end - discourse_start)\n",
    "            target_idx = []\n",
    "            for map_idx, (offset1, offset2) in enumerate(encoded_text[\"offset_mapping\"]):\n",
    "                if sum(text_labels[offset1:offset2]) > 0:\n",
    "                    if len(text[offset1:offset2].split()) > 0:\n",
    "                        target_idx.append(map_idx)\n",
    "\n",
    "            targets_start = target_idx[0]\n",
    "            targets_end = target_idx[-1]\n",
    "            pred_start = \"B-\" + prediction_label\n",
    "            pred_end = \"I-\" + prediction_label\n",
    "            input_labels[targets_start] = pred_start\n",
    "            input_labels[targets_start + 1 : targets_end + 1] = [pred_end] * (targets_end - targets_start)\n",
    "        sample[\"input_ids\"] = input_ids\n",
    "        sample[\"input_labels\"] = input_labels\n",
    "        training_samples.append(sample)\n",
    "    return training_samples\n",
    "        \n",
    "import copy\n",
    "def _prepare_training_data_helper_sentencebert(df, tokenizer, args, train_ids): #!#\n",
    "    '''\n",
    "    1. \n",
    "    '''\n",
    "    training_samples = []\n",
    "    for idx in train_ids:\n",
    "        filename = os.path.join(args.input_path, \"train\", idx + \".txt\")\n",
    "        with open(filename, \"r\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        input_ids = encoded_text[\"input_ids\"]\n",
    "        input_labels = copy.deepcopy(input_ids)\n",
    "        offset_mapping = encoded_text[\"offset_mapping\"]\n",
    "\n",
    "        for k in range(len(input_labels)):\n",
    "            input_labels[k] = \"O\"\n",
    "\n",
    "        sample = {\n",
    "            \"id\": idx,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"text\": text,\n",
    "            \"offset_mapping\": offset_mapping,\n",
    "        }\n",
    "\n",
    "        temp_df = df[df[\"id\"] == idx]\n",
    "        for _, row in temp_df.iterrows():\n",
    "            text_labels = [0] * len(text)\n",
    "            discourse_start = int(row[\"discourse_start\"])\n",
    "            discourse_end = int(row[\"discourse_end\"])\n",
    "            prediction_label = row[\"discourse_type\"]\n",
    "            text_labels[discourse_start:discourse_end] = [1] * (discourse_end - discourse_start)\n",
    "            target_idx = []\n",
    "            for map_idx, (offset1, offset2) in enumerate(encoded_text[\"offset_mapping\"]):\n",
    "                if sum(text_labels[offset1:offset2]) > 0:\n",
    "                    if len(text[offset1:offset2].split()) > 0:\n",
    "                        target_idx.append(map_idx)\n",
    "\n",
    "            targets_start = target_idx[0]\n",
    "            targets_end = target_idx[-1]\n",
    "            pred_start = \"B-\" + prediction_label\n",
    "            pred_end = \"I-\" + prediction_label\n",
    "            input_labels[targets_start] = pred_start\n",
    "            input_labels[targets_start + 1 : targets_end + 1] = [pred_end] * (targets_end - targets_start)\n",
    "        sample[\"input_ids\"] = input_ids\n",
    "        sample[\"input_labels\"] = input_labels\n",
    "        \n",
    "        sample[\"token_type_ids\"] = token_type_ids #!# \n",
    "        \n",
    "        \n",
    "        training_samples.append(sample)\n",
    "    return training_samples\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackDataset(Dataset):\n",
    "    def __init__(self, samples, max_len, tokenizer):\n",
    "        super(FeedbackDataset, self).__init__() #!#\n",
    "        self.samples = samples\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.length = len(samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.samples[idx][\"input_ids\"]\n",
    "        input_labels = self.samples[idx][\"input_labels\"]\n",
    "        input_labels = [target_id_map[x] for x in input_labels]\n",
    "        other_label_id = target_id_map[\"O\"]\n",
    "        padding_label_id = target_id_map[\"PAD\"]\n",
    "        # print(input_ids)\n",
    "        # print(input_labels)\n",
    "\n",
    "        # add start token id to the input_ids\n",
    "        input_ids = [self.tokenizer.cls_token_id] + input_ids\n",
    "        input_labels = [other_label_id] + input_labels\n",
    "\n",
    "        if len(input_ids) > self.max_len - 1:\n",
    "            input_ids = input_ids[: self.max_len - 1]\n",
    "            input_labels = input_labels[: self.max_len - 1]\n",
    "\n",
    "        # add end token id to the input_ids\n",
    "        input_ids = input_ids + [self.tokenizer.sep_token_id]\n",
    "        input_labels = input_labels + [other_label_id]\n",
    "\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        padding_length = self.max_len - len(input_ids)\n",
    "        if padding_length > 0:\n",
    "            if self.tokenizer.padding_side == \"right\":\n",
    "                input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n",
    "                input_labels = input_labels + [padding_label_id] * padding_length\n",
    "                attention_mask = attention_mask + [0] * padding_length\n",
    "            else:\n",
    "                input_ids = [self.tokenizer.pad_token_id] * padding_length + input_ids\n",
    "                input_labels = [padding_label_id] * padding_length + input_labels\n",
    "                attention_mask = [0] * padding_length + attention_mask\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"targets\": torch.tensor(input_labels, dtype=torch.long),\n",
    "        }\n",
    "    \n",
    "class FeedbackDatasetValid:\n",
    "    def __init__(self, samples, max_len, tokenizer):\n",
    "        self.samples = samples\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.length = len(samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.samples[idx][\"input_ids\"]\n",
    "        input_ids = [self.tokenizer.cls_token_id] + input_ids\n",
    "\n",
    "        if len(input_ids) > self.max_len - 1:\n",
    "            input_ids = input_ids[: self.max_len - 1]\n",
    "\n",
    "        # add end token id to the input_ids\n",
    "        input_ids = input_ids + [self.tokenizer.sep_token_id]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        return {\n",
    "            \"ids\": input_ids,\n",
    "            \"mask\": attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5# build model\n",
    "from torch.optim import AdamW\n",
    "class FeedbackModel(tez.Model):\n",
    "    '''Kaggle baseline 모델\n",
    "    특징\n",
    "        - LM : hidden dropout prob = 0.1, layer_norm_eps = 1e-7\n",
    "        - Head : dropout 을 0.1~0.5 로 설정하고 fc_layer 에 넣어준다. 이후 평균한 값을 예측에 활용한다.\n",
    "        - loss : 평균하기 전 결과에 대해 training 을 진행한다. 따라서, 일종의 ensemble 로 해석할 수 있을듯.\n",
    "    '''\n",
    "    def __init__(self, model_name, num_train_steps, learning_rate, num_labels, steps_per_epoch):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model_name = model_name\n",
    "        self.num_train_steps = num_train_steps\n",
    "        self.num_labels = num_labels\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.step_scheduler_after = \"batch\"\n",
    "\n",
    "        hidden_dropout_prob: float = 0.1\n",
    "        layer_norm_eps: float = 1e-7\n",
    "\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "        config.update(\n",
    "            {\n",
    "                \"output_hidden_states\": True,\n",
    "                \"hidden_dropout_prob\": hidden_dropout_prob,\n",
    "                \"layer_norm_eps\": layer_norm_eps,\n",
    "                \"add_pooling_layer\": False,\n",
    "                \"num_labels\": self.num_labels,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(model_name, config=config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        self.output = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "    def fetch_optimizer(self):\n",
    "        param_optimizer = list(self.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\"]\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.01,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        opt = AdamW(optimizer_parameters, lr=self.learning_rate)\n",
    "        return opt\n",
    "\n",
    "    def fetch_scheduler(self):\n",
    "        sch = get_cosine_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=int(0.1 * self.num_train_steps),\n",
    "            num_training_steps=self.num_train_steps,\n",
    "            num_cycles=1,\n",
    "            last_epoch=-1,\n",
    "        )\n",
    "        return sch\n",
    "\n",
    "    def loss(self, outputs, targets, attention_mask):\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "        active_loss = attention_mask.view(-1) == 1\n",
    "        active_logits = outputs.view(-1, self.num_labels)\n",
    "        true_labels = targets.view(-1)\n",
    "        outputs = active_logits.argmax(dim=-1)\n",
    "        idxs = np.where(active_loss.cpu().numpy() == 1)[0]\n",
    "        active_logits = active_logits[idxs]\n",
    "        true_labels = true_labels[idxs].to(torch.long)\n",
    "\n",
    "        loss = loss_fct(active_logits, true_labels)\n",
    "        return loss\n",
    "\n",
    "    def monitor_metrics(self, outputs, targets, attention_mask):\n",
    "        active_loss = (attention_mask.view(-1) == 1).cpu().numpy()\n",
    "        active_logits = outputs.view(-1, self.num_labels)\n",
    "        true_labels = targets.view(-1).cpu().numpy()\n",
    "        outputs = active_logits.argmax(dim=-1).cpu().numpy()\n",
    "        idxs = np.where(active_loss == 1)[0]\n",
    "        f1_score = metrics.f1_score(true_labels[idxs], outputs[idxs], average=\"macro\")\n",
    "        return {\"f1\": f1_score}\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids=None, targets=None):\n",
    "\n",
    "        if token_type_ids:\n",
    "            transformer_out = self.transformer(ids, mask, token_type_ids)\n",
    "        else:\n",
    "            transformer_out = self.transformer(ids, mask)\n",
    "        sequence_output = transformer_out.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "\n",
    "        logits1 = self.output(self.dropout1(sequence_output))\n",
    "        logits2 = self.output(self.dropout2(sequence_output))\n",
    "        logits3 = self.output(self.dropout3(sequence_output))\n",
    "        logits4 = self.output(self.dropout4(sequence_output))\n",
    "        logits5 = self.output(self.dropout5(sequence_output))\n",
    "\n",
    "        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n",
    "        logits = torch.softmax(logits, dim=-1)\n",
    "        loss = 0\n",
    "\n",
    "        if targets is not None:\n",
    "            loss1 = self.loss(logits1, targets, attention_mask=mask)\n",
    "            loss2 = self.loss(logits2, targets, attention_mask=mask)\n",
    "            loss3 = self.loss(logits3, targets, attention_mask=mask)\n",
    "            loss4 = self.loss(logits4, targets, attention_mask=mask)\n",
    "            loss5 = self.loss(logits5, targets, attention_mask=mask)\n",
    "            loss = (loss1 + loss2 + loss3 + loss4 + loss5) / 5\n",
    "            f1_1 = self.monitor_metrics(logits1, targets, attention_mask=mask)[\"f1\"]\n",
    "            f1_2 = self.monitor_metrics(logits2, targets, attention_mask=mask)[\"f1\"]\n",
    "            f1_3 = self.monitor_metrics(logits3, targets, attention_mask=mask)[\"f1\"]\n",
    "            f1_4 = self.monitor_metrics(logits4, targets, attention_mask=mask)[\"f1\"]\n",
    "            f1_5 = self.monitor_metrics(logits5, targets, attention_mask=mask)[\"f1\"]\n",
    "            f1 = (f1_1 + f1_2 + f1_3 + f1_4 + f1_5) / 5\n",
    "            metric = {\"f1\": f1}\n",
    "            return logits, loss, metric\n",
    "\n",
    "        return logits, loss, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6# 모델 후처리 및 tez 의 EarlyStopping module 정의\n",
    "#!# split 이랑 어떻게 연결짓는지 알아야함... offset -> string index -> \n",
    "\n",
    "class EarlyStopping(Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path,\n",
    "        valid_df,\n",
    "        valid_samples,\n",
    "        batch_size,\n",
    "        tokenizer,\n",
    "        patience=5,\n",
    "        mode=\"max\",\n",
    "        delta=0.001,\n",
    "        save_weights_only=True,\n",
    "    ):\n",
    "        self.patience = patience # update 없이 흘러가는 epoch. 그래서 인내심인듯\n",
    "        self.counter = 0 #!#\n",
    "        self.mode = mode #!#\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta # model update 의 최소 기준. delta 보다 작은 score 의 향상에 대해서는 model weight 저장이 이뤄지지 않는다.\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.model_path = model_path\n",
    "        self.valid_samples = valid_samples\n",
    "        self.batch_size = batch_size\n",
    "        self.valid_df = valid_df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if self.mode == \"min\":\n",
    "            self.val_score = np.Inf\n",
    "        else:\n",
    "            self.val_score = -np.Inf\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        '''\n",
    "        1. model.predict -> final_preds, final_class 를 self.valid_samples 에 저장\n",
    "        2. model.predict 를 토대로 min_thresh, proba_thresh 를\n",
    "        '''\n",
    "        model.eval()\n",
    "        valid_dataset = FeedbackDatasetValid(self.valid_samples, 4096, self.tokenizer)\n",
    "        collate = Collate(self.tokenizer)\n",
    "\n",
    "        #!# model.predict 는 tez 에서 제공하는 기능인듯.\n",
    "        preds_iter = model.predict(\n",
    "            valid_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            n_jobs=-1,\n",
    "            collate_fn=collate,\n",
    "        )\n",
    "        \n",
    "        final_preds = []\n",
    "        final_scores = []\n",
    "        for preds in preds_iter:\n",
    "            pred_class = np.argmax(preds, axis=2)\n",
    "            pred_scrs = np.max(preds, axis=2)\n",
    "            for pred, pred_scr in zip(pred_class, pred_scrs):\n",
    "                final_preds.append(pred.tolist())\n",
    "                final_scores.append(pred_scr.tolist())\n",
    "\n",
    "        for j in range(len(self.valid_samples)):\n",
    "            tt = [id_target_map[p] for p in final_preds[j][1:]]\n",
    "            tt_score = final_scores[j][1:]\n",
    "            self.valid_samples[j][\"preds\"] = tt\n",
    "            self.valid_samples[j][\"pred_scores\"] = tt_score\n",
    "\n",
    "        submission = []\n",
    "        min_thresh = {\n",
    "            \"Lead\": 9,\n",
    "            \"Position\": 5,\n",
    "            \"Evidence\": 14,\n",
    "            \"Claim\": 3,\n",
    "            \"Concluding Statement\": 11,\n",
    "            \"Counterclaim\": 6,\n",
    "            \"Rebuttal\": 4,\n",
    "        }\n",
    "        proba_thresh = {\n",
    "            \"Lead\": 0.7,\n",
    "            \"Position\": 0.55,\n",
    "            \"Evidence\": 0.65,\n",
    "            \"Claim\": 0.55,\n",
    "            \"Concluding Statement\": 0.7,\n",
    "            \"Counterclaim\": 0.5,\n",
    "            \"Rebuttal\": 0.55,\n",
    "        }\n",
    "\n",
    "        for _, sample in enumerate(self.valid_samples):\n",
    "            preds = sample[\"preds\"] # 각 token 들의 predict 결과 [id_target_map[p] for p in final_preds[j][1:]]\n",
    "            offset_mapping = sample[\"offset_mapping\"]\n",
    "            sample_id = sample[\"id\"]\n",
    "            sample_text = sample[\"text\"]\n",
    "            sample_pred_scores = sample[\"pred_scores\"]\n",
    "\n",
    "            # pad preds to same length as offset_mapping\n",
    "            if len(preds) < len(offset_mapping):\n",
    "                preds = preds + [\"O\"] * (len(offset_mapping) - len(preds))\n",
    "                sample_pred_scores = sample_pred_scores + [0] * (len(offset_mapping) - len(sample_pred_scores))\n",
    "\n",
    "            #!# \n",
    "            idx = 0\n",
    "            phrase_preds = []\n",
    "            while idx < len(offset_mapping):\n",
    "                start, _ = offset_mapping[idx]\n",
    "                if preds[idx] != \"O\":\n",
    "                    label = preds[idx][2:] # [2:] 는 \"B-<label>\", \"I-<label>\" \n",
    "                else:\n",
    "                    label = \"O\"\n",
    "                phrase_scores = []\n",
    "                phrase_scores.append(sample_pred_scores[idx])\n",
    "                idx += 1\n",
    "                while idx < len(offset_mapping):\n",
    "                    if label == \"O\":\n",
    "                        matching_label = \"O\"\n",
    "                    else:\n",
    "                        matching_label = f\"I-{label}\"\n",
    "                    if preds[idx] == matching_label:\n",
    "                        _, end = offset_mapping[idx]\n",
    "                        phrase_scores.append(sample_pred_scores[idx])\n",
    "                        idx += 1\n",
    "                    else:\n",
    "                        break # 해당 token 이 끝났기때문에 멈춘다.\n",
    "                        \n",
    "                if \"end\" in locals(): # _, end = offset_mapping[idx]\n",
    "                    phrase = sample_text[start:end]\n",
    "                    phrase_preds.append((phrase, start, end, label, phrase_scores))\n",
    "            \n",
    "            # \n",
    "            temp_df = []\n",
    "            for phrase_idx, (phrase, start, end, label, phrase_scores) in enumerate(phrase_preds):\n",
    "                word_start = len(sample_text[:start].split())\n",
    "                word_end = word_start + len(sample_text[start:end].split())\n",
    "                word_end = min(word_end, len(sample_text.split()))\n",
    "                ps = \" \".join([str(x) for x in range(word_start, word_end)])\n",
    "                if label != \"O\":\n",
    "                    if sum(phrase_scores) / len(phrase_scores) >= proba_thresh[label]:\n",
    "                        temp_df.append((sample_id, label, ps))\n",
    "\n",
    "            temp_df = pd.DataFrame(temp_df, columns=[\"id\", \"class\", \"predictionstring\"])\n",
    "\n",
    "            submission.append(temp_df)\n",
    "\n",
    "        submission = pd.concat(submission).reset_index(drop=True)\n",
    "        submission[\"len\"] = submission.predictionstring.apply(lambda x: len(x.split()))\n",
    "\n",
    "        # 각 token 이 점수를 넘었는지 boolean 으로 반환\n",
    "        def threshold(df):\n",
    "            df = df.copy()\n",
    "            for key, value in min_thresh.items():\n",
    "                index = df.loc[df[\"class\"] == key].query(f\"len<{value}\").index\n",
    "                df.drop(index, inplace=True)\n",
    "            return df\n",
    "\n",
    "        submission = threshold(submission)\n",
    "\n",
    "        # drop len\n",
    "        submission = submission.drop(columns=[\"len\"])\n",
    "\n",
    "        scr = score_feedback_comp(submission, self.valid_df, return_class_scores=True)\n",
    "        print(scr)\n",
    "        model.train()\n",
    "\n",
    "        epoch_score = scr[0]\n",
    "        if self.mode == \"min\":\n",
    "            score = -1.0 * epoch_score\n",
    "        else:\n",
    "            score = np.copy(epoch_score)\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(\"EarlyStopping counter: {} out of {}\".format(self.counter, self.patience))\n",
    "            if self.counter >= self.patience:\n",
    "                model.model_state = enums.ModelState.END\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, epoch_score, model):\n",
    "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
    "            print(\"Validation score improved ({} --> {}). Saving model!\".format(self.val_score, epoch_score))\n",
    "            model.save(self.model_path, weights_only=self.save_weights_only)\n",
    "        self.val_score = epoch_score\n",
    "\n",
    "class Collate:\n",
    "    '''\n",
    "    tokenizer 의 결과를 tensor 로 변환해준다.\n",
    "    cf) padding 을 수작업으로 하는게 인상적이다..\n",
    "    '''\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n",
    "        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n",
    "            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n",
    "        else:\n",
    "            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n",
    "            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n",
    "        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n",
    "\n",
    "        return output\n",
    "        \n",
    "def score_feedback_comp(pred_df, gt_df, return_class_scores=False):\n",
    "    class_scores = {}\n",
    "    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n",
    "    for discourse_type, gt_subset in gt_df.groupby(\"discourse_type\"):\n",
    "        pred_subset = pred_df.loc[pred_df[\"class\"] == discourse_type].reset_index(drop=True).copy()\n",
    "        class_score = score_feedback_comp_micro(pred_subset, gt_subset)\n",
    "        class_scores[discourse_type] = class_score\n",
    "    f1 = np.mean([v for v in class_scores.values()])\n",
    "    if return_class_scores:\n",
    "        return f1, class_scores\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'set_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4b910cf0c5d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mNUM_JOBS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#1# hyper parameter setting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mseed_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#2# fix seed for reproducability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'set_args' is not defined"
     ]
    }
   ],
   "source": [
    "NUM_JOBS = 12\n",
    "args = set_args() #1# hyper parameter setting\n",
    "\n",
    "seed_everything(args.seed) #2# fix seed for reproducability\n",
    "os.makedirs(args.output_path, exist_ok = True)\n",
    "\n",
    "#3# read kfold data\n",
    "if os.path.isfile(os.path.join(args.input_path, f'/train_{args.kfold}folds.csv')):\n",
    "    df = pd.read_csv(os.path.join(os.getcwd(), f'input/train_{args.kfold}folds.csv')) \n",
    "else:\n",
    "    df = pd.read_csv(os.path.join(os.getcwd(), 'input/train.csv'))\n",
    "    df = get_stratified_fold(df, args)\n",
    "    \n",
    "train_df = df[df[f\"{args.kfold}fold\"] != args.fold].reset_index(drop=True)\n",
    "valid_df = df[df[f\"{args.kfold}fold\"] == args.fold].reset_index(drop=True)\n",
    "\n",
    "#4# preprocess and wrap into dataset class\n",
    "args.model = './model'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model) #!# 나중에 local 에서 불러오도록 코드 수정 \n",
    "training_samples = prepare_training_data(train_df, tokenizer, args, num_jobs=NUM_JOBS)\n",
    "valid_samples = prepare_training_data(valid_df, tokenizer, args, num_jobs=NUM_JOBS)\n",
    "\n",
    "train_dataset = FeedbackDataset(training_samples, args.max_len, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model were not used when initializing BigBirdModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1560/1560 [20:41<00:00,  1.26it/s, f1=0.172, loss=1.32, stage=train]\n",
      "  0%|          | 0/390 [00:00<?, ?it/s]Attention type 'block_sparse' is not possible if sequence_length: 680 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3.Changing attention type to 'original_full'...\n",
      " 65%|██████▌   | 255/390 [00:38<00:20,  6.63it/s, stage=test]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 3.00 GiB (GPU 0; 23.65 GiB total capacity; 14.52 GiB already allocated; 861.31 MiB free; 18.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-330818623e28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mfp16\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m )\n",
      "\u001b[0;32m/opt/anaconda-3-2020.02/envs/snu36/lib/python3.6/site-packages/tez/model/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_dataset, valid_dataset, train_sampler, valid_sampler, device, epochs, train_bs, valid_bs, n_jobs, callbacks, fp16, train_collate_fn, valid_collate_fn, train_shuffle, valid_shuffle, accumulation_steps, clip_grad_norm)\u001b[0m\n\u001b[1;32m    421\u001b[0m                         \u001b[0mstep_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_to_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_scheduler_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainingState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_END\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1223\u001b[0m                     \u001b[0mbuffers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m                     \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda-3-2020.02/envs/snu36/lib/python3.6/site-packages/tez/model/model.py\u001b[0m in \u001b[0;36mtrain_state\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_runner\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mname_to_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda-3-2020.02/envs/snu36/lib/python3.6/site-packages/tez/callbacks/callbacks.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, current_state, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-3f74dfb44b8c>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mfinal_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mfinal_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mpred_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mpred_scrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda-3-2020.02/envs/snu36/lib/python3.6/site-packages/tez/model/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, dataset, sampler, batch_size, n_jobs, collate_fn)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtk0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda-3-2020.02/envs/snu36/lib/python3.6/site-packages/tez/model/model.py\u001b[0m in \u001b[0;36mpredict_one_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda-3-2020.02/envs/snu36/lib/python3.6/site-packages/tez/model/model.py\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-3720d1b1ba14>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ids, mask, token_type_ids, targets)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mtransformer_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mtransformer_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda-3-2020.02/envs/snu36/lib/python3.6/site-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2147\u001b[0m             \u001b[0mto_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m             \u001b[0mblocked_encoder_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocked_encoder_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2149\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2150\u001b[0m         )\n\u001b[1;32m   2151\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda-3-2020.02/envs/snu36/lib/python3.6/site-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, band_mask, from_mask, to_mask, blocked_encoder_mask, return_dict)\u001b[0m\n\u001b[1;32m   1639\u001b[0m                     \u001b[0mblocked_encoder_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1642\u001b[0m                 )\n\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda-3-2020.02/envs/snu36/lib/python3.6/site-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, band_mask, from_mask, to_mask, blocked_encoder_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m   1492\u001b[0m             \u001b[0mto_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m             \u001b[0mfrom_blocked_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocked_encoder_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m             \u001b[0mto_blocked_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocked_encoder_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m         )\n\u001b[1;32m   1496\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda-3-2020.02/envs/snu36/lib/python3.6/site-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, band_mask, from_mask, to_mask, from_blocked_mask, to_blocked_mask)\u001b[0m\n\u001b[1;32m   1388\u001b[0m                 \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m                 \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m             )\n\u001b[1;32m   1392\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda-3-2020.02/envs/snu36/lib/python3.6/site-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_probs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 0; 23.65 GiB total capacity; 14.52 GiB already allocated; 861.31 MiB free; 18.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#5# build model\n",
    "num_train_steps = int(len(train_dataset) / args.batch_size / args.accumulation_steps * args.epochs)\n",
    "\n",
    "model = FeedbackModel(\n",
    "    model_name=args.model,\n",
    "    num_train_steps=num_train_steps,\n",
    "    learning_rate=args.lr,\n",
    "    num_labels=len(target_id_map) - 1,\n",
    "    steps_per_epoch=len(train_dataset) / args.batch_size,\n",
    ")\n",
    "\n",
    "#6# define early stopping module : tez module 활용을 위한 것.\n",
    "es = EarlyStopping(\n",
    "    model_path=os.path.join(args.output_path, f\"model_{args.fold}.bin\"),\n",
    "    valid_df=valid_df,\n",
    "    valid_samples=valid_samples,\n",
    "    batch_size=args.valid_batch_size,\n",
    "    patience=5,\n",
    "    mode=\"max\",\n",
    "    delta=0.001,\n",
    "    save_weights_only=True,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "#7# train model\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    train_bs=args.batch_size,\n",
    "    device=\"cuda\",\n",
    "    epochs=args.epochs,\n",
    "    callbacks=[es],\n",
    "    fp16=True,\n",
    "    accumulation_steps=args.accumulation_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snu36",
   "language": "python",
   "name": "snu36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
