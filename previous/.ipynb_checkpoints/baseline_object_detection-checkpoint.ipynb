{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook baseline_object_detection.ipynb to script\n",
      "[NbConvertApp] Writing 34757 bytes to baseline_object_detection.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script baseline_object_detection.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences is 3\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "sents = nltk.sent_tokenize(original_train_df['discourse_text'][0])\n",
    "print(\"The number of sentences is\", len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Modern humans today are always on their phone.',\n",
       " 'They are always on their phone more than 5 hours a day no stop .All they do is text back and forward and just have group Chats on social media.',\n",
       " 'They even do it while driving.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmain ref) https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615?scriptVersionId=83230719\\ndetr ref) https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb\\ndetr ref) https://www.kaggle.com/tanulsingh077/end-to-end-object-detection-with-transformers-detr\\n코드 실행 전 준비 사항 :\\n    1) 먼저 kaggle 에서 데이터를 다운받고, './input' 에 압축해제 시켜준다.\\n    2) detr 을 fork 해준다. # !git clone https://github.com/facebookresearch/detr.git\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "main ref) https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615?scriptVersionId=83230719\n",
    "detr ref) https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb\n",
    "detr ref) https://www.kaggle.com/tanulsingh077/end-to-end-object-detection-with-transformers-detr\n",
    "코드 실행 전 준비 사항 :\n",
    "    1) 먼저 kaggle 에서 데이터를 다운받고, './input' 에 압축해제 시켜준다.\n",
    "    2) detr 을 fork 해준다. # !git clone https://github.com/facebookresearch/detr.git\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object detection 문제로 전처리하기\n",
    "# objective : use DETR structure for sentence segmentation.\n",
    "PATH = os.path.join(os.getcwd(), 'input')\n",
    "TRAIN_NER_PATH_DETR = os.path.join(PATH, 'train_detr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "#!# test code for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER label 로 전처리한 데이터 불러오기\n",
    "# 만약 starting class 를 원하지 않는다면 이하 코드를 실행할 것.\n",
    "\n",
    "try:\n",
    "    from ast import literal_eval\n",
    "    train_text_df = pd.read_csv(TRAIN_NER_PATH_DETR)\n",
    "    \n",
    "    # pandas saves lists as string, we must convert back\n",
    "    from ast import literal_eval\n",
    "    train_text_df.segment_label = train_text_df.segment_label.apply(lambda x: literal_eval(x))\n",
    "    \n",
    "    original_train_df = pd.read_csv(os.path.join(PATH, 'train.csv'))\n",
    "    \n",
    "except:\n",
    "    print('this is 1st time to run this code...')\n",
    "    print('try to convert original text to DETR labels...')\n",
    "    # read original text files0\n",
    "    train_ids, train_texts = [], []\n",
    "    for f in tqdm(list(os.listdir(os.path.join(PATH, 'train')))):\n",
    "        train_ids.append(f.replace('.txt', ''))\n",
    "        train_texts.append(open(os.path.join(PATH, 'train', f), 'r').read())\n",
    "    train_text_df = pd.DataFrame({'id': train_ids, 'text': train_texts})\n",
    "\n",
    "    # convert segment label into object detection label : [segment_type, x, y]\n",
    "    original_train_df = pd.read_csv(os.path.join(PATH, 'train.csv'))\n",
    "    label_list = []\n",
    "    for i, text_df in tqdm(train_text_df.iterrows()):\n",
    "        total = text_df['text'].split().__len__()\n",
    "        segment_label_list = []\n",
    "        for j, segment_df in original_train_df[original_train_df['id'] == text_df['id']].iterrows():\n",
    "            segment_label = [\n",
    "                segment_df['discourse_type'],\n",
    "                int(segment_df['predictionstring'].split(' ')[0]), \n",
    "                int(segment_df['predictionstring'].split(' ')[-1])\n",
    "            ]\n",
    "            segment_label_list.append(segment_label)\n",
    "\n",
    "        label_list.append(segment_label_list)\n",
    "\n",
    "    train_text_df['segment_label'] = label_list\n",
    "    train_text_df.to_csv(TRAIN_NER_PATH_DETR, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DICTIONARIES THAT WE CAN USE DURING TRAIN AND INFER\n",
    "output_labels_detr = [\n",
    "    'O', # detr need dummy class for padding\n",
    "    'Lead', \n",
    "    'Position', \n",
    "    'Claim', \n",
    "    'Counterclaim', \n",
    "    'Rebuttal', \n",
    "    'Evidence', \n",
    "    'Concluding Statement'\n",
    "]\n",
    "\n",
    "labels_to_ids = {v:k for k,v in enumerate(output_labels_detr)}\n",
    "ids_to_labels = {k:v for k,v in enumerate(output_labels_detr)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15594 train texts. We will split 90% 10% for validation.\n",
      "FULL Dataset: (15594, 3)\n",
      "TRAIN Dataset: (14034, 3)\n",
      "VALID Dataset: (1560, 3)\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE VALIDATION INDEXES\n",
    "IDS = original_train_df.id.unique()\n",
    "print('There are',len(IDS),'train texts. We will split 90% 10% for validation.')\n",
    "\n",
    "# TRAIN VALID SPLIT 90% 10%\n",
    "train_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\n",
    "valid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\n",
    "\n",
    "# CREATE TRAIN SUBSET AND VALID SUBSET\n",
    "data_df = train_text_df[['id','text', 'segment_label']]\n",
    "train_df = data_df.loc[data_df['id'].isin(IDS[train_idx]),['id', 'text', 'segment_label']].reset_index(drop=True)\n",
    "valid_df = data_df.loc[data_df['id'].isin(IDS[valid_idx])].reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
    "print(\"VALID Dataset: {}\".format(valid_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Venus', 'is', 'a', 'worthy', 'pursuit', 'despite', 'the', 'dangers.']\n",
      "39834    Venus is a worthy pursuit despite the dangers \n",
      "Name: discourse_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "'''test code for preprocessing'''\n",
    "i = 1\n",
    "j = 0\n",
    "\n",
    "# pre-processed\n",
    "label, start_idx, end_idx = data_df['segment_label'][i][j]\n",
    "text_id = data_df['id'][i]\n",
    "print(data_df['text'][i].split()[start_idx:end_idx+1])\n",
    "\n",
    "# original\n",
    "original_text = original_train_df[original_train_df['id'] == text_id]\n",
    "print(original_text[original_text['discourse_type'] == label]['discourse_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>segment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7301B174090E</td>\n",
       "      <td>I believe that a B average would be a good thi...</td>\n",
       "      <td>[[Position, 0, 14], [Claim, 32, 47], [Counterc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3799E21B6EC3</td>\n",
       "      <td>Venus is a worthy pursuit despite the dangers....</td>\n",
       "      <td>[[Position, 0, 7], [Claim, 8, 28], [Evidence, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29C5DBB0A339</td>\n",
       "      <td>Limiting car usage will have many advantages. ...</td>\n",
       "      <td>[[Position, 0, 6], [Claim, 11, 12], [Claim, 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1613BD216385</td>\n",
       "      <td>\"Making Mona Lisa Smile\" is about a computer h...</td>\n",
       "      <td>[[Lead, 0, 39], [Position, 40, 64], [Evidence,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D4A3E7EC982E</td>\n",
       "      <td>In this essay i will be explaining the differe...</td>\n",
       "      <td>[[Lead, 0, 22], [Position, 23, 33], [Claim, 34...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  \\\n",
       "0  7301B174090E  I believe that a B average would be a good thi...   \n",
       "1  3799E21B6EC3  Venus is a worthy pursuit despite the dangers....   \n",
       "2  29C5DBB0A339  Limiting car usage will have many advantages. ...   \n",
       "3  1613BD216385  \"Making Mona Lisa Smile\" is about a computer h...   \n",
       "4  D4A3E7EC982E  In this essay i will be explaining the differe...   \n",
       "\n",
       "                                       segment_label  \n",
       "0  [[Position, 0, 14], [Claim, 32, 47], [Counterc...  \n",
       "1  [[Position, 0, 7], [Claim, 8, 28], [Evidence, ...  \n",
       "2  [[Position, 0, 6], [Claim, 11, 12], [Claim, 14...  \n",
       "3  [[Lead, 0, 39], [Position, 40, 64], [Evidence,...  \n",
       "4  [[Lead, 0, 22], [Position, 23, 33], [Claim, 34...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 이 잘 작동하는지 확인하는 코드\n",
    "# #!# 로 표지된 index 를 바꿔주면 해당 dataset_row 에 대해서 전처리된 라벨과 실제 라벨에서 다른 부분을 출력해준다.\n",
    "\n",
    "# data = data_df\n",
    "# is_train = True\n",
    "\n",
    "# index = 2 #!# 바꾸면서 다양한 시도 해보기\n",
    "\n",
    "# text = data.text[index]        \n",
    "# text_id = data.id[index]\n",
    "# segment_label_list = data.segment_label[index] if is_train else None\n",
    "\n",
    "# # TOKENIZE TEXT\n",
    "# encoding = tokenizer(\n",
    "#     text.split(),\n",
    "#     is_split_into_words=True,\n",
    "#     padding='max_length', #!# need to check exist seq s.t. longer than 4094\n",
    "#     truncation=True, #!# need to check exist seq s.t. longer than 4094\n",
    "#     max_length=500\n",
    "# )\n",
    "        \n",
    "# word_ids = encoding.word_ids()\n",
    "\n",
    "# segment_ids_list = [[labels_to_ids[label], start_idx, end_idx] for label, start_idx, end_idx in segment_label_list]\n",
    "\n",
    "# processed_list = []\n",
    "# for ids, start_idx, end_idx in segment_ids_list:\n",
    "#     start_word_ids = word_ids.index(start_idx)\n",
    "#     end_word_ids = word_ids.index(end_idx)\n",
    "    \n",
    "#     processed_list.append(tokenizer.decode(encoding.input_ids[start_word_ids:end_word_ids+1]))\n",
    "    \n",
    "# original_list = list(train_df[train_df['id'] == text_id]['discourse_text'])\n",
    "\n",
    "# is_same = True\n",
    "# for p_discourse, o_discourse in zip(processed_list, original_list):\n",
    "#     if p_discourse.split() == o_discourse.split():\n",
    "#         continue\n",
    "        \n",
    "#     else: \n",
    "#         is_same = False\n",
    "#         for p, o in zip(p_discourse.split(), o_discourse.split()):\n",
    "#             if p != o:\n",
    "#                 print(p, o)\n",
    "# if is_same:\n",
    "#     print('every token in the label is same.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''baseline : ignore \\n\\n, 문장 기호들'''\n",
    "#!# 문장 기호는 상당히 중요한 정보를 담고 있어서 처리해주고 싶은데.. \n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, is_train):\n",
    "        super(dataset, self).__init__()\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.is_train = is_train # if test (or validation) period, we won't use word label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        global max_segment\n",
    "        # GET TEXT AND WORD LABELS \n",
    "        text = self.data.text[index]        \n",
    "        segment_label_list = self.data.segment_label[index] if self.is_train else None\n",
    "\n",
    "        # TOKENIZE TEXT\n",
    "        encoding = self.tokenizer(\n",
    "            text.split(),\n",
    "            is_split_into_words=True,\n",
    "            return_offsets_mapping=False, #!# how to use it for enabling tokenizer to \"see\" \\n\\n?\n",
    "            padding='max_length', #!# need to check exist seq s.t. longer than 4094\n",
    "            truncation=True, #!# need to check exist seq s.t. longer than 4094\n",
    "            max_length=self.max_len\n",
    "        )\n",
    "        \n",
    "        word_ids = encoding.word_ids()\n",
    "        \n",
    "        # CREATE TARGETS\n",
    "        #!# detr label padding 구현 : x, y 정보는 어떻게 넣어주는가? 0이어도 되나, random 이 더 좋으려나\n",
    "        #!# 결론 : padding 은 구현 안함. loss 계산에서 누락. 필요하면 이하 코드 활용.\n",
    "        #!# 근데, 또 null_object weight 같은걸 보면 아예 없지는 않은듯.... 어렵네\n",
    "        if self.is_train:\n",
    "            segment_ids_list = torch.as_tensor([[labels_to_ids[label], start_idx, end_idx] for label, start_idx, end_idx in segment_label_list]) # [num_seg, 3]\n",
    "            segment_ids_pad  = torch.zeros(max_segment - segment_ids_list.size(0), segment_ids_list.size(1)) # [max_seg - num_seg, 3]\n",
    "            segment_ids_pad[:, -1] = self.max_len * torch.ones(max_segment - segment_ids_list.size(0))\n",
    "            segment_ids_list = torch.cat((segment_ids_list, segment_ids_pad), dim = 0) # [max_seg, 3]\n",
    "            encoding['labels'] = segment_ids_list #!# .type(torch.LongTensor) # class, bound box must be long tensor\n",
    "\n",
    "        # CONVERT TO TORCH TENSORS\n",
    "        item = {k: torch.as_tensor(v) for k, v in encoding.items()}\n",
    "        \n",
    "        # id 를 넣어주면 마무리\n",
    "        item['id'] = self.data.id[index]        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref) https://github.com/facebookresearch/detr 를 참고했으나, review 필요함.\n",
    "class DetrHead(nn.Module):\n",
    "    def __init__(self, feature_extractor, transformer, prediction_head, pos_emb, max_seq, max_segment, d_model,device_0, device_1, split_gpus = True):\n",
    "        super(DetrHead, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transformer = transformer\n",
    "        self.prediction_head = prediction_head\n",
    "        \n",
    "        self.feature_pos = nn.Parameter(torch.rand(max_seq, d_model).cuda(device_1)) #!# pos_emb # absolute positional encoding (sinusodial, attention is all you need)\n",
    "        self.query_pos = nn.Parameter(torch.rand(max_segment, d_model).cuda(device_1))\n",
    "        \n",
    "#         self.split_gpus = split_gpus\n",
    "        \n",
    "#         if split_gpus:\n",
    "#             print('split model for parallel processing')\n",
    "#             self.feature_extractor.cuda(0) # LM\n",
    "#             self.query_pos.cuda(1)\n",
    "#             self.feature_pos.cuda(1)\n",
    "#             self.transformer.cuda(1)\n",
    "#             self.prediction_head.cuda(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.feature_extractor(x) # x -> [b, s, d_model]\n",
    "        out = out.cuda(device_1)\n",
    "        out = self.transformer(out + 0.1 * repeat(self.feature_pos, 'i j -> b i j', b = out.size(0)), repeat(self.query_pos, 'i j -> b i j', b = out.size(0))) # [b, s, d_model]\n",
    "        out = self.prediction_head(out)\n",
    "        return out\n",
    "    \n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, lm):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.lm = lm\n",
    "        self.max_seq = lm.config.max_position_embeddings\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.lm(input_ids = x['input_ids'], attention_mask = x['attention_mask']).last_hidden_state #!# todo : try other layer\n",
    "        return out\n",
    "    \n",
    "class Transformer(nn.Module): #!# todo : change transformer structure with user defined transformer structure\n",
    "    def __init__(self, d_model, nhead = 8, num_encoder_layers = 6, num_decoder_layers = 6, dim_feedforward = 2048):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.transformer = nn.Transformer( # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "            d_model, \n",
    "            nhead = nhead, \n",
    "            num_encoder_layers = num_encoder_layers, \n",
    "            num_decoder_layers = num_decoder_layers, \n",
    "            dim_feedforward = dim_feedforward,\n",
    "            batch_first = True\n",
    "        ) \n",
    "        \n",
    "    def forward(self, f, q):\n",
    "        out = self.transformer(f, q)\n",
    "        return out\n",
    "\n",
    "class PredictionHead(nn.Module): #!# todo : try diff. prediction head\n",
    "    def __init__(self, d_model, num_class):\n",
    "        super(PredictionHead, self).__init__()\n",
    "        self.fc_layer_class = nn.Linear(d_model, num_class + 1)\n",
    "        self.fc_layer_segment = nn.Linear(d_model, 2)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        c = self.fc_layer_class(x)\n",
    "        b = self.activation(self.fc_layer_segment(x))\n",
    "        return (c, b)\n",
    "    \n",
    "import math\n",
    "class PositionalEmbedding(nn.Module): #!# ref) https://github.com/codertimo/BERT-pytorch\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model, requires_grad = False).float()\n",
    "        pos = torch.arange(0, max_len).float()\n",
    "        div = (-(torch.arange(0, d_model, 2).float() / d_model) * math.log(10000.0)).exp()\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(torch.einsum('i,j->ij', pos, div))\n",
    "        pe[:, 1::2] = torch.cos(torch.einsum('i,j->ij', pos, div))\n",
    "        pe = rearrange(pe, 'i j -> () i j')\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting\n",
    "\n",
    "* build dataloader\n",
    "* model\n",
    "* loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "# fork ref) https://github.com/facebookresearch/detr/blob/091a817eca74b8b97e35e4531c1c39f89fbe38eb/models/detr.py#L83\n",
    "# code ref) https://www.kaggle.com/tanulsingh077/end-to-end-object-detection-with-transformers-detr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# baseline 이 작동하는지 확인하기 위해 bert-base 활용\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/bigbird-roberta-base')\n",
    "lm = AutoModel.from_pretrained('google/bigbird-roberta-base') #!# 제출을 위해 local 에 다운받는 과정 필요.\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('./model')\n",
    "# lm = AutoModel.from_pretrained('google/bigbird-roberta-base') #!# 제출을 위해 local 에 다운받는 과정 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!#\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "train_dataset_minibatch = dataset(train_df.iloc[:10], tokenizer, lm.config.max_position_embeddings, is_train = True)\n",
    "train_loader_minibatch = DataLoader(train_dataset_minibatch, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "train_dataset = dataset(train_df, tokenizer, lm.config.max_position_embeddings, is_train = True)\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model parameter'''\n",
    "max_seq = lm.config.max_position_embeddings\n",
    "d_model = lm.config.hidden_size\n",
    "max_segment = 50\n",
    "num_class = len(output_labels_detr) - 1 # null-class\n",
    "\n",
    "nhead = 8\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "dim_feedforward = 2048\n",
    "\n",
    "device_0, device_1 = 1, 0\n",
    "\n",
    "FEATURE_EXTRACTOR = FeatureExtractor(lm).cuda(device_0)\n",
    "TRANSFORMER = Transformer(d_model, nhead = nhead, num_encoder_layers = num_encoder_layers, num_decoder_layers = num_decoder_layers, dim_feedforward = dim_feedforward).cuda(device_1)\n",
    "PREDICTION_HEAD = PredictionHead(d_model, num_class).cuda(device_1)\n",
    "PE = PositionalEmbedding(d_model, max_seq).cuda(device_1)\n",
    "DETR_HEAD = DetrHead(FEATURE_EXTRACTOR, TRANSFORMER, PREDICTION_HEAD, PE, max_seq, max_segment, d_model, device_0, device_1)\n",
    "\n",
    "model = DETR_HEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''hyper parameter'''\n",
    "EPOCH = 1\n",
    "LR = 5e-5\n",
    "max_norm = 2 # gradient clipping\n",
    "\n",
    "weight_dict = {'weight_bbox' : 1, 'weight_class' : 0, 'weight_giou' : 0} #!# it is important to make sure the model learns bbox faster than class\n",
    "\n",
    "if_grad_clip = True\n",
    "if_scheduler = False\n",
    "\n",
    "class_loss = nn.CrossEntropyLoss()\n",
    "box_loss = nn.L1Loss()\n",
    "\n",
    "# 이하는 작업중인 hyper-parameters\n",
    "# null_class_coef = 0.5 #!# null class 학습이 전혀 안이뤄지는것도 문제가 있지... 지금은 전부 버리는데 이게 맞는 것 같지는 않아."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!# 여기부터 facebook source code 를 바꾸고 있음.\n",
    "#!# ref) https://github.com/facebookresearch/detr/blob/eb9f7e03ed8e2ed2cd55528989fe7df890bc3fc0/models/matcher.py#L12\n",
    "#!# 각 코드가 잘 작동하는지 *매우* 유의해서 살펴봐야 함.\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "#!# 높은 확률로 div zero 로 인한 overflow 예상됨. -> 아마 여기서 gradient exploding 문제가 발생했을듯\n",
    "#!# matching 이 초기에 이뤄지지 않으면 아예 학습이 안되는 알고리즘임에 주의하자. \n",
    "def one_dim_iou(out_bbox, tgt_bbox):\n",
    "    # assert (tgt_bbox[:, 1] >= tgt_bbox[:, 0]).all() #!# 이게 있어야 학습이 잘 이뤄질 것 같긴해... 근데 초기부터 이뤄지는게 가능한건가?\n",
    "    # assert (out_bbox[:, 1] >= out_bbox[:, 0]).all()\n",
    "    \n",
    "    max_matrix = torch.max(\n",
    "        repeat(out_bbox, 'bq i -> bq r i', r = tgt_bbox.size(0)), \n",
    "        repeat(tgt_bbox, 'bq i -> r bq i', r = out_bbox.size(0))\n",
    "    )\n",
    "\n",
    "    min_matrix = torch.min(\n",
    "        repeat(out_bbox, 'bq i -> bq r i', r = tgt_bbox.size(0)), \n",
    "        repeat(tgt_bbox, 'bq i -> r bq i', r = out_bbox.size(0))\n",
    "    )\n",
    "\n",
    "    return (min_matrix[:, :, 1] - max_matrix[:, :, 0]) / (max_matrix[:, :, 1] - min_matrix[:, :, 0])\n",
    "\n",
    "@torch.no_grad()\n",
    "def match(outputs : dict, targets : dict, weight_bbox = 1, weight_class = 1, weight_giou = 1):\n",
    "    '''scipy 의 linear_sum_assignment 를 활용해서 Hungarian matching 을 수행한다. \n",
    "    모델이 예측한 class 와 bounding box 를 토대로 target 과 비교해서\n",
    "    cost 를 최소화하는 matching 을 찾아낸다.\n",
    "    \n",
    "    #!# DETR 코드와 마찬가지로 target 에는 no-object class 가 없다. 즉, no-object 에 대해서는 loss 를 계산하지 않는다.\n",
    "    # ref) https://github.com/facebookresearch/detr/blob/eb9f7e03ed8e2ed2cd55528989fe7df890bc3fc0/models/matcher.py#L12\n",
    "    \n",
    "    Parameters\n",
    "        outputs: dict\n",
    "            - 'pred_logits': 각 class 에 대한 logit. 이후 softmax 를 통해 class 에 대한 확률 계산으로 사용된다.\n",
    "            - 'pred_boxes' :  예측한 bounding box. [x, y] 로 이뤄진다.\n",
    "        target: dict\n",
    "            - 'labels': 해당 segment 의 class\n",
    "            - 'boxes' : 해당 segment 의 bounding box [x, y]\n",
    "            \n",
    "    Returns\n",
    "        match_list: list [batch_size, [row_ind, col_ind]]\n",
    "            - 각 batch 에 대해서 row 와 col 의 matching index 정보를 담은 array 를 반환한다.\n",
    "    '''\n",
    "    \n",
    "    batch_size, num_query, num_tgt = outputs[\"pred_logits\"].size(0), outputs[\"pred_logits\"].size(1), targets['labels'].size(-1)\n",
    "\n",
    "    # We flatten to compute the cost matrices in a batch\n",
    "    out_prob = rearrange(outputs[\"pred_logits\"], 'b s c -> (b s) c').softmax(dim = -1)  # [batch_size * num_queries, num_classes]\n",
    "    out_bbox = rearrange(outputs[\"pred_boxes\"], 'b s box -> (b s) box')  # [batch_size * num_queries, 2]\n",
    "\n",
    "    # Also concat the target labels and boxes\n",
    "    tgt_ids  = torch.cat([v for v in targets[\"labels\"].type(torch.LongTensor)]).to(device)\n",
    "    tgt_bbox = torch.cat([v for v in targets[\"boxes\"]]).to(device)\n",
    "\n",
    "    # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
    "    # but approximate it in 1 - proba[target class].\n",
    "    # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
    "    cost_class = -out_prob[:, tgt_ids]\n",
    "\n",
    "    # Compute the L1 cost between boxes\n",
    "    #!# 이거 normalize 안돼서 너무 클거임. 1/n_seq 해주는게 좋을 듯\n",
    "    cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1) # [batch_size * num_queries, batch_size * num_queries]\n",
    "\n",
    "    # Compute the giou cost betwen boxes\n",
    "    #!# 분명 overflow 문제 발생함.. 어떻게 해결할 수 있을까. detr 은 assert 사용함\n",
    "    cost_giou = -one_dim_iou(out_bbox, tgt_bbox)\n",
    "\n",
    "    #!# Final cost matrix\n",
    "    #!# need to change weight for loss\n",
    "    C = weight_bbox * cost_bbox + weight_class * cost_class + weight_giou * cost_giou\n",
    "    C = torch.nan_to_num(C, nan = 1e9)\n",
    "    \n",
    "    if 1e9 in C:\n",
    "        print(float(torch.sum(C == 1e9))) #!# to track how many segments are discarded.\n",
    "        \n",
    "    C = rearrange(C, '(b1 q1) (b2 q2) -> b1 q1 b2 q2', # [batch_size, num_query, batch_size, num_query]\n",
    "                  b1 = batch_size, q1 = num_query, b2 = batch_size, q2 = num_tgt) \n",
    "\n",
    "    match_list = [linear_sum_assignment(C[b, :, b, :].detach().cpu()) for b in range(batch_size)]\n",
    "    return match_list\n",
    "\n",
    "\n",
    "def _index_select_for_batch(x : torch.tensor, index : torch.tensor) -> torch.tensor:\n",
    "    '''Unfold batch and select element\n",
    "    Parameters\n",
    "        x : [batch, ..., index]\n",
    "        index : [batch, index]\n",
    "        \n",
    "    Returns\n",
    "        x (selected) : [batch, ..., index]\n",
    "    '''\n",
    "    x_shape = x.shape\n",
    "    \n",
    "    # unfold batch\n",
    "    index = torch.cat(tuple([torch.as_tensor(index_batch) + batch_idx * x_shape[1] for batch_idx, index_batch in enumerate(index)]), dim = 0) #\n",
    "    index = index.to(x.device)\n",
    "    x = torch.cat(tuple([x_batch for x_batch in x]), dim = 0)\n",
    "    x = torch.index_select(x, 0, index)\n",
    "    \n",
    "    return x.view(x_shape)\n",
    "\n",
    "def _unfold_batch(x): # batch_first = True\n",
    "    return torch.cat(tuple([x_batch for x_batch in x]), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DETR_HEAD\n",
    "\n",
    "loss_traj = []\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer,\n",
    "    lr_lambda=lambda i: 0.95 ** i,\n",
    "    last_epoch=-1,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.query_pos.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.23it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.20it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -777.6716,  3595.9766],\n",
       "         [  325.2748,  3595.9692],\n",
       "         [ -718.4332,  3595.9507],\n",
       "         [  130.7974,  3595.9653],\n",
       "         [ -657.6916,  3595.9653],\n",
       "         [ -548.3517,  3595.9243],\n",
       "         [ -753.2715,  3595.9658],\n",
       "         [  199.9766,  3595.9622],\n",
       "         [ -422.4648,  3595.9590],\n",
       "         [  201.2695,  3595.9565],\n",
       "         [   26.0222,  3595.9639],\n",
       "         [ -482.1213,  3595.9634],\n",
       "         [   39.4480,  3595.9458],\n",
       "         [ -398.4665,  3595.9175],\n",
       "         [  243.8737,  3595.9622],\n",
       "         [ -768.8726,  3595.9375],\n",
       "         [ -391.9407,  3595.9514],\n",
       "         [ -175.3814,  3595.9565],\n",
       "         [  621.2871,  3595.9563],\n",
       "         [-1331.8142,  3595.9639],\n",
       "         [-1185.1816,  3595.9712],\n",
       "         [ -102.1380,  3595.9641],\n",
       "         [ -408.9997,  3595.9729],\n",
       "         [-1006.4448,  3595.9246],\n",
       "         [ -202.7234,  3595.9639],\n",
       "         [ -964.4291,  3595.9741],\n",
       "         [ -309.7928,  3595.9282],\n",
       "         [ -440.4927,  3595.9294],\n",
       "         [ -661.3203,  3595.9443],\n",
       "         [ -954.1018,  3595.9519],\n",
       "         [-1170.8821,  3595.9685],\n",
       "         [  935.5587,  3595.9785],\n",
       "         [  361.0809,  3595.9592],\n",
       "         [ -724.0349,  3595.9468],\n",
       "         [ 1176.2338,  3595.9458],\n",
       "         [ -389.9465,  3595.9702],\n",
       "         [ -396.5763,  3595.9375],\n",
       "         [   26.0800,  3595.9602],\n",
       "         [  684.8544,  3595.9382],\n",
       "         [ -581.8471,  3595.9519],\n",
       "         [ -154.3694,  3595.9585],\n",
       "         [-1654.8489,  3595.9453],\n",
       "         [ -498.3823,  3595.9722],\n",
       "         [  223.7096,  3595.9631],\n",
       "         [ -996.8885,  3595.9707],\n",
       "         [ -376.8915,  3595.9326],\n",
       "         [ -326.1344,  3595.9504],\n",
       "         [ -278.1424,  3595.9370],\n",
       "         [ -251.5998,  3595.9404],\n",
       "         [ -500.2256,  3595.9646]]], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "out_list = []\n",
    "for _ in range(2):\n",
    "    for batch in tqdm(train_loader_minibatch): #!# train_loader\n",
    "        text_id_list = batch.pop('id')\n",
    "        batch = {k : v.cuda(device_0) for k, v in batch.items()}\n",
    "\n",
    "        c, b = model(batch) # c : class, b : bounding box\n",
    "        out = {'pred_logits' : c, 'pred_boxes' : b}\n",
    "        tgt = {'labels' : batch['labels'][:, :, 0], 'boxes' : (batch['labels'][:, :, 1:] - 1000) / max_seq} #!# input normalization\n",
    "\n",
    "        match_list = match(out, tgt, **weight_dict)\n",
    "        match_list = [list(index_batch[1]) for index_batch in match_list]\n",
    "        \n",
    "        loss = \\\n",
    "            0 * class_loss( #!# learn only segmentation\n",
    "                _unfold_batch(out['pred_logits'].softmax(dim = -1)).cuda(device_1),\n",
    "                _unfold_batch(_index_select_for_batch(tgt['labels'].type(torch.LongTensor), match_list).cuda(device_1))\n",
    "            ) + \\\n",
    "            weight_dict['weight_bbox'] * box_loss(\n",
    "                _unfold_batch(out['pred_boxes']).cuda(device_1),\n",
    "                _unfold_batch(_index_select_for_batch(tgt['boxes'], match_list).cuda(device_1))\n",
    "            )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        if if_grad_clip:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if if_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        loss_traj.append(float(loss))\n",
    "        \n",
    "        out_list.append(out['pred_boxes'])\n",
    "        \n",
    "plt.plot(loss_traj)\n",
    "plt.show()\n",
    "\n",
    "out['pred_boxes'] * max_seq - 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_list = match(out, tgt, **weight_dict)\n",
    "match_list = [list(index_batch[1]) for index_batch in match_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0404, device='cuda:0', grad_fn=<L1LossBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_loss(\n",
    "    _unfold_batch(out['pred_boxes']).cuda(device_1),\n",
    "    _unfold_batch(_index_select_for_batch(tgt['boxes'].type(torch.LongTensor), match_list).cuda(device_1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_boxes = out['pred_boxes'].cuda(device_0)\n",
    "ex_boxes[tgt['labels'] != 0] = tgt['boxes'][tgt['labels'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_out = {'pred_logits' : out['pred_logits'].cuda(device_1), 'pred_boxes' : ex_boxes.cuda(device_1)}\n",
    "\n",
    "ex_match_list = match(ex_out, tgt, **weight_dict)\n",
    "ex_match_list = [list(index_batch[1]) for index_batch in ex_match_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0043, device='cuda:0', grad_fn=<L1LossBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_loss(\n",
    "    _unfold_batch(ex_out['pred_boxes']).cuda(device_1),\n",
    "    _unfold_batch(_index_select_for_batch(tgt['boxes'].type(torch.LongTensor), ex_match_list).cuda(device_1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = ex_out\n",
    "targets = tgt\n",
    "\n",
    "batch_size, num_query, num_tgt = outputs[\"pred_logits\"].size(0), outputs[\"pred_logits\"].size(1), targets['labels'].size(-1)\n",
    "\n",
    "# We flatten to compute the cost matrices in a batch\n",
    "out_prob = rearrange(outputs[\"pred_logits\"], 'b s c -> (b s) c').softmax(dim = -1)  # [batch_size * num_queries, num_classes]\n",
    "out_bbox = rearrange(outputs[\"pred_boxes\"], 'b s box -> (b s) box')  # [batch_size * num_queries, 2]\n",
    "\n",
    "# Also concat the target labels and boxes\n",
    "tgt_ids  = torch.cat([v for v in targets[\"labels\"].type(torch.LongTensor)]).to(device)\n",
    "tgt_bbox = torch.cat([v for v in targets[\"boxes\"]]).to(device)\n",
    "\n",
    "# Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
    "# but approximate it in 1 - proba[target class].\n",
    "# The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
    "cost_class = -out_prob[:, tgt_ids]\n",
    "\n",
    "# Compute the L1 cost between boxes\n",
    "#!# 이거 normalize 안돼서 너무 클거임. 1/n_seq 해주는게 좋을 듯\n",
    "cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1) # [batch_size * num_queries, batch_size * num_queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the giou cost betwen boxes\n",
    "#!# 분명 overflow 문제 발생함.. 어떻게 해결할 수 있을까. detr 은 assert 사용함\n",
    "cost_giou = -one_dim_iou(out_bbox, tgt_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0012, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_dim_iou(out_bbox, tgt_bbox)[2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ = torch.max(torch.stack((tgt_bbox[3], out_bbox[2])), dim = 0)\n",
    "min_ = torch.min(torch.stack((tgt_bbox[3], out_bbox[2])), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0126, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(min_[1] - max_[0]).sum() / (min_[0] - max_[1]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_bbox[2,3] == torch.abs(out_bbox[2] - tgt_bbox[3]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!# Final cost matrix\n",
    "#!# need to change weight for loss\n",
    "C = weight_bbox * cost_bbox + weight_class * cost_class + weight_giou * cost_giou\n",
    "C = torch.nan_to_num(C, nan = 1e9)\n",
    "\n",
    "if 1e9 in C:\n",
    "    print(float(torch.sum(C == 1e9))) #!# to track how many segments are discarded.\n",
    "\n",
    "C = rearrange(C, '(b1 q1) (b2 q2) -> b1 q1 b2 q2', # [batch_size, num_query, batch_size, num_query]\n",
    "              b1 = batch_size, q1 = num_query, b2 = batch_size, q2 = num_tgt) \n",
    "\n",
    "match_list = [linear_sum_assignment(C[b, :, b, :].detach().cpu()) for b in range(batch_size)]\n",
    "return match_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def _index_select_for_batch(x : torch.tensor, index : torch.tensor) -> torch.tensor:\n",
    "    '''Unfold batch and select element\n",
    "    Parameters\n",
    "        x : [batch, ..., index]\n",
    "        index : [batch, index]\n",
    "        \n",
    "    Returns\n",
    "        x (selected) : [batch, ..., index]\n",
    "    '''\n",
    "    x_shape = x.shape\n",
    "    \n",
    "    # unfold batch\n",
    "    index = torch.cat(tuple([torch.as_tensor(index_batch) + batch_idx * x_shape[1] for batch_idx, index_batch in enumerate(index)]), dim = 0) #\n",
    "    index = index.cuda(x.device)\n",
    "    x = torch.cat(tuple([x_batch for x_batch in x]), dim = 0)\n",
    "    x = torch.index_select(x, 0, index)\n",
    "    \n",
    "    return x.view(x_shape)\n",
    "\n",
    "def _unfold_batch(x): # batch_first = True\n",
    "    return torch.cat(tuple([x_batch for x_batch in x]), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:49<00:00,  2.02it/s]\n"
     ]
    }
   ],
   "source": [
    "for param in model.feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
    "\n",
    "for _ in range(1):\n",
    "    for batch in tqdm(train_loader_minibatch): #!# train_loader\n",
    "        text_id_list = batch.pop('id')\n",
    "        batch = {k : v.cuda(device_0) for k, v in batch.items()}\n",
    "\n",
    "        c, b = model(batch) # c : class, b : bounding box\n",
    "        out = {'pred_logits' : c, 'pred_boxes' : b}\n",
    "        tgt = {'labels' : batch['labels'][:, :, 0], 'boxes' : batch['labels'][:, :, 1:] / max_seq} #!# input normalization\n",
    "\n",
    "        match_list = match(out, tgt, **weight_dict)\n",
    "        match_list = [list(index_batch[1]) for index_batch in match_list]\n",
    "        \n",
    "        loss = \\\n",
    "            1 * class_loss( #!# learn classification module\n",
    "                _unfold_batch(out['pred_logits'].softmax(dim = -1)).cuda(device_1),\n",
    "                _unfold_batch(_index_select_for_batch(tgt['labels'].type(torch.LongTensor), match_list).cuda(device_1))\n",
    "            ) + \\\n",
    "            weight_dict['weight_bbox'] * box_loss(\n",
    "                _unfold_batch(out['pred_boxes']).cuda(device_1),\n",
    "                _unfold_batch(_index_select_for_batch(tgt['boxes'].type(torch.LongTensor), match_list).cuda(device_1))\n",
    "            )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        if if_grad_clip:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if if_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        loss_traj.append(float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcbec1246a0>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_traj[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!# why classification mal-functioning?\n",
    "# 1. positional encoding (query, feature)\n",
    "# 2. fc_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0042],\n",
       "         [0.0044, 0.0081],\n",
       "         [0.0083, 0.0105],\n",
       "         [0.0107, 0.0139],\n",
       "         [0.0142, 0.0151],\n",
       "         [0.0154, 0.0232],\n",
       "         [0.0627, 0.0688],\n",
       "         [0.0691, 0.0796],\n",
       "         [0.0833, 0.0957],\n",
       "         [0.0959, 0.1023],\n",
       "         [0.1025, 0.1062],\n",
       "         [0.1064, 0.1187],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.0000, 0.0000]]], device='cuda:1')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt['boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  21.3308,  148.8575],\n",
       "         [  73.1937,   41.8354],\n",
       "         [ -41.9145,  239.2831],\n",
       "         [  59.4469,   50.6216],\n",
       "         [  42.1644,   88.9807],\n",
       "         [ 239.6809,   55.3452],\n",
       "         [ -80.9757,  238.6915],\n",
       "         [ -72.4200,   47.7628],\n",
       "         [ 217.4932,  258.3589],\n",
       "         [ 212.7906, -103.6397],\n",
       "         [ 238.9761,  -52.8346],\n",
       "         [  23.2336,   90.0844],\n",
       "         [  52.7408, -150.0342],\n",
       "         [ 213.1765,  532.8762],\n",
       "         [ 291.7162,   26.0797],\n",
       "         [ -15.9739, -261.7321],\n",
       "         [ 239.1922,  277.3680],\n",
       "         [ 281.9629,   60.3299],\n",
       "         [  45.1648,  308.1915],\n",
       "         [ -65.3509,   97.2841],\n",
       "         [ -54.8236,  -90.8115],\n",
       "         [   1.8124,  -93.1089],\n",
       "         [ 198.7298,  -25.1922],\n",
       "         [ 169.7078,  -10.2121],\n",
       "         [ 255.2095,  -49.1883],\n",
       "         [ -14.4000,  -39.0859],\n",
       "         [  92.6680,  173.7189],\n",
       "         [-216.4725,  295.3028],\n",
       "         [ -51.8181,  101.8259],\n",
       "         [  88.1491,  219.7425],\n",
       "         [  82.5210,  -22.6537],\n",
       "         [  -3.5932,  271.1028],\n",
       "         [ 246.0753, -159.7444],\n",
       "         [  56.2286,   13.6156],\n",
       "         [ 309.1305,  152.5842],\n",
       "         [ -47.0182,  -57.1242],\n",
       "         [ 220.0634,  187.0221],\n",
       "         [  91.2293,   47.8300],\n",
       "         [ 222.8022,  -44.7552],\n",
       "         [ 454.5429,  -51.8591],\n",
       "         [ 129.2440,  -37.3549],\n",
       "         [ 481.1373,  287.9130],\n",
       "         [ 254.3487,  107.8847],\n",
       "         [ -21.9229,  108.6833],\n",
       "         [  73.5723,  108.2238],\n",
       "         [ 115.3093,   53.6571],\n",
       "         [-122.5216,  417.4773],\n",
       "         [ 116.5518,   28.5759],\n",
       "         [  84.1414, -119.0342],\n",
       "         [  -8.0417,  237.4301]]], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['pred_boxes'] * max_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 : matching function\n",
    "# 2 : loss function\n",
    "\n",
    "#!# todo : create error output to check whether model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument x2 in method wrapper___cdist_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2563a8e369e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mex_tgt_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_tgt_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex_tgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mex_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex_out_true_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex_tgt_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-c38ae19d1896>\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(outputs, targets, weight_bbox, weight_class, weight_giou)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Compute the L1 cost between boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m#!# 이거 normalize 안돼서 너무 클거임. 1/n_seq 해주는게 좋을 듯\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mcost_bbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size * num_queries, batch_size * num_queries]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Compute the giou cost betwen boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mcdist\u001b[0;34m(x1, x2, p, compute_mode)\u001b[0m\n\u001b[1;32m   1151\u001b[0m             cdist, (x1, x2), x1, x2, p=p, compute_mode=compute_mode)\n\u001b[1;32m   1152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'use_mm_for_euclid_dist_if_necessary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcompute_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'use_mm_for_euclid_dist'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument x2 in method wrapper___cdist_forward)"
     ]
    }
   ],
   "source": [
    "'''matching function'''\n",
    "# create exemplar batch\n",
    "BATCH_SIZE, num_class, max_segment = 2, 7, 5\n",
    "\n",
    "ex_tgt = torch.cat((torch.randint(1, num_class+1, (BATCH_SIZE, max_segment, 1)), torch.rand(BATCH_SIZE, max_segment, 2)), dim = -1) # [class, x, y]\n",
    "ex_indices = torch.stack([torch.randperm(max_segment) for _ in range(BATCH_SIZE)], dim = 0)\n",
    "ex_out_true = _index_select_for_batch(ex_tgt, ex_indices)\n",
    "\n",
    "convert_out_dict = lambda x : {'pred_logits' : 0.9 * F.one_hot(x[:, :, 0].type(torch.LongTensor), num_classes = num_class+1), 'pred_boxes' : x[:, :, 1:]}\n",
    "convert_tgt_dict = lambda x : {'labels' : x[:, :, 0], 'boxes' : x[:, :, 1:]}\n",
    "\n",
    "ex_out_true_dict = convert_out_dict(ex_out_true)\n",
    "ex_tgt_dict = convert_tgt_dict(ex_tgt)\n",
    "\n",
    "ex_indices == torch.as_tensor([list(tgt_idx) for out_idx, tgt_idx in match(ex_out_true_dict, ex_tgt_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''loss function'''\n",
    "tgt = ex_tgt_dict\n",
    "out = ex_out_true_dict\n",
    "# out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_loss(\n",
    "    _unfold_batch(out['pred_logits'].softmax(dim = -1)).cuda(device_1),\n",
    "    _unfold_batch(_index_select_for_batch(tgt['labels'].type(torch.LongTensor), match_list).cuda(device_1))\n",
    ")\n",
    "\n",
    "box_loss(\n",
    "    _unfold_batch(out['pred_boxes']).cuda(device_1),\n",
    "    _unfold_batch(_index_select_for_batch(tgt['boxes'].type(torch.LongTensor), match_list).cuda(device_1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!# Final cost matrix\n",
    "#!# need to change weight for loss\n",
    "C = weight_bbox * cost_bbox + weight_class * cost_class + weight_giou * cost_giou\n",
    "C = torch.nan_to_num(C, nan = 1e9) #!# linear_sum_assignment 는 nan 을 처리 못함.\n",
    "C = rearrange(C, '(b1 q1) (b2 q2) -> b1 q1 b2 q2', # [batch_size, num_query, batch_size, num_query]\n",
    "              b1 = batch_size, q1 = num_query, b2 = batch_size, q2 = num_tgt) \n",
    "\n",
    "match_list = [linear_sum_assignment(C[b, :, b, :].detach().cpu()) for b in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_indices = match(ex_out_true_dict, ex_tgt_dict) #!# awkard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_indices = torch.as_tensor([list(tgt_idx) for out_idx, tgt_idx in match_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_tgt_matched = _index_select_for_batch(ex_tgt, match_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.0000, 0.5725, 0.4980],\n",
       "         [4.0000, 0.6826, 0.3051],\n",
       "         [5.0000, 0.4635, 0.4550],\n",
       "         [4.0000, 0.9371, 0.6556],\n",
       "         [1.0000, 0.6387, 0.5247]],\n",
       "\n",
       "        [[4.0000, 0.9371, 0.6556],\n",
       "         [5.0000, 0.4635, 0.4550],\n",
       "         [1.0000, 0.4162, 0.2843],\n",
       "         [7.0000, 0.3138, 0.1980],\n",
       "         [4.0000, 0.5725, 0.4980]]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_out_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.6387, 0.5247],\n",
       "         [5.0000, 0.4635, 0.4550],\n",
       "         [4.0000, 0.6826, 0.3051],\n",
       "         [4.0000, 0.9371, 0.6556],\n",
       "         [4.0000, 0.5725, 0.4980]],\n",
       "\n",
       "        [[7.0000, 0.3138, 0.1980],\n",
       "         [4.0000, 0.9371, 0.6556],\n",
       "         [5.0000, 0.4635, 0.4550],\n",
       "         [4.0000, 0.5725, 0.4980],\n",
       "         [1.0000, 0.4162, 0.2843]]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_tgt_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_tgt_dict = {'labels' : out_true[:, :, 0], 'boxes' : out_true[:, :, 1:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 'pred_logits': 각 class 에 대한 logit. 이후 softmax 를 통해 class 에 대한 확률 계산으로 사용된다.\n",
    "            - 'pred_boxes' :  예측한 bounding box. [x, y] 로 이뤄진다.\n",
    "        target: dict\n",
    "            - 'labels': 해당 segment 의 class\n",
    "            - 'boxes' : 해당 segment 의 bounding box [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 3])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_tgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-d022bffc8354>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mex_out_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex_tgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "ex_out_true = torch.index_select(ex_tgt, 0, true_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. with true output, can matcher find true indices?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = DETR_HEAD.to(device)\n",
    "loss_traj = []\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer,\n",
    "    lr_lambda=lambda i: 0.95 ** i,\n",
    "    last_epoch=-1,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "model.train()\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for i, batch in enumerate(train_loader_minibatch):\n",
    "    batch = {k : v.to(device) for k, v in batch.items()}\n",
    "    break\n",
    "    \n",
    "class_pred, box_pred = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_pred.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2528],\n",
       "         [261.0119, 253.2528],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2528],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2530],\n",
       "         [261.0119, 253.2528],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0120, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2528],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529]],\n",
       "\n",
       "        [[261.0118, 253.2530],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2528],\n",
       "         [261.0118, 253.2528],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2528],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2528],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0119, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529],\n",
       "         [261.0118, 253.2529]]], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq * (0.5 - box_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = {'id' : [], 'class' : [], 'predictionstring' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_overlap(row):\n",
    "    \"\"\"\n",
    "    Calculates the overlap between prediction and\n",
    "    ground truth and overlap percentages used for determining\n",
    "    true positives.\n",
    "    \"\"\"\n",
    "    set_pred = set(row.predictionstring_pred.split(' '))\n",
    "    set_gt = set(row.predictionstring_gt.split(' '))\n",
    "    # Length of each and intersection\n",
    "    len_gt = len(set_gt)\n",
    "    len_pred = len(set_pred)\n",
    "    inter = len(set_gt.intersection(set_pred))\n",
    "    overlap_1 = inter / len_gt\n",
    "    overlap_2 = inter/ len_pred\n",
    "    return [overlap_1, overlap_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_pred, b = model(batch)\n",
    "c_pred = torch.argmax(c_pred, dim = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "= max_seq * (0.5 - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(batch):\n",
    "    # MOVE BATCH TO GPU AND INFER\n",
    "    ids = batch[\"input_ids\"].to(config['device'])\n",
    "    mask = batch[\"attention_mask\"].to(config['device'])\n",
    "    outputs = model(ids, attention_mask=mask, return_dict=False)\n",
    "    all_preds = torch.argmax(outputs[0], axis=-1).cpu().numpy() \n",
    "\n",
    "    # INTERATE THROUGH EACH TEXT AND GET PRED\n",
    "    predictions = []\n",
    "    for k,text_preds in enumerate(all_preds):\n",
    "        token_preds = [ids_to_labels[i] for i in text_preds]\n",
    "\n",
    "        prediction = []\n",
    "        word_ids = batch['wids'][k].numpy()  \n",
    "        previous_word_idx = -1\n",
    "        for idx,word_idx in enumerate(word_ids):                            \n",
    "            if word_idx == -1:\n",
    "                pass\n",
    "            elif word_idx != previous_word_idx:              \n",
    "                prediction.append(token_preds[idx])\n",
    "                previous_word_idx = word_idx\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/zzy990106/pytorch-ner-infer\n",
    "# changed a little bit (용현)\n",
    "\n",
    "df=valid_dataset\n",
    "loader=valid_loader\n",
    "\n",
    "# put model in eval mode\n",
    "model.eval()\n",
    "\n",
    "# calc prediction from model\n",
    "model_predict = []\n",
    "for batch in loader:\n",
    "    labels = inference(batch)\n",
    "    y_pred2.extend(labels)\n",
    "\n",
    "final_preds2 = []\n",
    "for i in range(len(df)):\n",
    "\n",
    "    idx = df.id.values[i]\n",
    "    #pred = [x.replace('B-','').replace('I-','') for x in y_pred2[i]]\n",
    "    pred = y_pred2[i] # Leave \"B\" and \"I\"\n",
    "    preds = []\n",
    "    j = 0\n",
    "    while j < len(pred):\n",
    "        cls = pred[j]\n",
    "        if cls == 'O': j += 1\n",
    "        else: cls = cls.replace('B','I') # spans start with B\n",
    "        end = j + 1\n",
    "        while end < len(pred) and pred[end] == cls:\n",
    "            end += 1\n",
    "\n",
    "        if cls != 'O' and cls != '' and end - j > 7:\n",
    "            final_preds2.append((idx, cls.replace('I-',''),\n",
    "                                 ' '.join(map(str, list(range(j, end))))))\n",
    "\n",
    "        j = end\n",
    "\n",
    "oof = pd.DataFrame(final_preds2)\n",
    "oof.columns = ['id','class','predictionstring']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def score_feedback_comp(pred_df, gt_df):\n",
    "    \"\"\"\n",
    "    A function that scores for the kaggle\n",
    "        Student Writing Competition\n",
    "        \n",
    "    Uses the steps in the evaluation page here:\n",
    "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
    "    \"\"\"\n",
    "    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df = pred_df[['id','class','predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df['pred_id'] = pred_df.index\n",
    "    gt_df['gt_id'] = gt_df.index\n",
    "    # Step 1. all ground truths and predictions for a given class are compared.\n",
    "    joined = pred_df.merge(gt_df,\n",
    "                           left_on=['id','class'],\n",
    "                           right_on=['id','discourse_type'],\n",
    "                           how='outer',\n",
    "                           suffixes=('_pred','_gt')\n",
    "                          )\n",
    "    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n",
    "    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n",
    "\n",
    "    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n",
    "\n",
    "    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n",
    "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
    "    # the prediction is a match and considered a true positive.\n",
    "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n",
    "    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n",
    "\n",
    "\n",
    "    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n",
    "    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n",
    "    tp_pred_ids = joined.query('potential_TP') \\\n",
    "        .sort_values('max_overlap', ascending=False) \\\n",
    "        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n",
    "\n",
    "    # 3. Any unmatched ground truths are false negatives\n",
    "    # and any unmatched predictions are false positives.\n",
    "    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n",
    "\n",
    "    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n",
    "    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n",
    "\n",
    "    # Get numbers of each type\n",
    "    TP = len(tp_pred_ids)\n",
    "    FP = len(fp_pred_ids)\n",
    "    FN = len(unmatched_gt_ids)\n",
    "    #calc microf1\n",
    "    my_f1_score = TP / (TP + 0.5*(FP+FN))\n",
    "    return my_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALID TARGETS\n",
    "valid = train_df.loc[train_df['id'].isin(IDS[valid_idx])]\n",
    "\n",
    "# OOF PREDICTIONS\n",
    "oof = get_predictions(test_dataset, testing_loader)\n",
    "\n",
    "# COMPUTE F1 SCORE\n",
    "f1s = []\n",
    "CLASSES = oof['class'].unique()\n",
    "print()\n",
    "for c in CLASSES:\n",
    "    pred_df = oof.loc[oof['class']==c].copy()\n",
    "    gt_df = valid.loc[valid['discourse_type']==c].copy()\n",
    "    f1 = score_feedback_comp(pred_df, gt_df)\n",
    "    print(c,f1)\n",
    "    f1s.append(f1)\n",
    "print()\n",
    "print('Overall',np.mean(f1s))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for submission\n",
    "# ref) https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615?scriptVersionId=83230719\n",
    "test_names, test_texts = [], []\n",
    "for f in list(os.listdir('../input/feedback-prize-2021/test')):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    test_texts.append(open('../input/feedback-prize-2021/test/' + f, 'r').read())\n",
    "test_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\n",
    "\n",
    "test_names, train_texts = [], []\n",
    "for f in tqdm(list(os.listdir('../input/feedback-prize-2021/train'))):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    train_texts.append(open('../input/feedback-prize-2021/train/' + f, 'r').read())\n",
    "train_text_df = pd.DataFrame({'id': test_names, 'text': train_texts})\n",
    "\n",
    "\n",
    "# # TEST DATASET\n",
    "# test_texts_set = dataset(test_texts, tokenizer, config['max_length'], True)\n",
    "# test_texts_loader = DataLoader(test_texts_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define config\n",
    "config = {'model_name': MODEL_NAME,   \n",
    "          'max_length': 1024,\n",
    "          'train_batch_size':4,\n",
    "          'valid_batch_size':4,\n",
    "          'epochs':5,\n",
    "          'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n",
    "          'max_grad_norm':10,\n",
    "          'device': 'cuda' if cuda.is_available() else 'cpu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666cc845bdc54244acb9aff81cef5626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51674fa78df049babf1e80f7e1b66fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e6301eb1b248a7aa55f1f9062defcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/846k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055c9165c0c64092805d94e89973a1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/775 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9508107e9dd543fa9c6ed48270f8d929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdForTokenClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# if you are first running this code, please download LM.\n",
    "MODEL_NAME = 'google/bigbird-roberta-base' # choose which model to download\n",
    "DOWNLOADED_MODEL_PATH = 'model'            # choose where to download the model\n",
    "\n",
    "if DOWNLOADED_MODEL_PATH == 'model':\n",
    "    os.mkdir('model')\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True) #!# add_prefix_space?\n",
    "    tokenizer.save_pretrained('model')\n",
    "\n",
    "    config_model = AutoConfig.from_pretrained(MODEL_NAME) \n",
    "    config_model.num_labels = 15\n",
    "    config_model.save_pretrained('model')\n",
    "\n",
    "    backbone = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, \n",
    "                                                               config=config_model)\n",
    "    backbone.save_pretrained('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!# implement tokenizer \\n\\n, to see paragraph information\n",
    "# ref) https://github.com/huggingface/tokenizers/issues/247\n",
    "# ref) https://www.kaggle.com/c/feedback-prize-2021/discussion/296713\n",
    "tokenizer.decode(tokenizer(r'\\\\n\\\\n', return_offsets_mapping=True)['input_ids'])\n",
    "\n",
    "'''add new special token to model and tokenizer'''\n",
    "special_tokens_dict = {'additional_special_tokens': [r'\\\\n\\\\n']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i) \n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:                \n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            else:  \n",
    "                label_ids.append(label[word_idx])\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snu36",
   "language": "python",
   "name": "snu36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
